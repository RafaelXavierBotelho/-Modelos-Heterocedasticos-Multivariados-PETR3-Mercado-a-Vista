# ============================  */                     \*   =============================                                 
#                                                                                         
#                                                                                         
#     *  Past profitability doesn't correspond to future ones.                            
#     *  All estimations made in this project are only for educational purpusoes!         
#     *  Highly biased on Anchoring bias                                                             
#                                                                                         
#                                                                                         
# ============================  */                     \*   =============================                                 

#  conda activate econometrics_cea

#  Calling the needed packages

from torch.autograd import Variable
import torch.nn.functional as F
from datetime import datetime
from statsmodels.stats.api import het_breuschpagan
import math
import numbers
import sgs
import pickle
import time
import torch
import sklearn
import torch.nn as nn
from functools import reduce
import seaborn as sns
import numpy as np 
from numpy import cumsum, log, polyfit, sqrt, std, subtract
from numpy.random import randn
import matplotlib.pyplot as plt
import statsmodels.api as sm
from torch.optim.optimizer import Optimizer, required
import copy
from sklearn.metrics import mean_absolute_error
from sklearn import preprocessing
from scipy.stats import ks_2samp
import os
import sys
import arviz as az
from scipy import signal
from scipy.optimize import minimize
import psutil
from statsmodels.graphics.tsaplots import plot_acf, plot_pacf
from tabulate import tabulate
from statsmodels.tsa.stattools import adfuller
from scipy import stats
from scipy.stats import kstest
from statsmodels.stats.diagnostic  import lilliefors
from scipy.stats import shapiro
from scipy.stats import normaltest
from torch.nn.modules.utils import _pair
import datetime
import pandas as pd 
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import mean_absolute_error
from torchviz import make_dot
from sklearn.linear_model import LinearRegression
from torch.optim.optimizer import Optimizer, required
import copy
from scipy.stats import norm
import sys
from torch.nn.modules.utils import _pair
import numba
import soundfile
import audioread
from distfit import distfit
from statsmodels.tsa.stattools import adfuller, kpss
from alpha_vantage.timeseries import TimeSeries
import requests
from datetime import datetime
import yfinance as yf
from scipy import signal
import scipy.signal as spsignal
import pystan
import emcee
import pyextremes as pyex
from scipy.stats import genpareto
from pyextremes import EVA
import ta
from statsmodels.regression.quantile_regression import QuantReg

#########################*/                                    *\#########################
#  
#                                       ---- Passage 1 ---- 
#                       This module provides descriptive statistical analysis .
#
#########################*/                                    *\#########################

#  ========== Webscrapping the data using alpha_vantage libray ==========

api_key = 'Your API key here'
symbol = 'PETR3.SA'

url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={symbol}&outputsize=full&apikey={api_key}'

response = requests.get(url)
data = response.json()

#  Check for errors in response
if "Time Series (Daily)" not in data:
    raise ValueError(f"Error fetching data: {data.get('Note') or data.get('Error Message') or 'Unknown error'}")

df = pd.DataFrame.from_dict(data['Time Series (Daily)'], orient='index')
df.index = pd.to_datetime(df.index)
df = df.sort_index()  # Ensure dates are sorted ascending

start_date = pd.to_datetime('2024-06-13')
end_date = pd.to_datetime('2025-06-13')   # Only 1 year of data was used due compunational cost

#  Filter dates between start and end
df_filtered = df.loc[(df.index >= start_date) & (df.index <= end_date)]

#  Rename columns for convenience
df_filtered = df_filtered.rename(columns={
    '1. open': 'Open',
    '2. high': 'High',
    '3. low': 'Low',
    '4. close': 'Close',
    '5. volume': 'Volume'
})

#  Convert columns to numeric
for col in ['Open', 'High', 'Low', 'Close', 'Volume']:
    df_filtered[col] = pd.to_numeric(df_filtered[col])

#  Plotting
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), gridspec_kw={'height_ratios': [3, 1]}, sharex=True)

ax1.plot(df_filtered.index, df_filtered['Close'], label='Close Price')
ax1.set_title('Historical Petrobras Stock Price (PETR3.SA)')
ax1.set_ylabel('Price (BRL)')
ax1.grid(True)

ax2.bar(df_filtered.index, df_filtered['Volume'], color='gray')
ax2.set_title('Trading Volume')
ax2.set_ylabel('Volume')
ax2.grid(True)

plt.tight_layout()
plt.show()

#  Figure_1.1a(Data Presentation)     

pd.DataFrame(df_filtered).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA.csv', index=None)
PETR3_SA = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA.csv")

pd.DataFrame(df_filtered.index).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA_Dates.csv', index=None)
PETR3_SA_Dates = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Dates.csv")


#########################*/                                    *\#########################
#
#    
#    Time Series Stationarity and Fractional Differencing:
#
#    Many macroeconomic indicators exhibit an exponential trend due to
#    their linkage with exponential population growth. To apply linear
#    modeling methods, achieving stationarity is crucial. The standard
#    process typically involves:
#
#       1. Take logarithms: This transforms the series to exhibit a linear trend.
#       2. Take the difference: This step is then applied to achieve a stationary series.
#
#    Reference:
#    COLE, T. J.; ALTMAN, D. G. Statistics Notes: Percentage differences, symmetry,
#    and natural logarithms. BMJ, v. 358, ago., p. j3683, 2017.
#    Available at: https://doi.org/20.1136/bmj.j3683. Accessed on: 19 Jun. 2025.
#
#    This module specifically focuses on **fractional differencing** of time
#    series data. Its primary goal is to remove trends and achieve stationarity
#    while, importantly, preserving the "memory" of the series. This memory
#    preservation is vital for effective predictive modeling, particularly
#    in domains like finance.
#
#    The underlying principle of fractional differencing is based on the series
#    expansion of the differencing operator, commonly represented as (1-L)^d,
#    where 'L' is the lag operator and 'd' denotes the fractional differentiation order.
#
#    Reference:
#    DE PRADO, Marcos López. Advances in Financial Machine Learning. 1st ed. Hoboken: Wiley, 2017.
# 
#
#########################*/                                    *\#########################



def getWeights(d,lags):
     """
       Calculates the weights (coefficients) for the fractional differencing operator.
       These weights are derived from the series expansion of (1 - L)^d, where 'L' is the
	   lag operator and 'd' is the fractional differencing order.
	 Args:
        d (float): The fractional differencing order. Can be any real number.
        lags (int): The number of coefficients to compute. Represents the depth
                    of the look-back window for the differencing operation.

     Returns:
        numpy.ndarray: A 2D NumPy array of shape (lags, 1) containing the
                       fractional differencing weights.
	 """
     w=[1] # Initialize with the first weight (coefficient of current observation)
     for k in range(1,lags):
	     # Calculate subsequent weights using the recursive formula:
		 # w_k = -w_{k-1} * (d - k + 1) / k		 
         w.append(-w[-1]*((d-k+1))/k)
	 # Reshape to a column vector for consistency with common linear algebra operations	 
     w=np.array(w).reshape(-1,1)
     return w
	 

def plotWeights(dRange, lags, numberPlots):
     """
     Plots the lag coefficients (weights) for various orders of fractional differencing.

     This visualization helps understand how different 'd' values impact the weights,
     especially their decay over increasing lags.

     Args:
        dRange (tuple): A tuple (start_d, end_d) defining the range of
                        fractional differencing orders to plot.
        lags (int): The maximum number of lag coefficients to plot for each 'd'.
        numberPlots (int): The number of distinct 'd' values to evenly sample
                           within dRange for plotting.
     """
	 # Initialize a DataFrame to store weights for different 'd' values
     weights=pd.DataFrame(np.zeros((lags, numberPlots)))
	 # Generate evenly spaced 'd' values within the specified range
     interval=np.linspace(dRange[0],dRange[1],numberPlots)
	 # Compute weights for each sampled 'd' value
     for i, diff_order in enumerate(interval):
         weights[i]=getWeights(diff_order,lags)
     # Set DataFrame column names to the corresponding 'd' values for clarity in the plot legend
	 #
     weights.columns = [round(x,2) for x in interval]
	 #
	 # Generate the plot
     fig=weights.plot(figsize=(15,6))
	 #
	 # Customize plot labels and title for better readability
     plt.legend(title='Order of differencing')
     plt.title('Lag coefficients for various orders of differencing')
     plt.xlabel('lag coefficients')
     plt.grid(False)
     plt.show()
	 
	 
def cutoff_find(order,cutoff,start_lags): 
     """
     Finds the minimum number of lags required for the absolute value of the
     last fractional differencing weight to fall below a specified cutoff.

     This function helps determine an appropriate 'memory cutoff' for the
     fractional differencing operation, ensuring that coefficients
     beyond a certain point are negligible.

     Args:
        order (float): The fractional differencing order (our 'd').
        cutoff (float): The threshold value. The function stops when the
                        absolute value of the last weight is less than or
                        equal to this cutoff. (e.g., 1e-5)
        start_lags (int, optional): An initial number of lags to start the search from.
                                     Can be set to a higher value to potentially speed
                                     up the function if a large number of lags is expected.
                                     Defaults to 1.
     Returns:
        int: The minimum number of lags at which the absolute value of the
             last computed weight falls below or equals the cutoff.
     """
     val=np.inf
     lags=start_lags
     while abs(val)>cutoff:
         w=getWeights(order, lags)
         val=w[len(w)-1]
         lags+=1
     return lags
	 
#########################*/                                    *\#########################
#
#                        ---- Fractional Differencing Considerations ----
#    Larger series may allow for a lower cutoff to preserve memory in the periodogram.
#   Generally, the higher the differentiation order 'd', the shorter the series becomes.
#   In this specific case, the cutoff, here referred to as 'tau', was set to 1e-2.
#
#########################*/                                    *\#########################
	 
	 
def ts_differencing_tau(series, order, tau):
     """
     Applies fractional differencing to a pandas Series using a cutoff
     determined by 'tau' (a significance threshold for the weights).
     This function performs the actual fractional differencing operation,
     effectively creating a new series that is approximately stationary
     while retaining long-term memory properties based on the chosen 'd'
     and 'tau' parameters.
     Args:
        series (pd.Series): The input time series to be fractionally differenced.
                            Expected to be a numerical pandas Series.
        order (float): The fractional differencing order (d).
        tau (float): The cutoff threshold for the weights. The number of lags
                     used in the differencing will be determined such that
                     the last weight is below this 'tau'.
     Returns:
        pd.Series: The fractionally differenced time series. The initial
                   `lag_cutoff` number of NaNs (or zeros if fillna(0) is used)
                   are removed, as these cannot be computed due to insufficient
                   past data.
     """
     lag_cutoff=(cutoff_find(order,tau,1))  # Find the required number of lags based on the specified order and tau cutoff
     weights=getWeights(order, lag_cutoff)  # Get the weights for the determined number of lags
	 
     res=0  # Initialize the result series by accumulating the weighted sum of lagged series
     for k in range(lag_cutoff):  #  Apply the weighted sum of lagged series
         res += weights[k]*series.shift(k).fillna(0) # iterates through the weights and shifts the series accordingly
     return res[lag_cutoff:]
	 


#########################*/                                    *\#########################
#            
#                       ---- Parameters for Fractional Differencing Optimization ----
#
#    Define a range of possible fractional differencing orders 'd'.
#    This creates a sequence from 1/200 (0.005) to 199/200 (0.995) in increments of 0.005.
#    This range typically covers values between 0 (original series) and 1 (integer differencing).
# 
#########################*/                                    *\#########################

possible_d=np.divide(range(1,200),200)

# Define 'tau' (cutoff for weights). This threshold determines the number of lags
# used in the fractional differencing calculation. Weights smaller than 'tau' are
# considered negligible, effectively truncating the memory window.
# As discussed, a larger series might allow for a lower 'tau' to preserve more memory.
# In this specific case, 'tau' is set to 1e-2 (0.01).

tau=1e-2

# Initialize a list to store the p-values from the Augmented Dickey-Fuller (ADF) test.
# The ADF test is used to check for stationarity of the differenced series.
# A lower p-value (typically < 0.05 or < 0.01) indicates stronger evidence against
# the null hypothesis of non-stationarity (i.e., the series is stationary).

log_adf_stat_holder=[None]*len(possible_d)

# --- Iterative Fractional Differencing and Stationarity Testing ---

# Loop through each possible 'd' value to find the optimal order.
# The goal is to find the smallest 'd' that results in a stationary series (low ADF p-value).


for i in range(len(possible_d)):
    current_d_order = possible_d[i]
	
	# 1. Apply natural logarithm transformation to the 'Close' prices of PETR3_SA.
    #    This step aims to linearize any exponential trends and stabilize variance.
    log_series = np.log(pd.DataFrame(PETR3_SA["Close"]))	
	# 2. Apply fractional differencing using the current 'd' order and 'tau' cutoff.
    #    This returns a new series that is potentially stationary.
    differenced_series = ts_differencing_tau(log_series, current_d_order, tau)
    # 3. Perform the Augmented Dickey-Fuller (ADF) test on the differenced series.
    #    adfuller() returns several values; the [1] index retrieves the p-value.
    log_adf_stat_holder[i] = adfuller(differenced_series)[1]
	
    
# --- Visualization of Stationarity vs. Differencing Order ---

# Plotting the p-values from the ADF test against the corresponding 'd' values.
# This plot visually helps identify the minimum 'd' order at which the series
# becomes stationary (i.e., where the p-value drops below a significance level).
plt.plot(possible_d, log_adf_stat_holder)

# Add a horizontal red line representing a common significance level (alpha).
# For ADF test, p-values below this line indicate rejection of the null hypothesis
# of non-stationarity. 
plt.axhline(y=0.05, color='r', linestyle='--', label='ADF P-value Threshold')


# Customize plot labels for clarity.
plt.xlabel('Fractional Differencing Order (d)') 
plt.ylabel('ADF Test P-value for Log-Differenced PETR3_SA Close Price')
plt.title('ADF Test P-values vs. Fractional Differencing Order for PETR3.SA') 
plt.legend() 
plt.grid(True) 
plt.show()

# After running this, you would typically inspect the plot to find the smallest 'd'
# for which the p-value consistently stays below your chosen significance threshold.
# This 'd' value is then considered the optimal fractional differencing order
# for your time series, balancing stationarity with memory preservation.

#Figura1.1b_(ADF Test P-values vs. Fractional Differencing Order for PETR3.SA)

d=1.00
PETR3_SA_Close_fractional_difference  = (ts_differencing_tau(np.log(PETR3_SA["Close"]), d,tau )) 
PETR3_SA_Close_fractional_difference = PETR3_SA_Close_fractional_difference.fillna(method='ffill')  
pd.DataFrame(PETR3_SA_Close_fractional_difference).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA_Close_fractional_difference.csv', index=None)
PETR3_SA_Close_fractional_difference = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Close_fractional_difference.csv")

def nan_helper(y):
    """
    Helper function to locate NaNs and provide an interpolation utility.

    This function is designed to assist in cleaning time series data by identifying
    missing values (NaNs) and providing a mechanism to interpolate them,
    commonly used for linear interpolation.

    Args:
        y (np.ndarray or pd.Series): A 1D array or Series that may contain NaNs.

    Returns:
        tuple: A tuple containing:
            - nans (np.ndarray or pd.Series of bool): A boolean array/Series where
              True indicates NaN values.
            - x (function): A lambda function that, when called with a boolean
              array, returns the indices of the True values. This is typically
              used to convert NaN indices to interpolated values.

    Example:
        # Linear interpolation of NaNs
        # (Assuming 'y' is your original data with NaNs)
        nans, x = nan_helper(y)
        y[nans] = np.interp(x(nans), x(~nans), y[~nans])
    """
    return np.isnan(y), lambda z: z.nonzero()[0]


nans, x= nan_helper(PETR3_SA_Close_fractional_difference.values)
PETR3_SA_Close_fractional_difference.values[nans]= np.interp(x(nans), x(~nans), PETR3_SA_Close_fractional_difference.values[~nans])
check_nan_in_df = PETR3_SA_Close_fractional_difference.isnull().values.any()
print (check_nan_in_df)
#False


PETR3_SA_Dates_log_dif = PETR3_SA_Dates[len(PETR3_SA_Dates) - len(PETR3_SA_Close_fractional_difference):]
pd.DataFrame(PETR3_SA_Dates_log_dif).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA_Dates_log_dif.csv', index=None)
PETR3_SA_Dates_log_dif = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Dates_log_dif.csv")
PETR3_SA_Dates_log_dif = pd.to_datetime(PETR3_SA_Dates_log_dif.squeeze())

# ---  Plotting  ---
fig, ax1 = plt.subplots(figsize=(16, 8)) # Increased figure size for more room
ax1.plot(PETR3_SA_Dates_log_dif, PETR3_SA_Close_fractional_difference.values , color='blue', label='PETR3_SA_Close_fractional_differentiated', linewidth=1.5)
ax1.set_xlabel('Date', fontsize=12)
ax1.set_ylabel('Log returns ', color='blue', fontsize=12)
ax1.tick_params(axis='y', labelcolor='blue')
ax1.set_title(' Log Returns of PETR3.SA Over Time', fontsize=16)
# Add grids and legends
ax1.grid(True, linestyle=':', alpha=0.6) # Grid for prices
# Automatically adjust subplot parameters for a tight layout
plt.tight_layout()
plt.show()

#  Figure_1.1c(Log-Returns of PETR3.SA)


######*/               *\########
#                               #                           
#      Linear Kalman Filter     #
#         for denoising         #
#                               #                          
######*/               *\########


def matrix_to_symmetric_matrix(x):
    """
    Ensures a given matrix is symmetric by averaging it with its transpose.
    This is often used to ensure covariance matrices, which must be symmetric,
    remain so due to numerical precision issues or specific calculations.

    Args:
        x (np.ndarray): The input matrix.

    Returns:
        np.ndarray: A symmetric version of the input matrix.
    """
    return 0.5 * (x + np.transpose(x))


Linear_Kalman_Filter = """

/*  
   
    References

*   GREWAL, Mohinder S.; ANDREWS, Angus P. Kalman Filtering: Theory and Practice with MATLAB. 
    4. ed. Hoboken: Wiley, 2014.

*   KALMAN, R. E. A New Approach to Linear Filtering and Prediction Problems.
    Journal of Basic Engineering, v. 82, n. 1, p. 35-45, 1960.

*   KALMAN, R. E.; BUCY, R. S. New Results in Linear Filtering and Prediction Theory.
    Journal of Basic Engineering, v. 83, n. 1, p. 95-108, 1961.

*   LABBE, Roger. Kalman and Bayesian Filters in Python. 
    Disponível em: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python. Acesso em: 19 jun. 2025.

*   ARNOLD, J. [globaltemp.R]. Gist.GitHub,/2012.
    Disponível em: https://gist.github.com/jrnold/4700387. Acesso em: 19 jun. 2025.

estimated with:
  - Covariance filter (no square root or sequential processing)
  - time-invariant parameters
  - initial values, where the y observations are assumed to be normally distributed

*/


data {
    int<lower=1>  n;              // Number of observations (time steps) in the sample. 
    int p;                        // Number of observed variables (dimensions of y).
    int m;                        // Number of state variables (dimensions of x).
    int r;                        // Dimension of the system error equations. r >= m; R: m x r (error loading matrix), Q: r x r (covariance of errors)
    real dt;                      // Time interval (delta t). This can be related to the 'd' (differencing order)
    vector[p] y[n];               // Array of observed vectors. y[i] is the observation at time i.
    matrix[p, m] Z;               // Observation model matrix. Maps state space to observation space (y = Z*x + noise) 
    // System (Process) Equations
    matrix[m, m] Ac;              // Continuous-time system matrix (coefficients)
    matrix[m, r] R;               // System error loading matrix  
    // Initial values for the state mean (x_1) and covariance (P_1) at time t=1.
    vector[m] x_1;
    cov_matrix[m] P_1;            // Initial state covariance matrix. Must be symmetric and positive definite.
}

parameters {
  /* 
     Measurement (Observation) Errors
     These represent the standard deviations of the independent observation noise components.
  */	 
  vector<lower=0.0>[p] log_h_raw; // Sample on the log scale
  /*
     System (Process) Errors
     These represent the standard deviations of the independent process noise components.
  */
  vector<lower=0.0>[r] log_q_raw;
}

transformed parameters {
  vector<lower=0.0>[p] h = exp(log_h_raw); // Transform back to standard deviations
  vector<lower=0.0>[r] q = exp(log_q_raw);
  matrix[p, p] H;         // Observation noise covariance matrix.
  matrix[r, r] Q;         // Process noise covariance matrix
  vector[m] x[n+1];       // Array to store the predicted a priori state means over time (including x[n+1] for the last update).
  matrix[m,m] P[n+1];     // Array to store the predicted a priori state covariance matrices over time.
  matrix[m, m] A;         // Discrete-time system transition matrix.
  real log_lik [n];       // Array to store the log-likelihood of each observation.
  real lp=0;              // Acumulator of the log-likelihood for posterior check
  /*
         Constructing diagonal covariance matrices from standard deviations (h and q).
  */
  H = diag_matrix(h);
  Q = diag_matrix(q);
  /*	
             Initial values    
  */
  x[1] = x_1;        //     Values for the mean states
  P[1] = P_1;        //     Values for the covariance state matrix
  /* 
       Linear approximation/scaling of continuous matrix.	   
  */
  A = (Ac*dt);       //     Euler discretization 
  //
  for (i in 1:n) {
    vector[p] v;             // Innovation (prediction residual).
    matrix[p, p] F;          // Innovation (prediction) covariance matrix.
    matrix[p, p] Finv;       // Inverse of the innovation covariance matrix.
    matrix[m, p] K;          // Kalman Gain matrix.
	// Identity matrix, used in covariance update.
    matrix[m, m] I = diag_matrix(rep_vector(1, m)); 
	/*  predict the next mean states  (a priori) */
    x[i] = A*x[i];  
    /*  predict the next states  covariance matrix (a priori) */ 	
    P[i] = (A*P[i])*A' + (R*Q)*R';  
	/* 
	          measurements  
	*/
	//  Innovation or pre-fit residuals
    v = y[i] - Z * x[i];  
    //  Innovation (or prediction) covariance: uncertainty of the innovation.	
    F = (Z  * P[i]) * Z' + H;
    // Inverse of the innovation covariance matrix (needed for Kalman Gain and likelihood).	
    Finv = inverse(F);
	// Kalman Gain: Blends the prediction and the observation.
    K = (P[i] * Z' )* Finv;
	/*
         	--- Update System State (a posteriori ) ---
                      Kalman Gain 
    */
    P[i+1] = (I-K*Z)*P[i]*(I - K*Z)' + K*H*K';  // P[i+1] becomes the updated P for next step
    x[i+1] = x[i] + K * v;  //  Update the state mean vector (x_posterior) 
    /*      
	         --- Log-Likelihood Calculation ---
	*/
    log_lik[i] = multi_normal_lpdf(y[i]| Z * x[i], F); 
    }
    lp += lp + sum(log_lik);
}

model {
  /*  -----   Ensuring Numerical Stability  -----   */  
  log_h_raw ~ normal(log(0.01), 1.0); // Prior for log(h). Centered around log(0.01)
  log_q_raw ~ normal(log(0.01), 1.0); // Prior for log(q). Scale '1.0' means reasonable spread.
 }
  
"""

Linear_Kalman_Filter = pystan.StanModel(model_code= Linear_Kalman_Filter)
   
x0 = float(PETR3_SA_Close_fractional_difference.iloc[0])

"""

  * Local Trend Model (m=2)
  * Concept: Assumes the observed series (log-returns) is driven by a latent level and trend,
  * both of which can evolve over time via random walks.

"""

data_Linear_Kalman_Filter = ({
          'n' : len(PETR3_SA_Close_fractional_difference),
          'p' : 1,
          'm' : 2,    
          'r' : 2,
          'dt' : 1,
          'y':  np.array(PETR3_SA_Close_fractional_difference).reshape(-1, 1).astype(np.float64), # shape(n,1) 
          'Z' : np.array([[1.0, 0.0]]).astype(np.float64),   # shape (1, 2)
          'Ac' :  np.array([[0.0, 1.0],   # shape (2, 2)
                           [1.0, 0.0]]).astype(np.float64),
          'R' : np.array([[1.0, 0.0],
                          [0.0, 1.0]]).astype(np.float64),
          'x_1' : np.array([x0, x0]).astype(np.float64),
          'P_1' : matrix_to_symmetric_matrix(np.array([[1.0, 0.0],
                                                       [0.0, 1.0]])).astype(np.float64)
          })
		  
control = {}
control['max_treedepth'] = 11
control['adapt_delta'] = 0.9999

fit_Linear_Kalman_Filter = Linear_Kalman_Filter.sampling(data=data_Linear_Kalman_Filter, iter=15000,  chains=1, warmup=7000 , thin=1, seed=101, n_jobs = 1,  control=control)

#   [distfit] >WARNING> Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.
#    To run all diagnostics call pystan.check_hmc_diagnostics(fit)


# Define the directory and filename
save_directory = r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA'
filename = 'Linear_Kalman_Filter_log_returns.pkl'

# Construct the full file path
full_path = os.path.join(save_directory, filename)

# Save the model
with open(full_path, 'wb') as f:
    pickle.dump(fit_Linear_Kalman_Filter, f)

# Load the model
with open(full_path, 'rb') as f:
    Linear_Kalman_Filter_log_returns = pickle.load(f)

Linear_Kalman_Filter = az.from_pystan(fit_Linear_Kalman_Filter, log_likelihood="log_lik", observed_data={"y": ['PETR3_SA_Close_fractional_difference.squeeze(1)']})
# --- Calculate WAIC ---
waic_result = az.waic(Linear_Kalman_Filter)

print("\n--- WAIC Results ---")
print(waic_result)

###################################*/                      \*###################################
#
#          Computed from 8000 by 746 log-likelihood matrix
#
#                     Estimate       SE
#           elpd_waic  -416.29     0.14
#           p_waic        1.00        -
#
#
#
#      ---- Local Trend Kalman Filter for Financial Time Series ----
#	  
#   This model implements a linear Kalman filter with a local trend model (m=2)
#   to perform  denoising on financial log-returns.
#   
#   The model's primary function is to filter out random noise and estimate the
#   underlying, latent level and trend of the time series. This is clearly
#   visible in the results, where the "Estimated Latent Level" (x1) is a
#   smoother, denoised version of the "Observed Log Returns" (y).
#   
#   The model's effectiveness as a **denoising** tool is confirmed, but its
#   predictive performance is **extremely poor**. The WAIC diagnostics reveal
#   this limitation:
#    
#    416.29 (elpd_waic)  :  (Expected Log Pointwise Predictive Density)
#
#      This is the core measure of the model's predictive fit. It's the sum of the log-likelihoods for each data point, 
#      averaged over the posterior distribution of the model's parameters. A higher elpd_waic value (or, as in your case,
#	   a less positive value) means better out-of-sample predictive accuracy. Your value of 416.29 is a log-scale value.
#	   Its corresponding standard error (SE) of 0.14 provides a measure of uncertainty. When comparing two models, 
#	   the one with the higher elpd_waic (closer to zero or more negative) is generally preferred.
#   
#   3.03 (p_waic)   :    (Effective Number of Parameters)  
#		  
#     This term is a penalty for model complexity. It corrects for the fact that more complex models with more parameters tend to overfit the data.
#     The p_waic value is an estimate of how many parameters are being effectively determined by the data. A higher p_waic suggests a more flexible or complex model. 
#     A p_waic of 1.00 suggests that your model is relatively simple, with an effective number of parameters close to 1.
#
#  0.14 (SE - Standard Error)  : 
#
#     This represents the uncertainty in the elpd_waic estimate.
#     A smaller standard error indicates a more precise estimate. When comparing models,
#	  if the difference in elpd_waic values is larger than the sum of their standard errors,
#	  it suggests a statistically significant difference in predictive performance.
#
#     For example, if:
#
#       Model A: elpd_waic = -224.20, SE = 21.64
#
#       Model B: elpd_waic = -200.00, SE = 18.00
#               
#       Difference = 24.20,
#       Sum of SEs = 21.64 + 18.00 = 39.64
#       Since 24.20 < 39.64 → not significant
#   
#   In summary, while this Kalman filter implementation successfully performs  its intended  filtering/denoising  task, it is not at all suitable for
#   forecasting financial time series due to its linear and constant-volatility assumptions. For forecasting, a more advanced model, such as a Stochastic
#   Volatility (SV) or GARCH model, would be required to capture the time-varying volatility inherent in financial data.
#
#
###################################*/                      \*###################################



samples = fit_Linear_Kalman_Filter.extract()
x_samples = samples["x"]


"""
             -----   The Latent Level  -----
 
    Estimate of the true underlying log-return filtered through 
    the model's dynamics and noise assumptions.
"""

x1_smoothed_mean = x_samples[:, 1:len(PETR3_SA_Close_fractional_difference)+1, 0].mean(axis=0)


"""
           -----  The Latent Trend/Velocity  -----

    Represents the estimated rate of change or momentum in the underlying 
	log-returns at each point in time.If this value is positive, it suggests 
	the underlying level is increasing; if negative, it's decreasing.

"""

x2_smoothed_mean = x_samples[:, 1:len(PETR3_SA_Close_fractional_difference)+1, 1].mean(axis=0) 


# ===================-----------------------------
# Calculate the 95% Equal-Tailed Credibility Intervals
# For each time point, you need the 2.5th and 97.5th percentiles of its posterior samples.
# axis=0 means we're taking percentiles across the rows (the different posterior draws)
# ===================-----------------------------

x1_lower, x1_upper = np.percentile(x_samples[:,1:len(PETR3_SA_Close_fractional_difference)+1, 0], [2.5, 97.5], axis=0)
x2_lower, x2_upper = np.percentile(x_samples[:,1:len(PETR3_SA_Close_fractional_difference)+1, 1], [2.5, 97.5], axis=0)


# --- 4. Plotting ---

# Create a figure with two subplots (one for each state)
fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(14, 10), sharex=True)

# --- Plot State 1 (Level) ---

ax1 = axes[0]
ax1.plot(PETR3_SA_Dates_log_dif, PETR3_SA_Close_fractional_difference, label='Observed Log Returns (y)', color='grey', alpha=0.7)
ax1.plot(PETR3_SA_Dates_log_dif, x1_smoothed_mean, label='Estimated State 1 (Latent Level)', color='blue', linewidth=2)
ax1.fill_between(PETR3_SA_Dates_log_dif, x1_lower, x1_upper, color='blue', alpha=0.2, label='95% Credible Interval (State 1)')
ax1.set_ylabel('Value')
ax1.set_title('Observed Log Returns vs. Estimated Latent Level ($x_1$)')
ax1.legend()
ax1.grid(True, linestyle='--', alpha=0.6)

# --- Plot State 2 (Trend/Velocity) ---

ax2 = axes[1]
ax2.plot(PETR3_SA_Dates_log_dif, x2_smoothed_mean, label='Estimated State 2 (Latent Trend/Velocity)', color='green', linewidth=2)
ax2.fill_between(PETR3_SA_Dates_log_dif, x2_lower, x2_upper, color='green', alpha=0.2, label='95% Credible Interval (State 2)')
ax2.set_xlabel('Date')
ax2.set_ylabel('Value')
ax2.set_title('Estimated Latent Trend/Velocity ($x_2$)')
ax2.legend()
ax2.grid(True, linestyle='--', alpha=0.6)

# --- Enhance Date Formatting (Optional but recommended for long series) ---
fig.autofmt_xdate() # Automatically formats date labels to prevent overlap

plt.tight_layout() # Adjusts plot to prevent labels overlapping
plt.show()

#  Figure_1.1d ( Denoised Log Returns)

pd.DataFrame(x1_smoothed_mean).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA_Close_fractional_difference_Linear_filtred.csv', index=None)
PETR3_SA_Close_fractional_difference_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Close_fractional_difference_Linear_filtred.csv")

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min 
	
PETR3_SA_Close_fractional_difference_Linear_filtred_scaled = function_to_scale_data(PETR3_SA_Close_fractional_difference_Linear_filtred,-1,+1)
															 
# Calling distfit library to identify which pdf fits the data the most 
dist = distfit()

# Getting the best pdf 
dist.fit_transform(PETR3_SA_Close_fractional_difference_Linear_filtred_scaled.values)
print(dist.summary)


#              name     score         loc  ... bootstrap_score bootstrap_pass    color
#    0            t   0.30949 -0.00948979  ...               0           None  #e41a1c
#    1     dweibull  0.407324  -0.0126681  ...               0           None  #e41a1c
#    2         norm   1.03894   -0.012967  ...               0           None  #377eb8
#    3      lognorm   1.04559    -88.6716  ...               0           None  #4daf4a
#    4         beta   1.05083    -94.9836  ...               0           None  #984ea3
#    5     loggamma   1.05372     -6.1074  ...               0           None  #ff7f00
#    6        gamma   1.16122    -4.57472  ...               0           None  #ffff33
#    7   genextreme   2.08819    -0.10463  ...               0           None  #a65628
#    8      uniform   13.0287          -1  ...               0           None  #f781bf
#    9        expon   16.1383          -1  ...               0           None  #999999
#    10      pareto   16.1408     -430636  ...               0           None  #999999

dist.plot()
plt.show()

#  Figure1.1e ( PETR3_SA_Close_fractional_difference_Linear_filtred pdf)


############################
#                          #
#    Stationarity          #
#        Tests             #
#                          #
############################

# ADF Test
result = adfuller(PETR3_SA_Close_fractional_difference_Linear_filtred_scaled.values, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
# ADF Statistic: -7.039355180539017

print(f'p-value: {result[1]}')
# p-value: 5.901647077829791e-10
# (The time series does not have a unit root (i.e., it is stationary))

for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

#  Critial Values:
#     1%,  -3.4573260719088132
#  Critial Values:
#     5%,  -2.873410402808354
#  Critial Values:
#     10%, -2.573095980841316


# KPSS Test
result = kpss(PETR3_SA_Close_fractional_difference_Linear_filtred_scaled.values, regression='c')
print('\nKPSS Statistic: %f' % result[0])
# KPSS Statistic: 0.173702

print('p-value: %f' % result[1])
# p-value: 0.100000 p-value > 0.05 (To accept the series is non-stationary)

for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')
		
#  Critial Values:
#     10%,  0.347
#  Critial Values:
#     5%,   0.463
#  Critial Values:
#     2.5%, 0.574
#  Critial Values:
#     1%,   0.739


########################*/                      \*########################
#                                                                        #
#         Since KPSS Statistic is lower than all critical values         #
#         above , we accept the null hypothesis the series               #
#         is stationary.                                                 #
#                                                                        #
########################*/                      \*########################

def transform_volume_data(raw_volume_series):
    """
	
    Applies the volume transformation described in Abanto-Valle (2010) paper.
	
	   ABANTO-VALLE, C. A.; MIGON, H. S.; LOPES, H. F. Bayesian modeling of financial returns: a relationship between
	   volatility and trading volume. Applied Stochastic Models in Business and Industry, v. 26, n. 2, p. 172–193, 2010.
	
	Transformation for Stationarity: The raw volume data is first transformed using a custom function called transform_volume_data. 
	This function performs the following steps to ensure stationarity:

       1)  Logarithmic Transformation: 
	   
	       The first step, taking the logarithm of the volume, is crucial. It stabilizes the series' variance, which is a major issue 
		   with volume series (periods of high trading activity have much greater variance). The variance of the log-volume is much more constant.

       2)  Deterministic Trend Removal: 
	   
	       Instead of using a simple differentiation that removes all types of trends, this method regresses the log-volume on a linear trend.
		   This only removes the deterministic trend (a constant upward or downward trend over time), while preserving the stochastic trend and 
		   the short-term dynamics you want to model.

       3)  Structure Preservation: 
	   
	       By taking the exponential of the residuals, you return to the original scale but without the removed linear trend.
    	   The result is a series that maintains its seasonality and volatility clustering dynamics but with a more stable mean and variance, which makes modeling 
		   far more easier.

       4)  Rescaling: 
	   
	       The final step, which rescales the series to match the original mean and standard deviation, is an optional but useful stage. 
		   It ensures the transformed series maintains the same magnitude as the original data, which can make interpreting the forecasting model's results easier.



    Args:
        raw_volume_series (pd.Series or np.array): A series or array of raw trading volume data.

    Returns:
        pd.Series: The transformed volume data.
		
		
    """
    if not isinstance(raw_volume_series, (pd.Series, np.ndarray)):
        raise ValueError("Input must be a pandas Series or numpy array.")
    if isinstance(raw_volume_series, pd.Series):
        raw_volume_series = raw_volume_series.values # Convert to numpy array for consistency
    T = len(raw_volume_series)
    # 1. Take the logarithm of the trading volume
    log_volume = np.log(raw_volume_series)
    # 2. Regress the log-transformed volume on a constant and a time trend
    # Create the design matrix for the regression: [constant, time_trend]
    time_trend = np.arange(1, T + 1)
    X = sm.add_constant(time_trend) # Adds a constant term to the regression
    # Perform the OLS regression
    model = sm.OLS(log_volume, X)
    results = model.fit()
    # 3. Calculate the exponential of the residuals from this regression
    residuals = results.resid
    exp_residuals = np.exp(residuals)
    # 4. Linearly transform this series to have the same mean and variance as the RAW volume data
    # Calculate the mean and standard deviation of the raw volume data
    mean_raw_volume = np.mean(raw_volume_series)
    std_raw_volume = np.std(raw_volume_series)
    # Calculate the mean and standard deviation of the exp_residuals series
    mean_exp_residuals = np.mean(exp_residuals)
    std_exp_residuals = np.std(exp_residuals)
    # Apply the linear transformation: y = a*x + b, where 'x' is exp_residuals
    # We want: mean(y) = mean_raw_volume, std(y) = std_raw_volume
    # So: a = std_raw_volume / std_exp_residuals
    # And: b = mean_raw_volume - a * mean_exp_residuals
    if std_exp_residuals == 0: # Avoid division by zero if all residuals are the same
        # This case is highly unlikely with real data but good to handle
        raw_volume_series = np.full_like(exp_residuals, mean_raw_volume)
    else:
        a = std_raw_volume / std_exp_residuals
        b = mean_raw_volume - a * mean_exp_residuals
        raw_volume_series = a * exp_residuals + b
    # Return as a pandas Series if original input was a Series, maintaining index
    if isinstance(raw_volume_series, pd.Series):
        return pd.Series(raw_volume_series, index=raw_volume_series.index)
    else:
        return raw_volume_series
		
PETR3_SA_Volume = transform_volume_data(PETR3_SA["Volume"])
PETR3_SA_Volume_scaled = function_to_scale_data(PETR3_SA_Volume,0.0001,10)


Linear_Kalman_Filter = """

/*  
   
    References

*   GREWAL, Mohinder S.; ANDREWS, Angus P. Kalman Filtering: Theory and Practice with MATLAB. 
    4. ed. Hoboken: Wiley, 2014.

*   KALMAN, R. E. A New Approach to Linear Filtering and Prediction Problems.
    Journal of Basic Engineering, v. 82, n. 1, p. 35-45, 1960.

*   KALMAN, R. E.; BUCY, R. S. New Results in Linear Filtering and Prediction Theory.
    Journal of Basic Engineering, v. 83, n. 1, p. 95-108, 1961.

*   LABBE, Roger. Kalman and Bayesian Filters in Python. 
    Disponível em: https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python. Acesso em: 19 jun. 2025.

*   ARNOLD, J. [globaltemp.R]. Gist.GitHub,/2012.
    Disponível em: https://gist.github.com/jrnold/4700387. Acesso em: 19 jun. 2025.

estimated with:
  - Covariance filter (no square root or sequential processing)
  - time-invariant parameters
  - initial values, where the y observations are assumed to be normally distributed

*/


data {
    int<lower=1>  n;              // Number of observations (time steps) in the sample. 
    int p;                        // Number of observed variables (dimensions of y).
    int m;                        // Number of state variables (dimensions of x).
    int r;                        // Dimension of the system error equations. r >= m; R: m x r (error loading matrix), Q: r x r (covariance of errors)
    real dt;                      // Time interval (delta t). This can be related to the 'd' (differencing order)
    vector[p] y[n];               // Array of observed vectors. y[i] is the observation at time i.
    matrix[p, m] Z;               // Observation model matrix. Maps state space to observation space (y = Z*x + noise) 
    // System (Process) Equations
    matrix[m, m] Ac;              // Continuous-time system matrix (coefficients)
    matrix[m, r] R;               // System error loading matrix  
    // Initial values for the state mean (x_1) and covariance (P_1) at time t=1.
    vector[m] x_1;
    cov_matrix[m] P_1;            // Initial state covariance matrix. Must be symmetric and positive definite.
}

parameters {
  /* 
     Measurement (Observation) Errors
     These represent the standard deviations of the independent observation noise components.
  */	 
  vector<lower=0.0>[p] log_h_raw; // Sample on the log scale
  /*
     System (Process) Errors
     These represent the standard deviations of the independent process noise components.
  */
  vector<lower=0.0>[r] log_q_raw;
}

transformed parameters {
  vector<lower=0.0>[p] h = exp(log_h_raw); // Transform back to standard deviations
  vector<lower=0.0>[r] q = exp(log_q_raw);
  matrix[p, p] H;         // Observation noise covariance matrix.
  matrix[r, r] Q;         // Process noise covariance matrix
  vector[m] x[n+1];       // Array to store the predicted a priori state means over time (including x[n+1] for the last update).
  matrix[m,m] P[n+1];     // Array to store the predicted a priori state covariance matrices over time.
  matrix[m, m] A;         // Discrete-time system transition matrix.
  real log_lik [n];       // Array to store the log-likelihood of each observation.
  real lp=0;              // Acumulator of the log-likelihood for posterior check
  /*
         Constructing diagonal covariance matrices from standard deviations (h and q).
  */
  H = diag_matrix(h);
  Q = diag_matrix(q);
  /*	
             Initial values    
  */
  x[1] = x_1;        //     Values for the mean states
  P[1] = P_1;        //     Values for the covariance state matrix
  /* 
       Linear approximation/scaling of continuous matrix.	   
  */
  A = (Ac*dt);       //     Euler discretization 
  //
  for (i in 1:n) {
    vector[p] v;             // Innovation (prediction residual).
    matrix[p, p] F;          // Innovation (prediction) covariance matrix.
    matrix[p, p] Finv;       // Inverse of the innovation covariance matrix.
    matrix[m, p] K;          // Kalman Gain matrix.
	// Identity matrix, used in covariance update.
    matrix[m, m] I = diag_matrix(rep_vector(1, m)); 
	/*  predict the next mean states  (a priori) */
    x[i] = A*x[i];  
    /*  predict the next states  covariance matrix (a priori) */ 	
    P[i] = (A*P[i])*A' + (R*Q)*R';  
	/* 
	          measurements  
	*/
	//  Innovation or pre-fit residuals
    v = y[i] - Z * x[i];  
    //  Innovation (or prediction) covariance: uncertainty of the innovation.	
    F = (Z  * P[i]) * Z' + H;
    // Inverse of the innovation covariance matrix (needed for Kalman Gain and likelihood).	
    Finv = inverse(F);
	// Kalman Gain: Blends the prediction and the observation.
    K = (P[i] * Z' )* Finv;
	/*
         	--- Update System State (a posteriori ) ---
                      Kalman Gain 
    */
    P[i+1] = (I-K*Z)*P[i]*(I - K*Z)' + K*H*K';  // P[i+1] becomes the updated P for next step
    x[i+1] = x[i] + K * v;  //  Update the state mean vector (x_posterior) 
    /*      
	         --- Log-Likelihood Calculation ---
	*/
    log_lik[i] = multi_normal_lpdf(y[i]| Z * x[i], F); 
    }
    lp += lp + sum(log_lik);
}

model {
  /*  -----   Ensuring Numerical Stability  -----   */  
  log_h_raw ~ normal(log(0.01), 1.0); // Prior for log(h). Centered around log(0.01)
  log_q_raw ~ normal(log(0.01), 1.0); // Prior for log(q). Scale '1.0' means reasonable spread.
 }
  
"""

Linear_Kalman_Filter = pystan.StanModel(model_code= Linear_Kalman_Filter)

   
x0 = float(PETR3_SA_Volume_scaled[0])

"""
    	
 *   Local Level Model (m=1)
 *   Concept: This model assumes the observed series is driven by a single latent
 *   state, a latent level, which follows a random walk. The observations are
 *   this latent level plus some measurement noise. This is the simplest Kalman
 *   filter model and is an ideal choice for denoising a series that has
 *   already been made stationary, like the transformed volume data.

"""

data_Linear_Kalman_Filter = ({
       'n': len(PETR3_SA_Volume_scaled),
       'p': 1,
       'm': 1,  # Only one latent state: the level
       'r': 1,  # Error dimension is 1
       'dt': 1,
       'y': np.array(PETR3_SA_Volume_scaled).reshape(-1, 1).astype(np.float64),
       'Z': np.array([[1.0]]).astype(np.float64),  # Observation matrix (1x1)
       'Ac': np.array([[1.0]]).astype(np.float64),  # System matrix (1x1), for a random walk
       'R': np.array([[1.0]]).astype(np.float64),  # System error matrix (1x1)
       'x_1': np.array([x0]).astype(np.float64),  # Initial state vector (1x1)
       'P_1': matrix_to_symmetric_matrix(np.array([[1.0]])).astype(np.float64)  # Initial covariance matrix (1x1)
})
		  
control = {}
control['max_treedepth'] = 11
control['adapt_delta'] = 0.9999

fit_Linear_Kalman_Filter = Linear_Kalman_Filter.sampling(data=data_Linear_Kalman_Filter, iter=15000,  chains=1, warmup=7000 , thin=1, seed=101, n_jobs = 1,  control=control)


#   [distfit] >WARNING> Maximum (flat) parameter count (1000) exceeded: skipping diagnostic tests for n_eff and Rhat.
#   To run all diagnostics call pystan.check_hmc_diagnostics(fit)


# Define the directory and filename
save_directory = r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA'
filename = 'Linear_Kalman_Filter_volume.pkl'

# Construct the full file path
full_path = os.path.join(save_directory, filename)

# Save the model
with open(full_path, 'wb') as f:
    pickle.dump(fit_Linear_Kalman_Filter, f)

# Load the model
with open(full_path, 'rb') as f:
    Linear_Kalman_Filter_volume = pickle.load(f)

Linear_Kalman_Filter = az.from_pystan(fit_Linear_Kalman_Filter, log_likelihood="log_lik", observed_data={"y": ['np.array(PETR3_SA_Volume_scaled).reshape(-1, 1).astype(np.float64)']})
# --- Calculate WAIC ---
waic_result = az.waic(Linear_Kalman_Filter)

print("\n--- WAIC Results ---")
print(waic_result)


###################################*/                      \*###################################
#
#    Computed from 8000 by 750 log-likelihood matrix
#
#              Estimate       SE
#    elpd_waic  -448.60    16.50
#    p_waic        4.91        -
#
#    There has been a warning during the calculation. Please check the results.
#
#
###################################*/                      \*###################################


# Data extraction section
samples = fit_Linear_Kalman_Filter.extract()
x_samples = samples["x"]

# --- State x1 (Level) ---
# The shape of x_samples is now (n_draws, n_steps, n_states), so the state index is 0.
# The [:, :, 0] selects the first and only state dimension.
# `x_samples[:, 1:len(PETR3_SA_Volume_scaled)+1, 0]` is still correct as it accesses the first state dimension.
x1_smoothed_mean = x_samples[:, 1:len(PETR3_SA_Volume_scaled)+1, 0].mean(axis=0)

# --- Calculate credibility intervals for x1 ---
x1_lower, x1_upper = np.percentile(x_samples[:,1:len(PETR3_SA_Volume_scaled)+1, 0], [2.5, 97.5], axis=0)

# Convert the date series to the plotting format, keeping the structure of your line.
PETR3_SA_Dates_volume = pd.to_datetime(PETR3_SA_Dates.squeeze())


# --- Plotting ---
# Create a figure with only one subplot.
# fig, ax1 = plt.subplots(figsize=(14, 10)) # Option for a single subplot
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(14, 10)) # Or maintain the `axes` structure
ax1 = axes

# --- Plot State 1 (Level) ---
ax1.plot(PETR3_SA_Dates_volume, function_to_scale_data(np.array(PETR3_SA_Volume_scaled).reshape(-1, 1).astype(np.float64),0.0001,10), label='Observed volume (v)', color='grey', alpha=0.7)
ax1.plot(PETR3_SA_Dates_volume, x1_smoothed_mean, label='Estimated State 1 (Latent Level)', color='blue', linewidth=2)
ax1.fill_between(PETR3_SA_Dates_volume, x1_lower, x1_upper, color='blue', alpha=0.2, label='95% Credible Interval (State 1)')
ax1.set_ylabel('Value')
ax1.set_title('Observed Volume vs. Estimated Latent Level ($x_1$)')
ax1.legend()
ax1.grid(True, linestyle='--', alpha=0.6)
ax1.set_xlabel('Date') # Add the X-axis label, since it's now a single subplot

# --- Enhance Date Formatting (Optional but recommended for long series) ---
fig.autofmt_xdate() # Automatically formats date labels to prevent overlap
plt.tight_layout() # Adjusts plot to prevent labels overlapping
plt.show()

#  Figure_1.1f ( Denoised volume)

pd.DataFrame(x1_smoothed_mean).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\PETR3_SA_volume_Linear_filtred.csv', index=None)
PETR3_SA_volume_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_volume_Linear_filtred.csv")

############################
#                          #
#    Stationarity          #
#        Tests             #
#                          #
############################

# ADF Test
result = adfuller(PETR3_SA_volume_Linear_filtred, autolag='AIC')
print(f'ADF Statistic: {result[0]}')
# ADF Statistic: -8.04448245071588

print(f'p-value: {result[1]}')
# p-value: 1.8099380150920225e-12
# (The time series does not have a unit root (i.e., it is stationary))

for key, value in result[4].items():
    print('Critial Values:')
    print(f'   {key}, {value}')

#  Critial Values:
#     1%,  -3.456780859712
#  Critial Values:
#     5%,  -2.8731715065600003
#  Critial Values:
#     10%, -2.572968544


# KPSS Test
result = kpss(PETR3_SA_Volume_scaled, regression='c')
print('\nKPSS Statistic: %f' % result[0])
# KPSS Statistic: 0.072925

print('p-value: %f' % result[1])
# p-value: 0.100000 p-value > 0.05 (To accept the series is non-stationary)

for key, value in result[3].items():
    print('Critial Values:')
    print(f'   {key}, {value}')
		
#  Critial Values:
#     10%,  0.347
#  Critial Values:
#     5%,   0.463
#  Critial Values:
#     2.5%, 0.574
#  Critial Values:
#     1%,   0.739

# Calling distfit library to identify which pdf fits the data the most 
dist = distfit()

# Getting the best pdf 
dist.fit_transform(PETR3_SA_volume_Linear_filtred.values)
print(dist.summary)

#            name      score       loc  ... bootstrap_score bootstrap_pass    color
#  0   genextreme  0.0493141    1.3321  ...               0           None  #e41a1c
#  1      lognorm  0.0745611   0.15862  ...               0           None  #e41a1c
#  2        gamma   0.127764  0.356396  ...               0           None  #377eb8
#  3         beta   0.129292  0.358475  ...               0           None  #4daf4a
#  4     dweibull   0.148719   1.48671  ...               0           None  #984ea3
#  5            t   0.189039   1.55497  ...               0           None  #ff7f00
#  6         norm   0.409872   1.74797  ...               0           None  #ffff33
#  7     loggamma   0.417209  -221.465  ...               0           None  #a65628
#  8        expon   0.795487  0.423217  ...               0           None  #f781bf
#  9      uniform    1.21601  0.423217  ...               0           None  #999999
#  10      pareto    1.42649 -0.956265  ...               0           None  #999999

dist.plot()
plt.show() 

#  Figure1.1g( PETR3_SA_Volume_Linear_filtred pdf)

##############################################################################
#                                Data Partitioning                           #
#                                                                            #
# For time series data, due to interdependence and autocorrelation,          #
# only the last 'forecasting_extrapolation_length' observations are reserved #
# for the test set partition.                                                #
##############################################################################

forecasting_extrapolation_lengh=5

PETR3_SA_Close_fractional_difference_Linear_filtred_train  =  PETR3_SA_Close_fractional_difference_Linear_filtred_scaled[:-forecasting_extrapolation_lengh]
PETR3_SA_Close_fractional_difference_Linear_filtred_test   =  PETR3_SA_Close_fractional_difference_Linear_filtred_scaled[-forecasting_extrapolation_lengh:]

PETR3_SA_volume_Linear_filtred_train =  PETR3_SA_volume_Linear_filtred[:-forecasting_extrapolation_lengh]  
PETR3_SA_volume_Linear_filtred_test  =  PETR3_SA_volume_Linear_filtred[-forecasting_extrapolation_lengh:]

sm.graphics.tsa.plot_pacf((PETR3_SA_Close_fractional_difference_Linear_filtred_train), lags=50, method="ywm") # p = 2
# Figure_1.1h ( PETR3_SA_Close_fractional_difference_Linear_filtred_train PACF)
sm.graphics.tsa.plot_acf((PETR3_SA_Close_fractional_difference_Linear_filtred_train), lags=50)  # q = 2
# Figure_1.1i ( PETR3_SA_Close_fractional_difference_Linear_filtred_train ACF)
plt.show()

sm.graphics.tsa.plot_pacf((PETR3_SA_volume_Linear_filtred_train), lags=50, method="ywm") # p = 1
# Figure_1.1j (PETR3_SA_Volume_train PACF)
sm.graphics.tsa.plot_acf((PETR3_SA_volume_Linear_filtred_train), lags=50)  # q = 1   
# Figure_1.1l (PETR3_SA_Volume_train ACF)
plt.show()

#############################/*                               *\##################################
#
#                                   Spectogram Analysis 
#
#  Reference: 
#                                                
#    WOOLF, Peter. Noise Modeling: White, Pink, and Brown Noise, Pops and Crackles.
#    In: WOOLF, Peter. Chemical Process Dynamics and Controls. [S. l.]: LibreTexts,.       
#    Capítulo 2, Seção 2.5. Disponível em: 
#    https://eng.libretexts.org/Bookshelves/Industrial_and_Systems_Engineering/Chemical_Process_Dynamics_and_Controls_(Woolf)/02%3A_Modeling_Basics/2.05%3A_Noise_modeling-more_detailed_information_on_noise_modeling-_white%2C_pink%2C_and_brown_noise%2C_pops_and_crackles.           
#    Acesso em: 20 jun. 2025.                                   
#
#
#############################/*                                *\##################################

# Obtaining the sampling rate through the first two observations of the log returns series for the training sample
inter_sample_time = PETR3_SA_Close_fractional_difference_Linear_filtred_train[2] - PETR3_SA_Close_fractional_difference_Linear_filtred_train[1]
print('inter_sample_time =', inter_sample_time, 'd')
# inter_sample_time = [0.33478232] d

# Check if they have the same size
print('Same size?:', np.allclose(PETR3_SA_Close_fractional_difference_Linear_filtred_train, inter_sample_time))
# Same size?: False. It is not a Uniform White Noise (Infinite) 

print('n_samples =', len(PETR3_SA_Close_fractional_difference_Linear_filtred_train))
# n_samples = 242
n_samples = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)
sampling_freq =  1 / inter_sample_time

# Obtaining the frequencies of the spectrogram
f = np.fft.fftfreq(len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)) * sampling_freq

# Obtaining the power spectral density (PSD) of the spectrogram
psd = np.abs(np.fft.fft(PETR3_SA_Close_fractional_difference_Linear_filtred_train))**2 / len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)

# Set a more elegant style
sns.set(style="whitegrid")

# Plot settings
plt.figure(figsize=(12, 6))
plt.plot(f[f >= 0], psd[f >= 0], color='darkblue', linewidth=2)

# Add labels and title
plt.title('Power Spectral Density of Filtered Log Returns', fontsize=16, fontweight='bold')
plt.xlabel('Frequency (Hz)', fontsize=14)
plt.ylabel('Power Spectral Density (PSD)', fontsize=14)

# Improve tick format
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Add horizontal grid lines for readability
plt.grid(True, which='major', linestyle='--', linewidth=0.5)

# Shading key frequency regions
plt.axvspan(0.01, 0.1, color='orange', alpha=0.1, label='Low-Frequency Band')
plt.legend()

# Set margins to avoid cutting labels
plt.margins(0.02)

# Tight layout and show
plt.tight_layout()
plt.show()

# Figure_1.1m ( PETR3_SA_Close_fractional_difference_Linear_filtred_train PSD)


#############################/*                               *\##################################
#
#      Linear Kalman Filter Action: 
#	  
#	   The sharp drop of the dark blue line in the low-frequency band
#      (the orange shaded area) is a clear sign that the high-pass filter  used
#      to remove long-term trends is working as intended. It has successfully
#      removed the low-frequency components (the trends).
#
#      X-axis (Frequency): 
#	   
#	   The horizontal axis represents frequency in Hertz (Hz), which corresponds to the number of cycles per unit of time.
#	   Lower frequencies (closer to 0) represent long-term trends or cycles, while higher frequencies represent shorter, more rapid fluctuations.
#
#      Y-axis (Power Spectral Density): 
#	   
#	   The vertical axis, PSD, measures the concentration of power per unit of frequency. Peaks in the PSD indicate that a significant amount
#	   of the signal's power is concentrated at that specific frequency, suggesting a strong periodic component.
#
#      Dominant Peaks: 
#	   
#	   The most prominent feature is the large spike around 0.6 Hz. This indicates a very strong periodic signal in the filtered log returns at this frequency. 
#	   This suggests that there's a recurring pattern or cycle in the data that repeats approximately 0.6 times per unit of time.
#
#      Low-Frequency Band: 
#	   
#	   The highlighted yellow region on the left (from 0 to roughly 0.1 Hz) is the "Low-Frequency Band." 
#	   This area represents the long-term behavior of the time series. The PSD is relatively low in this band,
#	   suggesting that there are no strong, slow-moving trends or long-term cycles in the data.
#
#      High-Frequency Peaks: 
#	  
#	   The graph shows several tall peaks. This indicates that most of the power
# 	   (energy) in your log-returns series is concentrated at higher frequencies.
#	   This is expected, as log-returns are dominated by rapid, short-term fluctuations. 
#	   The highest peak (around 0.6 Hz) points to a dominant frequency at which the series varies.
#
#      Other Peaks: 
#	   
#	   There are other, smaller peaks visible, such as the one around 1.35 Hz. While not as dominant 
#	   as the peak at 0.6 Hz, these still represent other, less pronounced periodicities in the data.
#
#      In summary, the plot reveals that the most significant periodic behavior in the filtered 
#	   log returns of PETR3_SA is a high-frequency cycle around 0.6 Hz.
#	  
#############################/*                               *\##################################


frequency = np.log(abs(f.flatten()))
frequency[np.isneginf(frequency)] = np.mean(frequency)
frequency = pd.DataFrame(frequency)
frequency = frequency.replace([np.inf, -np.inf], 0).fillna(0)
psd = np.log(abs(psd.flatten())).reshape(-1, 1)
psd[np.isneginf(psd)] = np.mean(psd)
psd = pd.DataFrame(psd)
psd = psd.replace([np.inf, -np.inf], 0).fillna(0)
reg = LinearRegression().fit(frequency,psd)
print("Slope (regression coefficient):", reg.coef_[0])
#  Slope (regression coefficient): [-0.08320246]

#  |   Slope ≈ | Interpretation   |
#  |-----------|------------------|
#  |         0 | White noise      |
#  |        -1 | Pink noise       |
#  |        -2 | Brown noise      |
#  |         1 | Blue noise       |
#  |         2 | Violet noise     |


# Same here with log-volume 

# Obtaining the sampling rate through the first two observations of the log returns series for the training sample
inter_sample_time = PETR3_SA_volume_Linear_filtred_train.values[2] - PETR3_SA_volume_Linear_filtred_train.values[1]
print('inter_sample_time =', inter_sample_time, 'd')
# inter_sample_time = [1.38133468] d


# Check if they have the same size
print('Same size?:', np.allclose(PETR3_SA_volume_Linear_filtred_train.values, inter_sample_time))
# Same size?: False. It is not a Uniform White Noise (Infinite) 

print('n_samples =', len(PETR3_SA_volume_Linear_filtred_train.values))
# n_samples = 246
n_samples = len(PETR3_SA_volume_Linear_filtred_train.values)
sampling_freq =  1 / inter_sample_time

# Obtaining the frequencies of the spectrogram
f = np.fft.fftfreq(len(PETR3_SA_volume_Linear_filtred_train.values)) * sampling_freq

# Obtaining the power spectral density (PSD) of the spectrogram
psd = np.abs(np.fft.fft(PETR3_SA_volume_Linear_filtred_train.values))**2 / len(PETR3_SA_volume_Linear_filtred_train.values)

# Set a more elegant style
sns.set(style="whitegrid")

# Plot settings
plt.figure(figsize=(12, 6))
plt.plot(f[f >= 0], psd[f >= 0], color='darkblue', linewidth=2)

# Add labels and title
plt.title('Power Spectral Density of Filtered Log Volume', fontsize=16, fontweight='bold')
plt.xlabel('Frequency (Hz)', fontsize=14)
plt.ylabel('Power Spectral Density (PSD)', fontsize=14)

# Improve tick format
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Add horizontal grid lines for readability
plt.grid(True, which='major', linestyle='--', linewidth=0.5)

# Shading key frequency regions
plt.axvspan(0.01, 0.1, color='orange', alpha=0.1, label='Low-Frequency Band')
plt.legend()

# Set margins to avoid cutting labels
plt.margins(0.02)

# Tight layout and show
plt.tight_layout()
plt.show()

# Figure_1.1n ( PETR3_SA_volume_Linear_filtred_train PSD)

#############################/*                               *\##################################
#
#      
#	  Filter Action (Low-Frequency Band): 
#
#     X-axis (Frequency): 
#	  
#	  The horizontal axis is frequency in Hertz (Hz), representing cycles per unit of time. The plot ranges from 0 to 0.35 Hz.
#
#     Y-axis (Power Spectral Density):
#
#	  The vertical axis, PSD, measures the power concentration at each frequency. Higher peaks indicate stronger periodic components.
#
#     Dominant Peak: 
#	  
#	  The most prominent feature is the large spike around 0.16 Hz. This signifies a very strong periodic signal in the filtered log volume at this frequency. 
#	  This suggests a significant and regular cycle in trading volume.
#
#     Low-Frequency Band: 
#	  
#	  The highlighted yellow region on the left (from 0 to roughly 0.1 Hz) is the "Low-Frequency Band."
#	  The PSD is relatively high in this region, with a notable peak near 0.015 Hz.
#	  Suggesting the presence of a strong long-term trend or very slow cycle in the trading volume. 
#	  This contrasts with the previous plot where the low-frequency band was relatively flat.
#
#     Other Peaks: 
#	  
#	  There are other, smaller peaks visible, such as those around 0.05 Hz and 0.25 Hz.
#	  These indicate less dominant but still present periodicities in the volume data.
#
#     In summary, this PSD plot reveals that the filtered log volume has both a strong, 
#	  slow-moving cycle (around 0.015 Hz) and a more pronounced, higher-frequency cycle (around 0.16 Hz).
#	  
#	  
#############################/*                               *\##################################


frequency = np.log(abs(f.flatten()))
frequency[np.isneginf(frequency)] = np.mean(frequency)
frequency = pd.DataFrame(frequency)
frequency = frequency.replace([np.inf, -np.inf], 0).fillna(0)
psd = np.log(abs(psd.flatten())).reshape(-1, 1)
psd[np.isneginf(psd)] = np.mean(psd)
psd = pd.DataFrame(psd)
psd = psd.replace([np.inf, -np.inf], 0).fillna(0)
reg = LinearRegression().fit(frequency,psd)
print("Slope (regression coefficient):", reg.coef_[0])
#  Slope (regression coefficient): [-0.09347325]


#  |   Slope ≈ | Interpretation   |
#  |-----------|------------------|
#  |         0 | White noise      |
#  |        -1 | Pink noise       |
#  |        -2 | Brown noise      |
#  |         1 | Blue noise       |
#  |         2 | Violet noise     |

#  (Weakly white noise)

def get_top_periodic_components(series, top_n=3):
    """
    Estimate the power spectral density (PSD) using a periodogram,
    to identify the top N dominant frequencies (excluding zero),
    and return their corresponding periods and power levels.

    Parameters:
        series (array-like): 1D time series data
        top_n (int): Number of dominant components to return (default is 3)

    Returns:
        dict: Dictionary containing top N periods, frequencies, and powers
    """
    # Ensure 1D input
    series = np.squeeze(series)
	#
    # Compute the periodogram (frequency and power)
	#
    f, Pxx = signal.periodogram(series)
	#
    # Exclude the zero frequency to avoid division by zero (inf periods)
	#
    nonzero_mask = f > 0
    f = f[nonzero_mask]
    Pxx = Pxx[nonzero_mask]
	#
    # Get indices of top N power peaks
	#
    top_indices = np.flip(np.argsort(Pxx))[:top_n]
	#
    # Extract corresponding frequencies, powers, and compute periods
	#
    top_freqs = f[top_indices]
    top_power = Pxx[top_indices]
    top_periods = 1 / top_freqs
	#
    # Build result dictionary
	#
    top_components = {}
    for i in range(top_n):
        top_components[f'period{i+1}'] = top_periods[i]
        top_components[f'freq{i+1}'] = top_freqs[i]
        top_components[f'power{i+1}'] = top_power[i]
    return top_components


# Get top 3 cyclical components
top_cycles = get_top_periodic_components(PETR3_SA_Close_fractional_difference_Linear_filtred_train.squeeze())

print(top_cycles)


#   { 'period1': 2.033613445378151, 'freq1': 0.49173553719008267, 'power1': 0.8421447739014166,
#     'period2': 34.57142857142857, 'freq2': 0.028925619834710745, 'power2': 0.776671331150416,
#     'period3': 30.25, 'freq3': 0.03305785123966942, 'power3': 0.6640507570769116}

# Period 1 = 2.03
# This is a very short-term cycle, likely related to noise or market microstructure effects
# such as daily fluctuations or autocorrelation artifacts.
# While it shows strong spectral power, it may not be economically meaningful
# unless you're working with high-frequency (e.g., intraday) data.

# Period 2 = 34.57
# This suggests a more stable and interpretable cycle.
# Since the time series is daily, this could correspond to a ~1.5 month pattern.
# It may reflect medium-term investor behavior, reporting cycles, or macroeconomic influences.

# Period 3 = 30.25
# Also close to a monthly frequency — strengthens the evidence for a ~30–35-day cycle.
# May capture recurring patterns like monthly fund flows, economic releases, or trading strategies.

frequencies = [1/30.25, 1/34.57]
print(len(frequencies))
# 2

#  The same here for volume 

# Get top 3 cyclical components
top_cycles = get_top_periodic_components(PETR3_SA_Volume_train.squeeze())

print(top_cycles)

#   {'period1': 61.49999999999999, 'freq1': 0.016260162601626018, 'power1': 14.618663796561565, 
#    'period2': 41.0, 'freq2': 0.024390243902439025, 'power2': 12.532665976445577,
#	 'period3': 6.833333333333332, 'freq3': 0.14634146341463417, 'power3': 10.280020917906102}



# Period 1 = 61.50 days
# This represents the strongest cyclical component for trading volume, suggesting a recurring 
# pattern approximately every two months (or 60-61 days). This could be related to:
# Quarterly or Bi-Monthly Reporting Cycles: Companies often release financial reports or hold investor calls on a quarterly basis.
# Volume might increase significantly around these events.

# Macroeconomic Data Releases: Certain economic indicators are released on a bi-monthly schedule, 
# which can influence trading activity.

# Longer-Term Investor Behavior: Institutional investors or larger market participants might 
# rebalance portfolios or make strategic decisions on a bi-monthly cadence.


# Period 2 = 41.00 days
# This indicates another significant cyclical pattern in trading volume, occurring roughly every month 
# and a half (or 40-41 days). This could be linked to:

# Monthly Market Events: Similar to the PETR3_SA_Close analysis, this period could reflect monthly economic data releases,
# index rebalancings, or expiration of derivatives contracts, all of which can affect trading volume.

# Investor Sentiment Cycles: Medium-term cycles in investor optimism or pessimism that influence buying and selling activity.



# Period 3 = 6.83 days
# This is a relatively short-term cycle, very close to seven days. This strongly suggests a weekly periodicity in trading volume. 
# This is a very common and expected pattern in financial markets:

# Day-of-the-Week Effects: Trading volume often shows systematic differences on certain days of the week (e.g., higher volume on Mondays or Fridays, lower volume mid-week).
# This cycle is almost certainly capturing these regular weekly fluctuations.

# Trading Hour Consistency: The consistent weekly pattern implies that market participants' activity levels follow a
# predictable rhythm throughout the trading week.
# This cycle is highly interpretable and crucial for understanding daily volume dynamics.

frequencies_volume = [1/61.50]
print(len(frequencies_volume))
# 1


#########################    
#                       #     
#   Extreme Values      #
#             Analysis  #     
#                       #      
#########################


#   =========================================================
#           1. Fitting  Peaks Over Threshold (POT) Model
#   =========================================================

#   ========================================================= 
#   Data truction to fit log returns length
#   =========================================================


# Assuming PETR3_SA_Dates and PETR3_SA_Volume_train are defined and PETR3_SA_Dates has 'Date' as its index.
Time_length_to_trim = len(PETR3_SA_Dates) - len(PETR3_SA_Volume_train)

# Get the dates directly from the index of the trimmed DataFrame
dates = PETR3_SA_Dates.index[Time_length_to_trim:]

# Now, create the new Series with the correct index
PETR3_SA_Volume_train = pd.Series(PETR3_SA_Volume_train, index=dates)

# Print the head of the new Series to verify the result
print(PETR3_SA_Volume_train.head())

PETR3_SA_Volume_train = pd.Series(PETR3_SA_Volume_train, index=dates)


#   =========================================================
#   Creating an EVA model
#   =========================================================

model = EVA(PETR3_SA_Volume_train)

#   =========================================================
#   Threshold = 95th percentile
#   =========================================================

model.get_extremes(method="POT", threshold=PETR3_SA_Volume_train.quantile(0.95))

#   =========================================================
#   Fit a GPD or GEV model (default auto-selects best)
#   =========================================================

model.fit_model()

#   =================================================================
#   Print summary including return values for typical return periods
#   =================================================================


summary = model.get_summary(return_period=[10, 30, 90, 365], alpha=0.95)
print(summary)

#                  return value  lower ci       upper ci
#   return period
#   10                16.550194  6.103648     391.915540
#   30                23.112905  6.109009    2424.693484
#   90                32.346000  6.110625   15111.389522
#   365               49.747146  6.111182  155894.258181

#   =========
#   Plotting 
#   =========

plt.figure(figsize=(12, 4))
plt.plot(PETR3_SA_Volume_train, label='Volume')
thr = model.extremes.min()  # or PETR3_SA_Volume_train.quantile(0.95)
plt.axhline(thr, color='red', linestyle='--', label='Threshold')
plt.scatter(model.extremes.index, model.extremes, color='orange', label='Extracted Extremes')
plt.legend()
plt.title("Volume, Threshold, and Extracted Peaks")
plt.xlim(pd.Timestamp('2024-06-20'), pd.Timestamp('2025-06-13'))
plt.show()      # Figure1.1n


#    ===================
#    Frequency analysis
#    ===================

extremes = model.extremes
threshold_value = thr  # unify naming

# ===================
# 1. Frequency analysis
# ===================
n_extremes = len(extremes)
mean_excess = (extremes - threshold_value).mean()
median_excess = (extremes - threshold_value).median()

# Interarrival times in days
interarrival = extremes.index.to_series().diff().dt.days.dropna()
mean_gap = interarrival.mean()
min_gap = interarrival.min()

print("===== Frequency analysis =====")
#    ===== Frequency analysis =====
print(f"Number of peaks: {n_extremes}")
#    Number of peaks: 12

print(f"Mean excess: {mean_excess:.2f}")
#    Mean excess: 1.54

print(f"Median excess: {median_excess:.2f}")
#    Median excess: 0.91

print(f"Mean gap between peaks: {mean_gap:.1f} days")
#    Mean gap between peaks: 29.0 days

print(f"Min gap between peaks: {min_gap} days")
#    Min gap between peaks: 2.0 days


#     ==============================
#      Tail heaviness estimation (Xi)
#     ==============================


exceedances = extremes - threshold_value
shape, loc, scale = genpareto.fit(exceedances, floc=0)

print("\n===== Tail heaviness =====")
#    ===== Tail heaviness =====

print(f"Estimated GPD shape (ξ): {shape:.3f}")
#    Estimated GPD shape (ξ): 0.377

print(f"Estimated GPD scale (σ): {scale:.3f}")
#    Estimated GPD scale (σ): 1.027


if shape > 0.5:
     print("Very heavy tail: large jumps are relatively common")
elif shape > 0:
     print("Moderate heavy tail: jumps possible but less frequent")
else:
     print("Thin tail: jumps are rare and bounded")
	
#    Moderate heavy tail: jumps possible but less frequent

#      ===================
#      Exceedance rate
#      ===================


total_days = len(PETR3_SA_Volume_train)
freq_per_day = n_extremes / total_days
avg_gap_days = mean_gap  # already computed above

print("\n===== Exceedance rate =====")
#     ===== Exceedance rate =====

print(f"Number of spikes: {n_extremes}")
#     Number of spikes: 12

print(f"Average gap between spikes: {avg_gap_days:.2f} days")
#     Average gap between spikes: 29.00 days

print(f"Empirical exceedance rate: {freq_per_day:.4f} per day")
#     Empirical exceedance rate: 0.0488 per day


#       =========================================
#         Recommend Spike-and-Slab prior settings
#       =========================================


spike_prob = 1 - freq_per_day
slab_var = np.var(exceedances)
baseline_vol = PETR3_SA_Volume_train[PETR3_SA_Volume_train <= threshold_value]
spike_var = np.var(baseline_vol)

print("\n===== Recommended Spike-and-Slab priors =====")
#     ===== Recommended Spike-and-Slab priors =====

print(f"Spike variance σ_spike² ≈ {spike_var:.2e}")
#     Spike variance σ_spike² ≈ 6.25e-01

# Obs:    baseline (no-jump) variance ≈ 0.625 ⇒ baseline sd ≈ √0.625 = 0.79.

print(f"Slab variance σ_slab²  ≈ {slab_var:.2e}")
#     Slab variance σ_slab²  ≈ 3.26e+00

# Obs:    jump (slab) variance ≈ 3.26 ⇒ slab sd ≈ √3.26 = 1.81.

print(f"Spike probability π_spike ≈ {spike_prob:.3f}")
#     Spike probability π_spike ≈ 0.951

# Obs:    probability of no jump ≈ 95.1% , so jump probability is
#         p_jump ≈ 1 − 0.951 = 0.049 (≈4.9%).


#   Same here with log returns 


#   =========================================================
#           1. Fitting  Peaks Over Threshold (POT) Model
#   =========================================================

#   ========================================================= 
#   Data truction to fit log returns length
#   =========================================================

# Assuming PETR3_SA_Dates and PETR3_SA_Close_fractional_difference_Linear_filtred_train  are defined and PETR3_SA_Dates has 'Date' as its index.
Time_length_to_trim = len(PETR3_SA_Dates) - len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)

# Get the dates directly from the index of the trimmed DataFrame
dates = PETR3_SA_Dates.index[Time_length_to_trim:]

# Now, create the new Series with the correct index
PETR3_SA_Close_fractional_difference_Linear_filtred_train = pd.Series(PETR3_SA_Close_fractional_difference_Linear_filtred_train.flatten(), index=dates)

# Print the head of the new Series to verify the result
print(PETR3_SA_Close_fractional_difference_Linear_filtred_train.head())

PETR3_SA_Close_fractional_difference_Linear_filtred_train = pd.Series(PETR3_SA_Close_fractional_difference_Linear_filtred_train, index=dates)


#   =========================================================
#   Creating an EVA model
#   =========================================================

model = EVA(PETR3_SA_Close_fractional_difference_Linear_filtred_train)

#   =========================================================
#   Threshold = 95th percentile
#   =========================================================

model.get_extremes(method="POT", threshold=PETR3_SA_Close_fractional_difference_Linear_filtred_train.quantile(0.95))

#   =========================================================
#   Fit a GPD or GEV model (default auto-selects best)
#   =========================================================

model.fit_model()

#   =================================================================
#   Print summary including return values for typical return periods
#   =================================================================


summary = model.get_summary(return_period=[10, 30, 90, 365], alpha=0.95)
print(summary)

#                  return value  lower ci     upper ci
#   return period
#   10                 1.837380  0.512619    37.964468
#   30                 2.991397  0.529009    241.700568
#   90                 4.937471  0.533414   1596.364989
#   365                9.462508  0.537002  18368.262294

#   =========
#   Plotting 
#   =========

plt.figure(figsize=(12, 4))
plt.plot(PETR3_SA_Close_fractional_difference_Linear_filtred_train, label='Log Returns')
thr = model.extremes.min()  # or PETR3_SA_Close_fractional_difference_Linear_filtred_train.quantile(0.95)
plt.axhline(thr, color='red', linestyle='--', label='Threshold')
plt.scatter(model.extremes.index, model.extremes, color='orange', label='Extracted Extremes')
plt.legend()
plt.title("Volume, Threshold, and Extracted Peaks")
plt.xlim(pd.Timestamp('2024-06-20'), pd.Timestamp('2025-06-13'))  
plt.show()    # Figure_1.1o    


#    ===================
#    Frequency analysis
#    ===================

extremes = model.extremes
threshold_value = thr  # unify naming

# ===================
# 1. Frequency analysis
# ===================
n_extremes = len(extremes)
mean_excess = (extremes - threshold_value).mean()
median_excess = (extremes - threshold_value).median()

# Interarrival times in days
interarrival = extremes.index.to_series().diff().dt.days.dropna()
mean_gap = interarrival.mean()
min_gap = interarrival.min()

print("===== Frequency analysis =====")
#    ===== Frequency analysis =====
print(f"Number of peaks: {n_extremes}")
#    Number of peaks: 13

print(f"Mean excess: {mean_excess:.2f}")
#    Mean excess: 0.13

print(f"Median excess: {median_excess:.2f}")
#    Median excess: 0.06

print(f"Mean gap between peaks: {mean_gap:.1f} days")
#    Mean gap between peaks: 22.8 days

print(f"Min gap between peaks: {min_gap} days")
#    Min gap between peaks: 2.0 days


#     ==============================
#      Tail heaviness estimation (Xi)
#     ==============================


exceedances = extremes - threshold_value
shape, loc, scale = genpareto.fit(exceedances, floc=0)

print("\n===== Tail heaviness =====")
#    ===== Tail heaviness =====

print(f"Estimated GPD shape (ξ): {shape:.3f}")
#    Estimated GPD shape (ξ): 0.482

print(f"Estimated GPD scale (σ): {scale:.3f}")
#    Estimated GPD scale (σ): 0.077


if shape > 0.5:
     print("Very heavy tail: large jumps are relatively common")
elif shape > 0:
     print("Moderate heavy tail: jumps possible but less frequent")
else:
     print("Thin tail: jumps are rare and bounded")
	
#    Moderate heavy tail: jumps possible but less frequent

#      ===================
#      Exceedance rate
#      ===================


total_days = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)
freq_per_day = n_extremes / total_days
avg_gap_days = mean_gap  # already computed above

print("\n===== Exceedance rate =====")
#     ===== Exceedance rate =====

print(f"Number of spikes: {n_extremes}")
#     Number of spikes: 13

print(f"Average gap between spikes: {avg_gap_days:.2f} days")
#     Average gap between spikes: 22.75 days

print(f"Empirical exceedance rate: {freq_per_day:.4f} per day")
#     Empirical exceedance rate: 0.0537 per day


#       =========================================
#         Recommend Spike-and-Slab prior settings
#       =========================================


spike_prob = 1 - freq_per_day
slab_var = np.var(exceedances)
baseline_vol = PETR3_SA_Close_fractional_difference_Linear_filtred_train[PETR3_SA_Close_fractional_difference_Linear_filtred_train <= threshold_value]
spike_var = np.var(baseline_vol)

print("\n===== Recommended Spike-and-Slab priors =====")
#     ===== Recommended Spike-and-Slab priors =====

print(f"Spike variance σ_spike² ≈ {spike_var:.2e}")
#     Spike variance σ_spike² ≈ 4.16e-02  0.0416

# Obs:    baseline (no-jump) variance ≈ 0.0416 ⇒ baseline sd ≈ √0.0416 = 0.20.

print(f"Slab variance σ_slab²  ≈ {slab_var:.2e}")
#     Slab variance σ_slab²  ≈ 3.24e-02

# Obs:    jump (slab) variance ≈ 0.0326 ⇒ slab sd ≈ √0.0326 = 0.18.

print(f"Spike probability π_spike ≈ {spike_prob:.3f}")
#     Spike probability π_spike ≈ 0.946

# Obs:    probability of no jump ≈ 94.6% , so jump probability is
#         p_jump ≈ 1 − 0.946 = 0.054 (≈5.4%).


#########################    
#                       #     
#     Days of           #
#         the week      #     
#                       #      
#########################


#  Calling the original dates 
PETR3_SA_Dates = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Dates.csv",header=None,parse_dates=[0])
print(PETR3_SA_Dates.head())

#        0
#  0  2024-06-13
#  1  2024-06-14
#  2  2024-06-17
#  3  2024-06-18
#  4  2024-06-19


# Renaming the column and converting to datetime objects
PETR3_SA_Dates.rename(columns={0: 'Date'}, inplace=True)
PETR3_SA_Dates['Date'] = pd.to_datetime(PETR3_SA_Dates['Date'], errors='coerce')

# Drop rows where 'Date' is NaT
PETR3_SA_Dates.dropna(subset=['Date'], inplace=True)

# Getting the day of the week column name and setting the proper order
PETR3_SA_Dates['DayOfWeek'] = PETR3_SA_Dates['Date'].dt.day_name()
day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']

# Converting 'DayOfWeek' column to a Categorical type with the specified order
PETR3_SA_Dates['DayOfWeek'] = pd.Categorical(PETR3_SA_Dates['DayOfWeek'],
                                             categories=day_order, ordered=True)

# Set the 'Date' column as the DataFrame's index.
PETR3_SA_Dates.set_index('Date', inplace=True)

# Creating dummy variables (one-hot encoding) for 'DayOfWeek'
day_of_week_dummies = pd.get_dummies(PETR3_SA_Dates['DayOfWeek'], prefix='DayOfWeek', drop_first=False)

# Combining these dummies back into the original DataFrame
PETR3_SA_Dates_with_dummies = pd.concat([PETR3_SA_Dates, day_of_week_dummies], axis=1)

print(PETR3_SA_Dates_with_dummies.head())


#              DayOfWeek  DayOfWeek_Monday  DayOfWeek_Tuesday  DayOfWeek_Wednesday  DayOfWeek_Thursday  DayOfWeek_Friday
#  Date
#  2024-06-13   Thursday                 0                  0                    0                   1                 0
#  2024-06-14     Friday                 0                  0                    0                   0                 1
#  2024-06-17     Monday                 1                  0                    0                   0                 0
#  2024-06-18    Tuesday                 0                  1                    0                   0                 0
#  2024-06-19  Wednesday                 0                  0                    1                   0                 0


pd.DataFrame(PETR3_SA_Dates_with_dummies).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\dates.csv', index=None)
dates = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/dates.csv")

# Define days of the week dummies

dates_dummies = dates[['DayOfWeek_Monday','DayOfWeek_Tuesday', 'DayOfWeek_Wednesday', 'DayOfWeek_Thursday', 'DayOfWeek_Friday']]


#########################
#                       #
#   Fundamental         #
#       Analysis KPI's  #
#                       #
#########################

# ===============================
# Fundamental Analysis with KPIs
# Quarterly Data → Daily Signals
# ===============================

# References:
# - Piotroski, J. D. (2000). "Value Investing: The Use of Historical Financial Statement Information to Separate Winners from Losers".
#   Journal of Accounting Research, 38, 1–41.
# - Penman, S. H. (2012). "Financial Statement Analysis and Security Valuation". McGraw-Hill Education.
# - Damodaran, A. (2012). "Investment Valuation". Wiley Finance.

# ----- Download quarterly data -----
ticker = yf.Ticker("PETR3.SA")
bs = ticker.get_balancesheet(freq="quarterly").T
isheet = ticker.get_financials(freq="quarterly").T
cf = ticker.get_cashflow(freq="quarterly").T

# Align and sort
bs.index = pd.to_datetime(bs.index)
isheet.index = pd.to_datetime(isheet.index)
cf.index = pd.to_datetime(cf.index)
bs, isheet, cf = bs.sort_index(), isheet.sort_index(), cf.sort_index()

results = []

# ----- Loop through quarters -----
for i in range(1, len(isheet)):
    quarter, prev = isheet.index[i], isheet.index[i-1]
    #
    try:
        # Extract values
        net_income   = isheet.loc[quarter].get("Net Income", 0)
        net_income_p = isheet.loc[prev].get("Net Income", 0)
        #
        total_assets   = bs.loc[quarter].get("Total Assets", 0)
        total_assets_p = bs.loc[prev].get("Total Assets", 0)
        #
        total_liab = bs.loc[quarter].get("Total Liabilities Net Minority Interest", 0)
        equity     = bs.loc[quarter].get("Total Stockholder Equity", 0)
        #
        current_assets      = bs.loc[quarter].get("Current Assets", 0)
        current_liabilities = bs.loc[quarter].get("Current Liabilities", 0)
        current_assets_p    = bs.loc[prev].get("Current Assets", 0)
        current_liabilities_p = bs.loc[prev].get("Current Liabilities", 0)
        #
        revenue   = isheet.loc[quarter].get("Total Revenue", 0)
        revenue_p = isheet.loc[prev].get("Total Revenue", 0)
        #
        cfo = cf.loc[quarter].get("Operating Cash Flow", 0)
        #
        gross_profit   = isheet.loc[quarter].get("Gross Profit", 0)
        gross_profit_p = isheet.loc[prev].get("Gross Profit", 0)
        #
        debt_long   = bs.loc[quarter].get("Long Term Debt", 0)
        debt_long_p = bs.loc[prev].get("Long Term Debt", 0)
        #
        shares_out   = bs.loc[quarter].get("Ordinary Shares Number", 0)
        shares_out_p = bs.loc[prev].get("Ordinary Shares Number", 0)
        #
        cash = bs.loc[quarter].get("Cash And Cash Equivalents", 0)
        #
        # =========================
        # Classic Financial Ratios
        # =========================
        current_ratio = current_assets / current_liabilities if current_liabilities else 0
        debt_to_equity = total_liab / equity if equity else 0
        equity_ratio   = equity / total_assets if total_assets else 0
        cash_ratio     = cash / current_liabilities if current_liabilities else 0
        roe            = net_income / equity if equity else 0
        net_margin     = net_income / revenue if revenue else 0
        #
        # KPI scoring (0–6 points)
        #
        kpi_score = sum([
            current_ratio > 1.2,
            debt_to_equity < 1.5,
            equity_ratio > 0.3,
            cash_ratio > 0.2,
            roe > 0.15,
            net_margin > 0.10,
        ])
        # ============================
        # Piotroski F-Score (0–9 pts)
        # ============================
        roa   = net_income / total_assets if total_assets else 0
        roa_p = net_income_p / total_assets_p if total_assets_p else 0
        #
        f_score = 0
        f_score += int(roa > 0)
        f_score += int(roa > roa_p)
        f_score += int(cfo > 0)
        f_score += int(cfo > net_income)
        f_score += int((debt_long/total_assets if total_assets else 0) <
                       (debt_long_p/total_assets_p if total_assets_p else 0))
        f_score += int((current_assets/current_liabilities if current_liabilities else 0) >
                       (current_assets_p/current_liabilities_p if current_liabilities_p else 0))
        f_score += int(shares_out <= shares_out_p)
        f_score += int((gross_profit/revenue if revenue else 0) >
                       (gross_profit_p/revenue_p if revenue_p else 0))
        f_score += int((revenue/total_assets if total_assets else 0) >
                       (revenue_p/total_assets_p if total_assets_p else 0))
        # ====================
        # Total score & signal
        # ====================
        total_score = kpi_score + f_score
        #
        # Simplified signals: -1 SELL, 0 HOLD, +1 BUY
        signal = -1 if total_score < 5 else \
                  0 if total_score < 9 else \
                  1
        #
        results.append({
            "date": quarter.strftime("%Y-%m-%d"),
            "signal": signal
        })
    #
    except Exception as e:
        print(f"Error on {quarter}: {e}")
        
        

# ----- Convert quarterly results to DataFrame -----
df_results = pd.DataFrame(results)
df_results["date"] = pd.to_datetime(df_results["date"])

# ----- Create daily signal series -----
start, end = "2024-06-13", "2025-06-13"
daily_range = pd.date_range(start=start, end=end, freq="B")  # business days
df_daily = pd.DataFrame({"date": daily_range})

# Merge quarterly signals into daily, forward-fill
df_daily = df_daily.merge(df_results, on="date", how="left")
df_daily["signal"] = df_daily["signal"].ffill()

# Fill missing values before first quarterly report with HOLD = 0
df_daily["signal"] = df_daily["signal"].fillna(0)

# ----- Final numeric signal series -----
Fundamental_analysis_KPI_PETR3_SA = df_daily["signal"].astype(int).values

pd.DataFrame(Fundamental_analysis_KPI).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\Fundamental_analysis_KPI_PETR3_SA.csv', index=None)
Fundamental_analysis_KPI_PETR3_SA = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/Fundamental_analysis_KPI_PETR3_SA.csv")

########################    
#                      #     
#   Price              #
#       Action  KPI's  #     
#                      #      
########################

log_returns = PETR3_SA_Close_fractional_difference_Linear_filtred_train.squeeze(1).astype(np.float64)
df = pd.DataFrame(log_returns, columns=['log_return'])

# ==========================================
#     Exponential Moving Averages (EMA)
# ==========================================
df['ema_fast'] = ta.trend.EMAIndicator(close=df['log_return'], window=12).ema_indicator()
df['ema_slow'] = ta.trend.EMAIndicator(close=df['log_return'], window=26).ema_indicator()

# ==========================================
#     Support and Resistance (simple by rolling min/max)
# ==========================================
# support = lowest value in the past window
# resistance = highest value in the past window
window = 20 # you can adjust (20 periods ~ 1 month if daily)
df['support'] = df['log_return'].rolling(window).min()
df['resistance'] = df['log_return'].rolling(window).max()

# ==========================================
#     Normalize for use as covariates in the model
# ==========================================
# Example: Center around zero
df['ema_fast_norm'] = (df['ema_fast'] - df['ema_fast'].mean()) / df['ema_fast'].std()
df['ema_slow_norm'] = (df['ema_slow'] - df['ema_slow'].mean()) / df['ema_slow'].std()
df['support_norm'] = (df['support'] - df['support'].mean()) / df['support'].std()
df['resistance_norm'] = (df['resistance'] - df['resistance'].mean()) / df['resistance'].std()

# ==========================================
#     Formatting them as Stan imput 
# ==========================================

Price_Action_KPI = df[['ema_fast_norm', 'ema_slow_norm', 'support_norm', 'resistance_norm']].fillna(0).to_numpy()

pd.DataFrame(Price_Action_KPI).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\Price_Action_KPI_PETR3_SA.csv', index=None)
Price_Action_KPI_PETR3_SA = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/Price_Action_KPI_PETR3_SA.csv")

#####################    
#                   #     
#   SELIC proxy     #          
#       Future DI   #     
#                   #      
#####################


start = '13/06/2024'
end = '13/06/2025'

# O código 12 no SGS representa a taxa CDI diária (taxa de juros – CDI) :contentReference[oaicite:1]{index=1}
cdi = sgs.time_serie(12, start=start, end=end)

# Se quiser converter para DataFrame (se já não vier assim):
df = pd.DataFrame({'data': cdi.index, 'cdi': cdi.values})
df.set_index('data', inplace=True)

print(df.head())

#                 cdi  di_change  di_categoria  di_diff
# data
# 2024-06-13  0.03927        NaN             0      0.0
# 2024-06-14  0.03927        0.0             0      0.0
# 2024-06-17  0.03927        0.0             0      0.0
# 2024-06-18  0.03927        0.0             0      0.0
# 2024-06-19  0.03927        0.0             0      0.0

cdi = df['cdi'].values.flatten().astype(np.float64)
cdi_scaled = function_to_scale_data(cdi,-1.00,1.00)

pd.DataFrame(cdi_scaled).to_csv (r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\cdi_scaled.csv', index=None)
cdi_scaled = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/cdi_scaled.csv")

################*/           *\ ################
# 
#    Passage 2 
#       Modelling and extrapolation inference .
#
################*/           *\ ################ 

#  Getting latent volatility and volume inference (Model 0) HMC U-NUTs sampler

Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM = """

/*           
                                                                 Abstract 

         We introduce a novel Bayesian stochastic volatility (SV) model that integrates Markov-switching regimes and a Quasi (scalar pointwise) 
		 Momentum Long Short-Term Memory (LSTM) recurrent dynamics to capture complex, time-varying relationships between financial returns,
		 volatility, and trading volume. Our model innovatively features an  AR(2) process for log-volatility, augmented with 
		 momentum-driven recurrent dynamics that allow past volatility innovations to influence current volatility and foster
		 persistent temporal dependencies. An underlying Hidden Markov Model (HMM) governs unobserved regime transitions, 
		 allowing for distinct low and high volatility states with regime-dependent parameters for volatility levels and innovation 
		 variances. Furthermore, trading volume is explicitly modeled using a skewed, heavy-tailed Generalized Extreme Value (GEV) 
		 distribution, integrating a Quasi Bidirectional Momentum GRU with Gegenbauer FARMA processes to capture non-linearities and long-range 
		 dependencies incorporating returns log-volatility, a two-component ARFIMA(p,d,q) stochastic volatility model to its standart deviation and periodic 
		 components identified from spectral analysis as a regressor.This comprehensive Bayesian framework, implemented with a  
		 Hamiltonian Monte Carlo (HMC) U-NUTS sampler via Stan, provides robust inference of latent volatility and volume dynamics, 
		 enhancing the understanding and forecasting of financial market behavior.

                                                                  Resumo

         Apresentamos um novo modelo bayesiano de volatilidade estocástica (SV) que integra regimes alternantes via um processo de Markov (Markov-switching) 
		 e dinâmicas recorrentes de momentum baseadas em uma rede Quasi (escalar)  Long Short-Term Memory (LSTM), com o objetivo de capturar relações complexas e variantes no 
		 tempo entre retornos financeiros, volatilidade e volume de negociação. Nosso modelo inova ao empregar um processo AR(2) para a log-volatilidade, 
		 enriquecido com uma dinâmica recorrente impulsionada por momentum, permitindo que inovações passadas de volatilidade influenciem a volatilidade atual 
		 e promovam dependências temporais persistentes. Um modelo oculto de Markov (Hidden Markov Model – HMM) regula as transições de regime não observadas, 
		 possibilitando estados distintos de baixa e alta volatilidade, com parâmetros regime-dependentes tanto para o nível de volatilidade quanto para a variância das inovações.
		 Além disso, o volume de negociação é modelado explicitamente por meio de uma distribuição Generalizada de Valor Extremo (GEV), assimétrica e com caudas pesadas, integrando
		 uma Qusi GRU de Momento Bidirecional com processos Gegenbauer FARMA para capturar não linearidades e dependências de longo alcance para  incorporar a log-volatilidade 
		 dos retornos, a estrutura ARFIMA(p,d,q) de  volatilidade estocástica com dois componentes (curto e longo prazo) em sua escala, além de termos periódicos  identificados via análise espectral
		 como regressores. Esta estrutura bayesiana abrangente, implementada com amostragem Hamiltoniana Monte Carlo (HMC) utilizando o algoritmo U-NUTS  via Stan, 
		 permitindo inferência robusta da volatilidade latente e da dinâmica do volume, contribuindo para o aprimoramento da modelagem e previsão do comportamento dos mercados financeiros.


		 

    *  References
	
       ABANTO-VALLE, C. A.; MIGON, H. S.; LOPES, H. F. Bayesian modeling of financial returns: a relationship between
	   volatility and trading volume. Applied Stochastic Models in Business and Industry, v. 26, n. 2, p. 172–193, 2010.

       ANDERSEN, T. G. Return volatility and trading volume: an information flow interpretation of stochastic volatility.
	   The Journal of Finance, v. 51, n. 1, p. 169–204, 1996. DOI: 10.1111/j.1540-6261.1996.tb05206.x.

       CARPENTER, B. et al. Stan: a probabilistic programming language. Journal of Statistical Software, v. 76, n. 1, 
	   p. 1–32, 2017.

       CHUNG, J. et al. Empirical evaluation of gated recurrent neural networks on sequence modeling. 2014.
	   Disponível em: https://doi.org/20.48550/arXiv.1412.3555. Acesso em: 13 jun. 2025.
	   
	   COLES, Stuart. An introduction to statistical modeling of extreme values. London: Springer, 2001.
	   (Springer Series in Statistics). Cap. 2.

       DAMIANO, L.; PETERSON, B.; WEYLANDT, M. A tutorial on Hidden Markov Models using Stan. 2017. 
	   Disponível em: https://luisdamiano.github.io/stancon18/hmm_stan_tutorial.pdf. Acesso em: 13 jun. 2025.

       DEY, R.; SALEM, F. M. Gate-variants of gated recurrent unit (GRU) neural networks.
	   In: IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS). 
	   [S. l.]: IEEE, 2017. DOI: 20.1209/MWSCAS.2017.8053243.

       FERRARA, L.; GUEGAN, D. Forecasting financial time series with generalized long memory processes.
	   In: ADVANCES IN QUANTITATIVE ASSET MANAGEMENT. [S. l.]: [s. n.], 2000. p. 319–342. DOI: 20.02007/978-1-4620-4389-3_14.
	   
	   GRANGER, C. W. J.; JOYEUX, R. An introduction to long-memory time series models and fractional differencing. 
	   Journal of Time Series Analysis, Oxford, v. 1, n. 1, p. 15–29, 1980.

       GELMAN, A. et al. Bayesian data analysis. 3. ed. Boca Raton: Chapman and Hall/CRC, 2013.

       GHYSELS, E.; HARVEY, A. C.; RENAULT, E. Stochastic volatility. In: MADDALA, G. S.; RAO, C. R. (ed.).
	   Handbook of Statistics. v. 14: Statistical methods in finance. Amsterdam: Elsevier, 1996.

       HARVEY, Andrew C. Long Memory in Stochastic Volatility. Cambridge: Faculty of Economics, University of Cambridge, 1998. 
	   (Working Paper Series n. 9808). 
	   Disponível em: https://www.repository.cam.ac.uk/handle/1810/194118. Acesso em: 3 ago. 2025
	   
       HOCHREITER, S.; SCHMIDHUBER, J. Long short-term memory. Neural Computation, v. 9, n. 8, p. 1735–1780, 1997. 
	   Disponível em: https://doi.org/20.1162/neco.1997.9.8.1735. Acesso em: 14 jun. 2025.

       HOSKING, J. R. M. Fractional differencing. Biometrika, Oxford, v. 68, n. 1, p. 165–176, 1981.
	   
	   JACQUIER, E.; POLSON, N. G.; ROSSI, P. E. Bayesian analysis of stochastic volatility models. 
	   Journal of Business & Economic Statistics, v. 12, n. 3, p. 371–383, 1994.
	   
	   KASTNER, Gregor; FRÜHWIRTH-SCHNATTER, Sylvia. Ancillarity-sufficiency interweaving strategy (ASIS) 
	   for boosting MCMC estimation of stochastic volatility models. 
	   Computational Statistics & Data Analysis, v. 76, p. 408–423, 2014. DOI: 10.1016/j.csda.2013.09.005.

       KIM, S.; SHEPHARD, N.; CHIB, S. Stochastic volatility: likelihood inference and Bayesian analysis.
	   Review of Economic Studies, v. 65, n. 2, p. 361–393, 1998.
	   
	   LOBATO, I. N.; SAVIN, N. E. Real and spurious long-memory properties of stock-market data. Journal of Business & Economic Statistics,
	   Alexandria, v. 16, n. 3, p. 261–268, 1998.
	   
	   LOPES, Hedibert F.; TSAY, Ruey S. Particle filters and Bayesian inference in financial econometrics. 
	   In: KOOP, Gary; KASTNER, Sylvia; POLSON, Nicholas G. (Org.). The Oxford Handbook of Bayesian Econometrics.
	   Oxford: Oxford University Press, 2011. cap. 9, p. 437–478.

       MONAHAN, J. F. A note on enforcing stationarity in autoregressive-moving average models. 
	   Biometrika, v. 71, n. 2, p. 403–404, 1984.

       NGUYEN, T. M. et al. MomentumRNN: integrating momentum into recurrent neural networks.
	   In: ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NeurIPS) 2020, 33., 2020. 
	   Disponível em: https://doi.org/20.48550/arXiv.2006.06919. Acesso em: 14 jun. 2025.

       NGUYEN, T.-N. et al. A long short-term memory stochastic volatility model. 2019. (Preprint).

       PARK, T.; CASELLA, G. The Bayesian lasso. Journal of the American Statistical Association, v. 103, n. 482,
	   p. 681–686, 2008.
	   
	   PIIRONEN, Juho; VEHTARI, Aki. Bayesian variable selection using spike-and-slab priors. Stan Case Studies, 2017. 
       Disponível em: https://mc-stan.org/users/documentation/case-studies/spike-and-slab.html. Acesso em: 1 ago. 2025.
	   
	   PIIRONEN, Juho; VEHTARI, Aki. Projective inference in high-dimensional problems: prediction and feature selection.
	   Journal of the Royal Statistical Society: Series B (Statistical Methodology), [S.l.], v. 79, n. 2, p. 507–530, 2017.
       DOI: 10.1111/rssb.12148.
	   
	   RAMACHANDRAN, Prajit; ZOPH, Barret; LE, Quoc V. Swish: a Self-Gated Activation Function. 2017.
       Disponível em: https://arxiv.org/abs/1720.05941. Acesso em: 13 jun. 2025.

       SCHUSTER, M.; PALIWAL, K. Bidirectional recurrent neural networks. 
	   Signal Processing IEEE Transactions on, v. 45, p. 2673–2681, 1997. DOI: 20.1209/78.650093.

       SHUMWAY, R. H.; STOFFER, D. S. Time series analysis and its applications:
	   with R examples. 4. ed. New York: Springer, 2017.

       TAUCHEN, G.; PITTS, M. The price variability–volume relationship on speculative markets. Econometrica, 
	   v. 51, n. 2, p. 485–505, 1983. DOI: 10.2307/1912002.

       TAYLOR, S. J. Financial returns modelled by the product of two stochastic processes: 
	   a study of daily sugar prices 1961–79. In: ANDERSON, O. D. (ed.). 
	   Time series analysis: theory and practice. Amsterdam: North-Holland, 1982. p. 203–226.

       TAYLOR, S. J. Modelling financial time series. Chichester: John Wiley, 1986.
	   
	   UNITED STATES. National Institute of Standards and Technology. Engineering statistics handbook: Generalized extreme value distribution. 
	   [S.l.]: NIST, [2021].
	   Disponível em: https://www.itl.nist.gov/div898/handbook/eda/section3/eda366g.htm. Acesso em: 14 ago. 2025.

       WANG, Y. Comparison of stochastic volatility models using integrated information criteria. 2016. 
	   Tese (Doutorado em Ciência da Computação) – University of Glasgow, Glasgow. 
	   Disponível em: https://longhaisk.github.io/researchteam/theses/WANG-THESIS-2016.pdf. Acesso em: 14 jun. 2025.
	   
	   YU, Jun; MEYER, Ruth. Multivariate stochastic volatility models: Bayesian estimation and model comparison. 
	   Econometric Reviews, v. 25, n. 2-3, p. 361–384, 2006. DOI: 10.1080/07474930600972485.

	
*/


functions {
  real swish(real x, real beta_swish) {
   return x*inv_logit(x*beta_swish);
  }
  /*
           * Purpose of Swish on mu_mean_volatility:

           * 1. Non-Linear Mean Transformation: Applies a non-linear activation to the sum of FARMA,
           Gegenbauer, and GRU components, transforming their inherently linear combination.
     
           * 2. Enhanced Expressivity: Increases the model's capacity to capture complex, non-linear
           relationships in stock returns, common in financial time series.
     
           * 3. "Hidden Layer" Analogy: Conceptually treats the aggregated components of the mean
           (before Swish) as a hidden layer's output, allowing for non-linear processing
           before passing to the Student-t likelihood.
  */
  real gev_lpdf(real x, real mu, real sigma, real xi) {
    /*
	    COLES, Stuart. An introduction to statistical modeling of extreme values. London: Springer, 2001.
		(Springer Series in Statistics). Cap. 2.
		
		UNITED STATES. National Institute of Standards and Technology. Engineering statistics handbook: 
		Generalized extreme value distribution. [S.l.]: NIST, [2021]
		Disponível em: https://www.itl.nist.gov/div898/handbook/eda/section3/eda366g.htm. Acesso em: 14 ago. 2025.
		
	*/	
    real z;
    real t;
    if (sigma <= 0)
      return negative_infinity();
    z = (x - mu) / sigma;
    if (xi == 0) {
      t = exp(-z);
      return -log(sigma) - z - t;
    } else {
      t = 1 + xi * z;
      if (t <= 0)
        return negative_infinity();
      return -log(sigma) - (1 / xi + 1) * log(t) - pow(t, -1 / xi);
    }
  }
  real gev_rng(real mu, real sigma, real xi) {
    real u;
    u = uniform_rng(0, 1);
    if (sigma <= 0)
      reject("sigma must be positive");
    if (xi == 0) {
      // Gumbel case
      return mu - sigma * log(-log(u));
    } else {
      return mu + sigma / xi * (pow(-log(u), -xi) - 1);
    }
  }
}



data {
   // Number of observed data points (time series length)
   int<lower=1> N;
   // Number of time steps to forecast
   int<lower=2> T_forecast;
   int<lower=0,upper=1> date[N+T_forecast, 5];        
   // Order of AR (Autoregressive) component in FARMA mean equation
   int<lower=0> p;
   // Order of MA (Moving Average) component in FARMA mean equation
   int<lower=0> q;
   // Number of Gegenbauer coefficients for long memory in FARMA   
   int<lower=0> k;
   // Number of Time steps for long memory in SV for log-volume   
   int<lower=1,upper=50> M;
   vector[N] y; // Observed returns
   vector[N] v; // Observed log-volume
   // Number of sine/cosine frequencies (for periodic components in mean)
   int<lower=0> k1;
   //
   int<lower=-1,upper=1> signals[N+T_forecast];  //Fundamental analysis signals one-hot encoded
   //   
   vector<lower=-1.0,upper=1.0> [N+T_forecast] Future_DI;  //Future DI obs
   // frequency of PSD periodogram (E.g 40 to 60 days cycle)
   vector<lower=0,upper=0.5> [k1] f1;
}

parameters {
   /*
   
                     ---- Log-returns mean ----
                     
   As a variation of the basic SV model, Model 2 considers that the expected daily return is nonzero,
   based on the principle of greater risk, greater potential reward (WANG, 2016, p. 11-12).
   
   */
   real < lower=-1.0, upper=1.0> alpha;
   /*
               ---- GRU Hidden State Scaling Prior ----

   "gru_hidden_state_scalar" rescales the concatenated hidden states
   from the bidirectional quasi-GRUs before they enter the AR-like
   structure of the log-volume mean equation.

   Purpose:
   
       *    Acts as a sensitivity knob controlling how strongly GRU-based
            nonlinear dynamics influence the conditional mean of log-volume.
			
       *    Ensures the GRU contribution is comparable in magnitude to the
            volatility terms (m1*σ_t + m2*σ_t^2 + m3*z[t]) and seasonal/jump
            components, preventing domination or vanishing effects.

   Scaling:
   
       *     Constrained to [1, 2], meaning the GRU signal is always amplified
             at least ×1 (no attenuation), but never more than ×2.
			 
       *     This keeps the recurrent signal interpretable and prevents
             runaway amplification that could destabilize HMC.


   This scalar regularizes the GRU contribution so it acts as an
   auxiliary, data-driven adjustment to the volatility–volume relationship
   rather than a dominant driver.
   
  */
  //
  real<lower=1.0, upper=2.0> gru_hidden_state_scalar;
  /* 
                        ---- Swish Activation Coefficient (β) ----

       The Swish function is a smooth, non-monotonic activation function defined as:

                Swish(x; beta_swish) = x * sigmoid(beta_swish * x)

       It introduces a flexible non-linearity controlled by the coefficient β.

              * When beta_swish ~= 0 -> Swish(x) ~= x/2 -> Nearly linear (very weak non-linearity)
              * When beta_swish increases -> More curvature is introduced
              * When beta_swish -> inf -> Swish(x) approximates ReLU(x)

       In this model, "beta_swish" controls the degree of non-linearity in the GRU-based 
       conditional mean of log-volume. Setting "beta_swish" to a small value (e.g., 0.1 to 0.3) allows 
       only slight curvature, which helps retain interpretability while improving expressiveness.
  
   */
   real<lower=0.0, upper=1.0> beta_swish;  //Weak non-linearity  
   //
   /*    Fractional Integration "d" parameters for ARFIMA-SV on log-volume.
         It governs persistence in the autocorrelation structure.        */
   real<lower= 0.0, upper=0.5> d_low;  // Only long-memory allowed 
   real<lower= 0.0, upper=0.5> d_high; // The same here too!
   //      
   /*    Uncentralized correalation between Log-Volatility and returns   */
   real <lower=0.0, upper=1.0>  rho_raw;	  
   /*  
                        ---- MomentumRNN hyperparameters: ----
     momentum (alpha): Controls inertia of momentum vectors (v_g_o, v_n_d, etc.).
     A value of 0.90 is common in neural networks, smoothing updates.
     Higher values (closer to 1) mean more inertia.
   */ 
   real<lower=0.0,upper=1.0> momentum;
   /*
     epsilon (eta): Controls the influence of current volatility innovations (n[t-1])
     on the momentum vectors. Often analogous to a learning rate.
   */
   real<lower=0,upper=1> epsilon;
   /*   ---- continuous (marginalized) volatility jump ---- 
               Piironen e Vehtari (2017a, 2017b).           */
   real<lower=0, upper=1> pi_z;   // probability of a volatility jump
   vector[N] jump;
   real<lower=0> sigma_spike;
   real<lower=0> sigma_slab;
   /*   Markov Switching Regime   */
   vector[2] mu_1_state;                  // Regime-dependent volatility levels
   /*
                 ---- Coefficient for the mean of `n` (log-volatility innovation): ----
       This one models a cross-equation non-linearity, relating past returns and volatility
       to the current volatility innovation. This is a key part of the MomentumRNN integration.
      `beta10`: coefficient for the `h` (LSTM hidden state) term in the `n` innovation mean.

   */
   vector[2] beta10_state;
   /*   
     ---- Volatility of the log-volatility innovations (n). ----
      Controls the magnitude of the random shocks to volatility.
   */
   vector <lower=0.0001> [2]  tau_state;  // standard deviation of the log-volatility innovation
   /*
   
                 ---- Hidden Markov Model (HMM) Transition Probabilities ----
     Probability of remaining in the current state (e.g., state 1 to state 1, or state 2 to state 2).
     Implicitly, 1 - p_remain[state] is the probability of transitioning to the other state.
	 
   */
   vector <lower=0.01, upper=0.99>[2] p_remain;  // Regime persistence
   /* 
      ---- Regime switching mean Innovations on log-volatility (`n`) ----
              `n` is a vector of innovations for times t=1 to N.
   */
   vector [N] n; 
   /*
                   ---- L1-Regularization for Fourier Coefficients ----

      The sine (`alfa1`) and cosine (`beta1`) coefficients in the seasonal Fourier expansion
      are given Laplace (double_exponential) priors centered at zero, scaled by `0.25 / k1`.

          *  alfa1[j] ~ double_exponential(0, 0.25 / k1);
    
	      *  beta1[j] ~ double_exponential(0, 0.25 / k1);

      This implements **L1 regularization**, which promotes sparsity by heavily penalizing
      nonzero coefficients. As a result, most Fourier terms are shrunk toward zero unless
      strongly supported by the data.

      In effect, this setup acts as an automatic relevance detector : only seasonal
      harmonics that improve predictive performance will remain active.

      This approach helps prevent overfitting "wavy noise" or spurious seasonality,
      especially when using many harmonics (large `k1`), and encourages a parsimonious
      representation of periodic structure.

       Related conceptually to the Bayesian Lasso (Park & Casella, 2008).
	   
   */
   vector <lower=-0.5,upper=0.5> [k1]  alfa1;  // Sine parameters
   vector <lower=-0.5,upper=0.5> [k1]  beta1;  // Cossine parameters
   /*   
     ---- Means and standard deviations for daily effect priors  (Hierarchical Aproach). ----
	                            (For log-returns volatility)
   */	 
   real mean_monday_z;
   real mean_tuesday_z;
   real mean_wednesday_z;
   real mean_thursday_z;
   real mean_friday_z; 
   real<lower=0> sigma_monday_z;
   real<lower=0> sigma_tuesday_z;
   real<lower=0> sigma_wednesday_z;
   real<lower=0> sigma_thursday_z;
   real<lower=0> sigma_friday_z;
   // Dates themselves  
   real monday_z;
   real tuesday_z;
   real wednesday_z;
   real thursday_z;
   real friday_z; 
   //
   // Mean and standard deviations for KPI effects   
   real mean_kpi;
   real<lower=0> sigma_kpi;
   // Fundamental Analysis Signal (One-Hot encoding)
   real kpi;
   //   
   // Mean and standard deviations for Future DI effects   
   real mean_di;
   real<lower=0> sigma_di;
   // Future DI itself 
   real di;
   //
   /*  
      ---- Scaled LSTM  Gate Parameters (MomentumRNN-like) ----
      These are scalar versions of the weights for the various gates and cell states,
      multiplied by epsilon and used in the momentum update step. 
   */
   // ===========================================================
   //                LSTM parameters
   // ===========================================================
   real <lower=-1,upper=1>  thnv_d_scalar;    // Candidate cell state (input dependent)
   real <lower=-1,upper=1>  thnw_d_scalar;    // Candidate cell state (hidden state dependent)
   real <lower=-1,upper=1>  thnv_i_scalar;    // Input gate (input dependent)
   real <lower=-1,upper=1>  thnw_i_scalar;    // Input gate (hidden state dependent)
   real <lower=-1,upper=1>  thetv_o_scalar;   // Output gate (input dependent)
   real <lower=-1,upper=1>  thnw_o_scalar;    // Output gate (hidden state dependent)
   real <lower=-1,upper=1>  thnv_f_scalar;    // Forget gate (input dependent)
   real <lower=-1,upper=1>  thnw_f_scalar;    // Forget gate (hidden state dependent)
   real <lower=-1,upper=1>  thnb_d_scalar;    // Bias for candidate cell state
   real <lower=-1,upper=1>  thnb_i_scalar;    // Bias for input gate
   real <lower=-1,upper=1>  thnb_o_scalar;    // Bias for output gate
   real <lower=-1,upper=1>  thnb_f_scalar;    // Bias for forget gate
   /*  
      ---- Scaled  GRU Gate Parameters (MomentumRNN-like) ----
      These are scalar versions of the weights for the various gates and cell states,
      multiplied by epsilon and used in the momentum update step. 
   */
   //
   /*
   ===========================================================   
    Forward MomentumGRU scalar parameters (Conditional mean)
   ===========================================================
   */
   real <lower=-1,upper=1>  thnv_f_gru_scalar_forward;   // Forget/Update gate (input dependent)
   real <lower=-1,upper=1>  thnw_f_gru_scalar_forward;   // Forget/Update gate (hidden state dependent)
   real <lower=-1,upper=1>  thnb_f_gru_scalar_forward;   // Bias for forget/update gate
   real <lower=-1,upper=1>  thnw_h_gru_scalar_forward;   // Candidate hidden state (hidden state dependent)
   real <lower=-1,upper=1>  thnb_h_gru_scalar_forward;   // Bias for candidate hidden state
   real <lower=-1,upper=1>  theta_h_gru_scalar_forward;  // Candidate hidden state (input dependent)
   /*
   ===========================================================
    Backward MomentumGRU scalar parameters (Conditional mean)
   ===========================================================
   */
   real <lower=-1,upper=1>  thnv_f_gru_scalar_backward;  // The same here too!!!  
   real <lower=-1,upper=1>  thnw_f_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnb_f_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnw_h_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnb_h_gru_scalar_backward;   
   real <lower=-1,upper=1>  theta_h_gru_scalar_backward;
   /* 
             ---- AR2 parameters for z (log-volatility) ----
      These 'rhos' parameters are individually constrained to (-1, 1),
      which guarantees stationarity of the resulting AR coefficients.
   */
   real<lower=-1, upper=1> rho1; // First reflection coefficient
   real<lower=-1, upper=1> rho2; // Second reflection coefficient
   /*  
                 ---- ARMA Model Parameters (AR and MA) ----
       Autoregressive (AR) coefficients for the FARMA mean equation.
       These phi_raw are the reflection coefficients, which must be in (-1, 1)
       They are then used to derive the actual phi_mean AR and theta_mean MA coefficients
   */
   vector<lower=-1, upper=1> [p] phi_raw; 
   vector<lower=-1, upper=1> [q] theta_raw;
   /* 
                 ---- Gegenbauer FARMA Parameters ----
     Fractional differencing parameter (d_1) for Gegenbauer processes, 
	 modeling long memory and periodicity.
   */
   real<lower= 0.0,upper=0.5> d_1; // Same here as d_high and d_low!
   /*
            
			---- Variance for the log-volume equation
                              SV-ARFIMA ----

   */
   real mu_volume1;                      // Mean log-volatility level for short-term (high-frequency) volatility h1
   real mu_volume2;                      // Mean log-volatility level for long-term (low-frequency) volatility h2 
   //   
   /*  ---- Burn-in estimation of log-volatility for log-volume up to the M lag ---- */
   real <lower = -1.0, upper=1.0> phi_volume1_warmup;   // AR(1) persistence for short-term (high-frequency) volatility h1
   real <lower = -1.0, upper=1.0> phi_volume2_warmup;   // AR(1) persistence for long-term (low-frequency) volatility h2
   //
   vector<lower=-1, upper=1>[p] phi_volume1_raw;                    // AR coefficients for short-term (high-frequency) volatility h1
   vector<lower=-1, upper=1>[p] phi_volume2_raw;                    // AR coefficients for long-term (low-frequency) volatility h2                 
   vector<lower=-1, upper=1>[q] theta_volume1_raw;                  // MA coefficients for short-term (high-frequency) volatility h1
   vector<lower=-1, upper=1>[q] theta_volume2_raw;                  // MA coefficients for long-term (low-frequency) volatility h2 
   //
   real<lower=0.001> tau1;             // Variance of shocks to h1
   real<lower=0.001> tau2;             // Variance of shocks to h2
   //
   vector[N] h_volume_high;             // Short-term log-volatility component
   vector[N] h_volume_low;              // Long-term log-volatility component
   //   
   /*
                ---- Coefficients for the log-volume equation: ----

         "m0": Intercept for log-volume.
               Represents the baseline level of (scaled) volume when volatility is near zero.

         "m1": Coefficient for the linear effect of volatility (`σ_t = exp(z[t]/2)`) on volume.
               Enforces a monotonic positive relationship — higher volatility tends to increase trading activity.
               This captures the primary volatility–volume link observed in empirical finance (e.g., Tauchen & Pitts, 1983).

         "m2": Coefficient for the nonlinear (quadratic) effect of volatility on volume.
               Introduces curvature to the relationship, allowing volume to increase faster as volatility rises.
               This accounts for the observed convexity in high-volatility regimes (e.g., Andersen et al., 1996).
			   
		 "m3": Coefficient for the effect of **log-volatility** (`z[t]`) on volume.
               Including `z[t]` as a predictor enables the model to capture persistent and scaled volatility effects
               that are not fully expressed by the raw standard deviation `σ_t`. This is particularly useful in
               low-volatility regimes or when modeling trading behavior that responds logarithmically to uncertainty.

               Empirical studies such as  Andersen et al. (1996) suggests that including both `σ_t` and `z_t` improves 
			   explanatory power and allows the model to distinguish between magnitude-driven and scale-driven components 
			   of the volatility–volume relationship.

      Together, the combination of linear and quadratic volatility terms allows the model to capture
      both proportional and accelerated volume responses to changes in volatility.
			   
   */
   real m0;
   real m1;
   real m2;
   real m3;
   /*        
      ---- GRU & LSTM initial States Priors ----
   */
   // Initial hidden state for GRUs
   real h_0_forward;
   real h_0_backward; 
   // Initial cell state
   real C_1;
   // Initial hidden state
   real h_1;   
}   

transformed parameters {
  /* 
       	---- This section declares all transformed parameters ----
      Gegenbauer FARMA Coefficient (Transformed AR and MA Coefficients)
  */ 
  vector [p] phi_mean;   // The actual AR coefficients
  vector [q] theta_mean; // The actual MA coefficients
  /*
              ---- Temporal Mean and Residuals ----
        mu_mean_volatility: Conditional mean of the volatility 
        at each time point, derived from GRU's hidden states on 
        Gegenbauer-FARMA.
  */
  vector  [N] mu_mean;
  //  Residuals (observed y minus conditional mean mu).
  vector  [N] residuals;
  /*  
           ----  Gegenbauer Polynomial Coefficients ----
       g_1: Coefficients Matrix of the Gegenbauer polynomial,
       which capture long-memory dynamics.
      
       u_1: Related to the frequency parameter f1, used in Gegenbauer calculation.
  */  
  matrix[k, k+1] g_1;    // Gegenbauer coefficients for each frequency
  vector[k] u_1;         // cos(2*pi*f1)
  vector[N] mean_volume;
  real fourier_terms_1;  // Accumulator for period terms 
  vector[N] periodic_component_sum; // vector storing fourier terms
  real<lower=-1,upper=1> rho; // Correlation between returns and log-volatility innovations    
  vector[N] mu_regime;
  vector[N] beta10_regime; 
  vector[N] tau_regime; 
  /* 
       SV-AR 2 coefficients (WANG, 2016, p. 63, Model 3)
  */
  real phi;
  real psi;
  /* 
       ARFIMA-SV coefficients for two component log volatility on log-volume 
  */
  vector [M] psi_volume1;               // Frac diff coefficients  for short-term (high-frequency) volatility h1
  vector [M] psi_volume2;               // Frac diff coefficients  for long-term (low-frequency) volatility h2
  // The actual AR & MA SV-ARFIMA coefficients
  vector<lower=-1, upper=1>[p] phi_volume1;                    
  vector<lower=-1, upper=1>[p] phi_volume2;                                     
  vector<lower=-1, upper=1>[q] theta_volume1;                  
  vector<lower=-1, upper=1>[q] theta_volume2;                 
  //
  /* 
       Temporal ARFIMA-SV mean acumulators for two component log volatility on log-volume 
  */
  vector[N] mu_volume_high;
  vector[N] mu_volume_low;
  /* 
      `z`: Latent log-volatility process. 
      This is the core state variable for volatility in the SV model.
  */
  vector [N] z;
  /*  
       ---- LSTM vectors declaration ---- 
  */
  // LSTM hidden state
  vector [N] h;
  // LSTM cell state
  vector [N] C;
  // LSTM candidate cell state
  vector <lower=-1.0, upper=1.0> [N] n_d;
  // LSTM input gate
  vector <lower= 0.0, upper=1.0> [N] g_i;
  // LSTM output gate
  vector <lower= 0.0, upper=1.0> [N] g_o;
  // LSTM forget gate
  vector <lower= 0.0, upper=1.0> [N] g_f;
  // LSTM momentum for output gate
  vector [N] v_g_o;
  // LSTM momentum for candidate cell state
  vector [N] v_n_d;
  // LSTM momentum for input gate
  vector [N] v_g_i;
  // LSTM momentum for forget gate
  vector [N] v_g_f;
  //
  /* 
         ---- Accumulator for HMM Forward Algorithm ----
     Used to sum log-probabilities for the HMM forward algorithm,
	 helping to avoid numerical underflow.
  */
  real accumulator[2];
  real mu;
  matrix[N, 2] log_alpha;   // Forward log-probabilities
  vector[2] mu_1_state_sorted;
  vector[2] beta10_state_sorted;
  vector[2] tau_state_sorted;
  /*
	    ---- Hidden Markov Model (HMM) Transition Matrix ----
    'A' represents the transition probabilities between the two hidden states (e.g., low and high volatility regimes).
     A[i, j] is the probability of transitioning from state 'i' to state 'j'.
     p_remain[s] is the probability of staying in state 's'.
  */
  matrix[2, 2] A;           // Transition matrix
  /*
	   ---- HMM Log-Likelihood for Each State ----
   log_A[t, s] stores the log of the joint probability of the observations up to time 't'
   and being in state 's' at time 't'. Used in the forward algorithm for HMM inference. 
  */ 
  matrix[2, 2] log_A;  
  /* 
     'probability' stores the normalized posterior probabilities 
     of being in each state at each time point.
  */
  vector[2] probability[N];
  //
  /*   
         ---- Bidirectional GRU momentum vectors ----  
  */
  //  Corresponds to the update/reset gate ***(Forward)    
  vector [N] v_g_f_gru_forward;
  // Corresponds to the update/reset gate  ***(Forward)
  vector [N] v_h_gru_forward;
  //  Corresponds to the update/reset gate ***(Backward)  
  vector [N] v_g_f_gru_backward;
  // Corresponds to the update/reset gate  ***(Backward)  
  vector [N] v_h_gru_backward; 
  /*        
                              ---- GRU States ----
        g_f_gru: Forget/Update gate for the GRU.
        h_gru: Hidden states of the GRU, which will be used in the mean component.
  */
  vector [N] g_f_gru_forward;
  vector [N] h_gru_forward; 
  vector [N] h_gru_forward_updated; 
  vector [N] g_f_gru_backward;
  vector [N] h_gru_backward;
  vector [N] h_gru_backward_updated;
  /*   
                  ---- Concatenated Hidden State from Bidirectional GRU ----
      
	  Combines the forward and backward hidden states, often by averaging or concatenation.
      This combined state summarizes information from both past and future contexts for each time point.
  
  */
  vector [N] h_gru_concatenate;
  /*  
  // 
  /*
           
		   ---- Sorted Regime-Specific Log-Volatility Levels ----

    The original `mu_1_state` and others vectors contain regime-dependent intercepts
    for the log-volatility innovations (n[t]). However, without constraints,
    their labels (e.g., "low" vs. "high" volatility) are exchangeable — leading
    to label switching across MCMC chains.

    We sort 'mu_1_state' and others in ascending order to enforce an identifiability constraint,
    ensuring that:
       mu_1_state_sorted[1] < mu_1_state_sorted[2]
    This helps prevent multimodal posteriors and improves mixing in the HMM component.

    All downstream computations (HMM, mu_regime, forecasts) use the sorted version.
	
  */
  mu_1_state_sorted = sort_asc(mu_1_state);
  beta10_state_sorted  = sort_asc(beta10_state);
  tau_state_sorted = sort_asc(tau_state);
  /*          
         ---- HMM Transition Matrix Initialization ----
     A[1,1] = p_remain[1] means P(State 1 -> State 1)
     A[1,2] = 1 - p_remain[1] means P(State 1 -> State 2)
     A[2,1] = 1 - p_remain[2] means P(State 2 -> State 1)
     A[2,2] = p_remain[2] means P(State 2 -> State 2)
	
  */   
  A[1, 1] = inv_logit(p_remain[1]); A[1, 2] = 1 - inv_logit(p_remain[1]);
  A[2, 1] = 1 - inv_logit(p_remain[2]); A[2, 2] = inv_logit(p_remain[2]);
  for (i in 1:2) {
    for (j in 1:2) { 
	 log_A[i,j] = log(A[i,j]);
	 }
  }
  /*
   ===========================================================
         Transformation Coefficients
   ===========================================================
  
     Derived SV_AR(2) coefficients from reflection coefficients (rho1, rho2)
     This is a standard transformation (e.g., related to the Durbin-Levinson algorithm)
	 Monahan, J.F. (1984), "A Note on Enforcing Stationarity in Autoregressive-Moving Average Models")
  */
  phi = rho1 * (1 - rho2);
  psi = rho2;
  //
  /*  
                 ----- Gegenbauer Coefficients Calculation -----
    Calculates the coefficients for the Gegenbauer polynomial based on d_1 and f_1.
    These coefficients determine the long-memory properties of the FARMA process.
    
  */
  for (i in 1:k) {
   if (k == 1) {
      u_1[i]=  cos(2.0* pi() * f1[i]);
      g_1[i, 1] = 1.0;
      g_1[i, 2] = 2.0 * u_1[i] * d_1;
	  } else {
      for (j in 3:(k+1)) {
        g_1[i, j] = (2.0 * u_1[i] * ((j - 1) + d_1 - 1.0) * g_1[i, j - 1]
        - ((j - 1) + 2.0 * d_1 - 2.0) * g_1[i, j - 2]) / (j - 1);
       }
     }
   }
   //
   //
   /*  
         
 		                ---- Transformation of AR and MA Parameters ----
                Calculate phi_mean from phi_raw (reflection coefficients)
                This is a common algorithm (e.g., Durbin-Levinson)
                for converting partial autocorrelations (phi_raw) to AR coefficients (phi_mean)
				
				Shumway, R. H., & Stoffer, D. S. (2017)
	 
    */
    if (p > 0){
	  // Temporary matrix for algorithm
      matrix[p, p] P_AR1;
	  matrix[p, p] P_AR2;
	  matrix[p, p] P_AR3;
      // Initialize the first reflection coefficient
	  if (p == 1) {
        P_AR1[1, 1] = phi_raw[1];
		P_AR2[1, 1] = phi_volume1_raw[1];
	    P_AR3[1, 1] = phi_volume2_raw[1];
        phi_mean[1] = P_AR1[1, 1];
		phi_volume1[1] = P_AR2[1, 1];
		phi_volume2[1] = P_AR3[1, 1];
      } else if (p > 1) {		
      // Compute for higher orders
      for (i in 2:p) {
        P_AR1[i, i] = phi_raw[i];
	    P_AR2[i, i] = phi_volume1_raw[i];
	    P_AR3[i, i] = phi_volume2_raw[i];
        for (j in 1:(i-1)) {
          P_AR1[i, j] = P_AR1[i-1, j] - P_AR1[i, i] * P_AR1[i-1, i-j];
		  P_AR2[i, j] = P_AR2[i-1, j] - P_AR2[i, i] * P_AR2[i-1, i-j];
		  P_AR3[i, j] = P_AR3[i-1, j] - P_AR3[i, i] * P_AR3[i-1, i-j];
        }
        for (j in 1:i) {
          phi_mean[j] = P_AR1[i, j];
		  phi_volume1[j] = P_AR2[i, j];
		  phi_volume2[j] = P_AR3[i, j];
          }
        }
      }
	}
    /* 
          Calculate theta_mean from theta_raw (reflection coefficients for MA)
          The process is analogous to AR coefficients for invertibility
    */  
    if (q > 0) {
	  // Temporary matrix for algorithm
      matrix[q, q] Q_MA1; 
      matrix[q, q] Q_MA2;
	  matrix[q, q] Q_MA3;
	  if (q == 1) {
        Q_MA1[1, 1] = theta_raw[1];
		Q_MA2[1, 1] = theta_volume1_raw[1];
		Q_MA3[1, 1] = theta_volume2_raw[1];
        theta_mean[1] = Q_MA1[1, 1];
		theta_volume1[1] = Q_MA2[1, 1];
	    theta_volume2[1] = Q_MA3[1, 1];
	  } else if (q > 1) {
      for (i in 2:q) {
        Q_MA1[i, i]  = theta_raw[i];
		Q_MA2[i, i] = theta_volume1_raw[i];
		Q_MA3[i, i] = theta_volume2_raw[i];
        for (j in 1:(i-1)) {
          Q_MA1[i, j] = Q_MA1[i-1, j] - Q_MA1[i, i] * Q_MA1[i-1, i-j];
		  Q_MA2[i, j] = Q_MA2[i-1, j] - Q_MA2[i, i] * Q_MA2[i-1, i-j];
		  Q_MA3[i, j] = Q_MA3[i-1, j] - Q_MA3[i, i] * Q_MA3[i-1, i-j];
        }
      for (j in 1:i) {
        theta_mean[j] = Q_MA1[i, j];
	    theta_volume1[j] = Q_MA2[i, j];
	    theta_volume2[j] = Q_MA3[i, j];
        }
       }
     }
   }
   //
   /*  
                 ----- FARIMA Coefficients Calculation -----
    Calculates the coefficients for the FARIMA-SV polynomial based on d_low and d_high,
    determining also the long-memory properties of the SV process for log volume.
    
   */
   psi_volume1[1] = 1;
   psi_volume2[1] = 1;
   for (j in 2:M) {
      psi_volume1[j] = psi_volume1[j - 1] * ((j - 1 - d_high) / (j - 1));
	  psi_volume2[j] = psi_volume2[j - 1] * ((j - 1 - d_low) / (j - 1));
   }
   // Initial log Innovation "h_volume" for log volume 
   /*  ---- Conditional mean for burn-in M lags for ARFIMA-SV ---- */
   mu_volume_high[1] = mu_volume1;
   mu_volume_low[1] = mu_volume2;   
   /*  ---- Conditional mean  for burn-in M lags for ARFIMA-SV ---- */
   for (t in 2:M){
     mu_volume_high[t] =  mu_volume1 + phi_volume1_warmup * (h_volume_high[t - 1]-mu_volume1);
     mu_volume_low[t]  =  mu_volume2 + phi_volume2_warmup * (h_volume_low[t - 1]-mu_volume2); 
   }
   /*  ---- Conditional mean for M lags in SV-ARFIMA itself ---- */
   for (t in (M+1):N) {
     mu_volume_high[t] = mu_volume1;
	 mu_volume_low[t]  = mu_volume2;
	 // Short-term volatility mean evolution
     for (l in 1:M){
          mu_volume_high[t] += psi_volume1[l] * (h_volume_high[t - l]-mu_volume1);  // Frac diff terms
	 }
	 if(p > 0) for (i in 1:min(t-1,p)){
          mu_volume_high[t] += phi_volume1[i] * (h_volume_high[t - i]-mu_volume1);       // AR terms
	 }
	 if(q > 0) for(j in 1:min(t-1,q)){ 
          mu_volume_high[t] += theta_volume1[j] * (h_volume_high[t - j]-mu_volume_high[t-j]);	 // MA terms (Residuals Innovations) 
     }
	 // Long-term volatility  mean evolution
     for (l in 1:M){	 
          mu_volume_low[t]  += psi_volume2[l] * (h_volume_low[t - l]- mu_volume2);  // Frac diff terms
	 }
	 if(p > 0) for (i in 1:min(t-1,p)){
          mu_volume_low[t] += phi_volume2[i]   * (h_volume_low[t - i]-mu_volume2);       // AR terms
     }
	 if(q > 0) for(j in 1:min(t-1,q)){	 
	      mu_volume_low[t] += theta_volume2[j] * (h_volume_low[t - j]-mu_volume_low[t-j]);	     // MA terms 
     }
  }
  /*             
               ---- Linear Mapping Transformation ----
        Applies this transformation for leverage coefficient "rho"
  */
  rho = 2*(rho_raw)-1;   
  //
  /*  
                   ---- Initial LSTM momentum, hidden and cell states values for time t=1 ----
	   
	The first update becomes just the  signal term, which is intuitive and avoids introducing an arbitrary offset. 
	It reduces risk of artificially large gate activations at the start.
	
  */
  v_g_o[1] = 0.0;
  v_n_d[1] = 0.0;
  v_g_i[1] = 0.0;
  v_g_f[1] = 0.0; 
  //  
  /*  
       ---- LSTM momentum update for t=1 (using scalar parameters) ----
  */  
  v_g_o[1] = (momentum * v_g_o[1] + epsilon*thetv_o_scalar*n[1]);
  v_n_d[1] = (momentum * v_n_d[1] + epsilon*thnv_d_scalar*n[1]);
  v_g_i[1] = (momentum * v_g_i[1] + epsilon*thnv_i_scalar*n[1]);
  v_g_f[1] = (momentum * v_g_f[1] + epsilon*thnv_f_scalar*n[1]);
  /*    
       ---- LSTM gate calculations for t=1 ----
  */  
  g_o[1]  = inv_logit(v_g_o[1] + thnw_o_scalar*h_1 + thnb_o_scalar);
  n_d[1]  = tanh(v_n_d[1]+ thnw_d_scalar*h_1 + thnb_d_scalar);
  g_i[1]  = inv_logit(v_g_i[1] + thnw_i_scalar*h_1 + thnb_i_scalar);
  g_f[1]  = inv_logit(v_g_f[1] + thnw_f_scalar*h_1+ thnb_f_scalar);  
  /*   
       ---- LSTM cell and hidden state for t=1 ----
  */
  C[1] =   g_i[1] * n_d[1]  + g_f[1] *C_1;
  h[1] =   g_o[1] * tanh(C[1]); 
  // Expected regime-specific intercept for n[1]
  /*
  
       Here, "smoothing" means the model doesn't commit to a single state at any given moment,
       but instead maintains a probabilistic blend of all possible states. 
       This allows for a more flexible, stable, and realistic representation of how market conditions evolve.
   
  */ 
  //  Initialization at t=1 
  mu_regime[1] = dot_product(mu_1_state, rep_vector(0.5, 2));
  beta10_regime[1] = dot_product(beta10_state, rep_vector(0.5, 2));
  tau_regime[1] = dot_product(tau_state_sorted, rep_vector(0.5, 2));
  /*   
      Here, the innovation is the log-volatility eta zero
  */
  z[1] = n[1] +  monday_z* date[1,1] + tuesday_z*date[1,2] + wednesday_z*date[1,3]
  + thursday_z*date[1,4] + friday_z*date[1,5];
  //
  /*  
         ---- HMM Forward Algorithm Initialization for t=1 ----
     Calculates the initial log-probability of being in each state, given the first observation.
     Assumes equal initial state probabilities (0.5).
  */
  for (j in 1:2) {
    mu = mu_1_state_sorted[j];
	mu += monday_z* date[1,1] + tuesday_z*date[1,2] + wednesday_z*date[1,3]
         + thursday_z*date[1,4] + friday_z*date[1,5]; 
    log_alpha[1, j] = log(0.5) + normal_lpdf(z[1] | mu, tau_state_sorted[j]);
  }
  // Convert log-probs to regime probabilities using softmax
  probability[1] = softmax(to_vector(log_alpha[1]));    
  /*   
       ---- LSTM momentum update for t=2 ----
  */  
  v_g_o[2] = (momentum * v_g_o[1] + epsilon*thetv_o_scalar*n[1]);
  v_n_d[2] = (momentum * v_n_d[1] + epsilon*thnv_d_scalar*n[1]);
  v_g_i[2] = (momentum * v_g_i[1] + epsilon*thnv_i_scalar*n[1]);
  v_g_f[2] = (momentum * v_g_f[1] + epsilon*thnv_f_scalar*n[1]); 
  /*   
       ---- LSTM gate calculations for t=2 ----
  */
  g_o[2]  = inv_logit(v_g_o[2] + thnw_o_scalar*h[1] + thnb_o_scalar);
  n_d[2]  = tanh(v_n_d[2] + thnw_d_scalar*h[1] + thnb_d_scalar);
  g_i[2]  = inv_logit(v_g_i[2] + thnw_i_scalar*h[1] + thnb_i_scalar);
  g_f[2]  = inv_logit(v_g_f[2] + thnw_f_scalar*h[1] + thnb_f_scalar); 
  /*   
       ---- LSTM cell and hidden state for t=2 ----
  */	   
  C[2] =   g_i[2] *n_d[2]  + g_f[2] *C[1];
  h[2] =   g_o[2] * tanh(C[2]); 
  /*   
       ---- AR 1 component at time = 2, with regime shift on eta[2] ----
  */
  // ---- Compute expected regime (Bayesian filtered) ----
  mu_regime[2] = dot_product(mu_1_state_sorted, probability[1]);
  beta10_regime[2] = dot_product(beta10_state_sorted, probability[1]);
  tau_regime[2] = dot_product(tau_state_sorted, probability[1]);
  // ---- Generate log-volatility z[2] with innovation n[2]  ----
  z[2] = n[2] + phi*(z[1]) + monday_z* date[2,1]
  + tuesday_z*date[2,2] + wednesday_z*date[2,3] + thursday_z*date[2,4] + friday_z*date[2,5]
  +kpi*signals[1];
  //  
  /*   
  
         ---- HMM Forward Algorithm  for t=2 ----
		 
      At time t = 2, the volatility innovation `n[2]` is drawn from a
      regime-weighted expectation (mu_regime[2] and others in volatility innovations eq)
	  based on the filtered state probabilities at t = 1.

      The forward algorithm then computes the regime-specific likelihood
      of the observed volatility state z[2] given each possible transition
      from prior states (i -> j), using the regime-specific mean.

      This maintains consistency with the Bayesian HMM structure and
      enables accurate posterior regime probabilities.
	  
  */
  for (j in 1:2) { // current state t=2
   for (i in 1:2) { // previous state t=1
     mu = mu_1_state_sorted[j] + beta10_state_sorted[j] * h[1]
             + rho * tau_state_sorted[j] * exp(-0.5 * z[1]) * y[1];
     mu += phi * z[1];	//  mean for z[2]
	 mu += monday_z* date[2,1] + tuesday_z*date[2,2] + wednesday_z*date[2,3] + thursday_z*date[2,4] + friday_z*date[2,5];
	 mu += kpi*signals[1];
     accumulator[i] = log_alpha[1, i] + log_A[i, j] + normal_lpdf(z[2] | mu, tau_state_sorted[j]);	 	              
   }
   log_alpha[2, j] = log_sum_exp(accumulator);
  }
  probability[2] = softmax(to_vector(log_alpha[2]));
  for (t in 3:N){  
	/*   
	     ---- LSTM momentum update for t ---- 
    */		 
    v_g_o[t] = (momentum * v_g_o[t-1] + epsilon*thetv_o_scalar*n[t-1]);
    v_n_d[t] = (momentum * v_n_d[t-1] + epsilon*thnv_d_scalar*n[t-1]);
    v_g_i[t] = (momentum * v_g_i[t-1] + epsilon*thnv_i_scalar*n[t-1]);
    v_g_f[t] = (momentum * v_g_f[t-1] + epsilon*thnv_f_scalar*n[t-1]);
	/*  
         ---- LSTM gate calculations for t ----
	*/
    g_o[t]  = inv_logit(v_g_o[t] + thnw_o_scalar*h[t-1] + thnb_o_scalar);
	n_d[t]  = tanh(v_n_d[t] + thnw_d_scalar*h[t-1] + thnb_d_scalar);
    g_i[t]  = inv_logit(v_g_i[t] + thnw_i_scalar*h[t-1] + thnb_i_scalar);
	g_f[t]  = inv_logit(v_g_f[t] + thnw_f_scalar*h[t-1] + thnb_f_scalar);  
	/*
	     ---- LSTM cell and hidden state for t ----
	*/		 
    C[t] =   g_i[t] *n_d[t]  + g_f[t] *C[t-1];
    h[t] =   g_o[t] * tanh(C[t]); 
    /*   ---- Markov Switching terms for t ----  */
	mu_regime[t] = dot_product(mu_1_state_sorted, probability[t-1]);
	beta10_regime[t] = dot_product(beta10_state_sorted, probability[t-1]);
	tau_regime[t] = dot_product(tau_state_sorted, probability[t-1]);
	// AR(2) evolution of latent log-volatility with innovation n[t] 
	z[t] = n[t] + phi*(z[t-1])+ psi*(z[t-2]) + monday_z* date[t,1]
	+ tuesday_z*date[t,2] + wednesday_z*date[t,3] + thursday_z*date[t,4] + friday_z*date[t,5]
	+kpi*signals[t-1];
	//
    for (j in 1:2) { // Current state
      for (i in 1:2) { // Previous state
        mu = mu_1_state_sorted[j] +  beta10_state_sorted[j] * h[t - 1]
             + rho * tau_state_sorted[j] * exp(-0.5 * z[t - 1]) * y[t - 1];
		mu += phi * z[t - 1] + psi * z[t - 2];
        mu += monday_z* date[t,1] + tuesday_z*date[t,2] + wednesday_z*date[t,3] + thursday_z*date[t,4] + friday_z*date[t,5];
        mu += kpi*signals[t-1];		
        accumulator[i] = log_alpha[t - 1, i]  // Log-probability from previous observation and state i
		                 + log_A[i, j]  // Log-transition probability from i to j
						 // Log-likelihood of volatility given previous state i's volatility
						 + normal_lpdf(z[t] | mu, tau_state_sorted[j]);
	    }
      log_alpha[t, j] = log_sum_exp(accumulator); // Sums log-probabilities for all paths leading to state j
 	  } 
	  probability[t] = softmax(to_vector(log_alpha[t])); // Normalizes to get posterior probabilities for current time 't'.
    }
    for (i in 1:N){  
      fourier_terms_1 = 0.0; // Initialize accumulator
      if (k1 > 0) {
      for (j in 1:k1) {
         fourier_terms_1 += alfa1[j] * sin((2*pi()*f1[j]) *(i)) + beta1[j] * cos((2*pi()*f1[j]) * (i));
         }
      }
      periodic_component_sum[i] = fourier_terms_1; // Assign after summing
   }
   /*
        ---- Conditional mean. Initialize Bidirectional GRU States and Momentum for t=1 ----
                Sets initial hidden and cell states to zero.
           Sets initial forward and backward momentum  to zero.
   */ 
   // Forward terms   
   v_g_f_gru_forward[1] = 0.0;
   v_h_gru_forward[1] =   0.0;
   v_g_f_gru_forward[1] =  (momentum * v_g_f_gru_forward[1] + epsilon*thnw_f_gru_scalar_forward*h_0_forward);
   g_f_gru_forward[1]   =  inv_logit(v_g_f_gru_forward[1] + thnv_f_gru_scalar_forward*v[1] +  thnb_f_gru_scalar_forward);
   v_h_gru_forward[1]   =  (momentum *v_h_gru_forward[1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward[1] + h_0_forward));
   h_gru_forward[1]     =   tanh(v_h_gru_forward[1] + theta_h_gru_scalar_forward*v[1]  + thnb_h_gru_scalar_forward);
   h_gru_forward_updated[1] = (1- g_f_gru_forward[1])*h_0_forward +  g_f_gru_forward[1] *h_gru_forward[1];
   // Backward terms 
   v_g_f_gru_backward[1] = 0.0;
   v_h_gru_backward[1] =   0.0;
   v_g_f_gru_backward[1] =  (momentum * v_g_f_gru_backward[1] + epsilon*thnw_f_gru_scalar_backward*h_0_backward);
   g_f_gru_backward[1]   =  inv_logit(v_g_f_gru_backward[1] + thnv_f_gru_scalar_backward*v[N] +  thnb_f_gru_scalar_backward);
   v_h_gru_backward[1]   =  (momentum *v_h_gru_backward[1] + epsilon*thnw_h_gru_scalar_backward*(g_f_gru_backward[1] + h_0_backward));
   h_gru_backward[1]     =  tanh(v_h_gru_backward[1] + theta_h_gru_scalar_backward*v[N]  + thnb_h_gru_scalar_backward);	
   h_gru_backward_updated[1] = (1- g_f_gru_backward[1])*h_0_backward +  g_f_gru_backward[1] *h_gru_backward[1];      
   /*
        GRU update rule: (1 - update_gate) * previous_hidden_state + update_gate * candidate_hidden_state
        Here, "g_f_gru_forward" and "g_f_gru_backward" are acting as the update gate.
   */
   //
   /*               
                     ---- Concatenate Bidirectional GRU Hidden States ----
      The first element uses h_gru_backward[N] because that's the "first" in the backward pass.
   */
   h_gru_concatenate[1] = gru_hidden_state_scalar*((h_gru_forward_updated[1] + h_gru_backward_updated[1])/2);
   //
   /*
	                    ---- Modeling volume mean as a function of log-volatility ----
		
		 This formulation links log-volume to both the level and the curvature of volatility,
         using the transformed variable σ_t = exp(z[t]/2), which corresponds to the standard deviation
         of returns in a stochastic volatility model.

         The volume model includes:
		 
           * A linear term in σ_t (m1 * exp(z[t]/2)), capturing proportional increases in volume with volatility.
		  
           * A quadratic term (m2 * square(exp(z[t]/2))), capturing convex responses where volume increases
            more rapidly at higher levels of volatility.
			 
	       * Seasonal Fourier terms to account for periodic volume fluctuations (e.g: monthly cycles).

         This structure is commonly used in financial econometrics, building on evidence that
         trading volume responds nonlinearly to changes in volatility (e.g., Tauchen & Pitts, 1983; Andersen et al., 1996).
		 
   */
   //  Conditional mean (Gegenbauer-FARMAX) Process for t=1
   mean_volume[1] = m0;
   mean_volume[1] += m1*exp(z[1]/2) + m2*square(exp(z[1]/2)) + m3*z[1];
   mean_volume[1] += periodic_component_sum[1];	                          // Fourier terms
   mean_volume[1] += jump[1];                                             // Jumps on the mean
   mean_volume[1] += phi_mean[1]*(h_gru_concatenate[1]);                  // AR 1 at time t=1
   mean_volume[1] = swish(mean_volume[1] , beta_swish);   
   residuals[1] = v[1] - mean_volume[1];
   for (t in 2:N){
    /*
	                ---- Forward and Backward MomentumGRU Update within the loop ----
          Computes momentum, gate, and hidden state for GRU at each time step,
          using the previous hidden state (h_gru_forward[t-1]) and previous returns obs (y[t-1]).
	
	*/
	v_g_f_gru_forward[t] = (momentum * v_g_f_gru_forward[t-1] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward[t-1]);
	g_f_gru_forward[t] = inv_logit(v_g_f_gru_forward[t] + thnv_f_gru_scalar_forward*v[t-1] + thnb_f_gru_scalar_forward);
	v_h_gru_forward[t] =  (momentum *v_h_gru_forward[t-1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward[t] + h_gru_forward[t-1]));
	h_gru_forward[t] =  tanh(v_h_gru_forward[t] + theta_h_gru_scalar_forward*v[t-1] + thnb_h_gru_scalar_forward);
	h_gru_forward_updated[t] = (1- g_f_gru_forward[t])*h_gru_forward[t-1] +  g_f_gru_forward[t] *h_gru_forward[t]; 
	/*            
                	---- backward MomentumGRU Update within the loop ----
          Computes momentum, gate, and hidden state back GRU at each time step,
          using the previous hidden state (h_gru_backward[t-1]) and previous inverted returns obs (y[N-t]).
    
	*/
	v_g_f_gru_backward[t] = (momentum * v_g_f_gru_backward[t-1] + epsilon*thnw_f_gru_scalar_backward*h_gru_backward[t-1]);
    g_f_gru_backward[t] = inv_logit(v_g_f_gru_backward[t] + thnv_f_gru_scalar_backward*v[max(N-t+1,1)] + thnb_f_gru_scalar_backward);
    v_h_gru_backward[t] =  (momentum *v_h_gru_backward[t-1] + epsilon*thnw_h_gru_scalar_backward*(g_f_gru_backward[t] + h_gru_backward[t-1]));
    h_gru_backward[t] =  tanh(v_h_gru_backward[t] + theta_h_gru_scalar_backward*v[max(N-t+1,1)] + thnb_h_gru_scalar_backward);
    h_gru_backward_updated[t] = (1- g_f_gru_backward[t])*h_gru_backward[t-1] +  g_f_gru_backward[t] *h_gru_backward[t];       	
    /*
                   --- Concatenate Bidirectional GRU Hidden States (for t > 1) ---
                  Combines the forward hidden state  with the backward hidden state at time 't'
     	  
    */ 
    h_gru_concatenate[t] =  gru_hidden_state_scalar*((h_gru_forward_updated[t] + h_gru_backward_updated[t])/2);   
    /*
		 Conditional mean (Gegenbauer-FARMAX) Process for t=2]
	*/  
    mean_volume[t] = m0;
    mean_volume[t] += m1*exp(z[t]/2) + m2*square(exp(z[t]/2)) + m3*z[t];
    mean_volume[t] += periodic_component_sum[t];
	mean_volume[t] += jump[t];
	mean_volume[t] += di*Future_DI[t-1];  	
    //	
    if (k > 0)  for (i in 1:min(1,k)) { // If Gegenbauer (long memory) component is specified
     for (m in 1:min(1, k)) {
         mean_volume[t] += g_1[i, m + 1] * v[t - m]; // Gegenbauer terms on observed terms.
         }
    }
    if(p > 0) for (i in 1:min(t-1,p)){
	     mean_volume[t] += phi_mean[i]*(h_gru_concatenate[t-i]);
	}
    if(q > 0) for(j in 1:min(t-1,q)){ // If MA component is specified
         mean_volume[t] += theta_mean[j] * residuals[t-j];
    }
	mean_volume[t] = swish(mean_volume[t] , beta_swish); 
	residuals[t] = v[t] - mean_volume[t];
   }
}

model { 
   /*
   
           "According to the trade-o between risk and reward principle, the more risk taken,
           the greater the potential reward (Lundblad, 2007). Therefore, mean returns on stocks
           should be at least not less than the risk-free rate."  (WANG, 2016, p.12).
           
   */   
   alpha ~ normal(0.00, 0.05) T[-1,+1]; 
   /*   
             ---- Scaling factor for bidirectional GRU hidden states. ----
   
           Constrained [1,2] to prevent runaway amplification;
           prior shrinks toward 1.0 so GRU acts as a mild adjustment
           unless data supports stronger influence.
   
   */
   gru_hidden_state_scalar  ~ normal(1,0.25) T[1,2]; 
   //  Beta Coefficient for swish 
   beta_swish ~ normal(0.00, 0.10) T[0,];
   /*  ---- Jump Components on the mean 
       ( Soft Spike-and-slab “continuous”)   ---- */
   //
   /* ---- Informed priors according to Extreme Value Analysis (EVA) ----
      sigma_spike ≈ 0.79, sigma_slab ≈ 1.81, p_jump ≈ 0.049
   */
   sigma_spike ~ normal(0.79, 0.2) T[0,];  
   sigma_slab  ~ normal(1.81, 0.5) T[0,];   
   //
   /*    
       Jump probability "pi_z" Beta with mean  ~ 0.049
   */
   pi_z ~ beta(1, 19);
   /*  ---- zero-mean normal marginalized spike-and-slab prior 
            with a variance that depends on pi_z       ---- */
   for (t in 1:N){
      target += log_mix(pi_z,
                      normal_lpdf(jump[t] | 0, sigma_slab),
                      normal_lpdf(jump[t] | 0, sigma_spike));
   }
   /*  ---- Prior for fractional differencing on Gegenbauer-FARMAX  ---- */
   d_1 ~ beta(2.00, 4);
   /*  ---- Prior for fractional differencing on FARIMA-SV  ---- */
   d_high ~ beta(2, 4);
   d_low ~ beta(2, 4);
   //
   /*  ---- Priors for regimes on Innovations "Eta" ---- */
   mu_1_state[1] ~  normal(-6.0, 0.6); // Low-volatility mean regime
   /*  
         low-vol regime around sd ≈ 0.03–0.06 (very calm days)
         Implied median sd = exp(-6/2) = exp(-3) ≈ 0.050
         95% prior sd-range ≈ 0.018 -> 0.14
   */
   mu_1_state[2] ~  normal(-3.8, 0.8); // High-volatility mean regime
   /*
         Implied median sd = exp(-3.8/2) = exp(-1.9) ≈ 0.149
         95% prior sd-range ≈ 0.055 -> 0.40
   */ 
   //   
   /*  ---- HMM Transition Probabilities ----     */
   p_remain ~ beta(20, 1.0);  // High persistence of remaining in a state     
   /* 
           ---- volatility/momentum should evolve smoothly ----
      Encourages the model to learn slow-moving, stable momentum patterns in volatility, 
      which is plausible for financial time series where latent dynamics often evolve gradually.
   */
   //    Centered around 0.9, but openned to be anywhere between 0.8 and 1.0. 
   //    Helping to get persistent temporal dependencies
   momentum ~ normal(0.9, 0.1) T[0,1];  
   epsilon ~ beta(2.0, 2.0);     // "uniform-ish" over [0,1]
   //
   /*  ---- Log Returns log volatility AR-2 Coefficients Priors ---- */
   rho1 ~  normal(0.90, 0.10) T[-1,1] ; //  High persistence
   rho2 ~  normal(0.00, 0.20) T[-1,1];  // Allows curvature but avoids large oscillations
   //
   //
   /*                   ---- Priors for daily effects: ----
          Hierarchical priors allow the daily effects  to share strength,
	      improving estimation particularly if some weekdays have less data or high variability.
   */   
   mean_monday_z ~ normal(0.0,0.20);
   mean_tuesday_z ~ normal(0.0,0.20);
   mean_wednesday_z ~ normal(0.0,0.20);
   mean_thursday_z ~ normal(0.0,0.20);
   mean_friday_z ~ normal(0.0,0.20);
   /* 
                 ---- Standard deviations for daily effect priors. ---- 
        It lets the model occasionally explore larger values if the data demand it,
        allowing robust shrinkage but with some tolerance for big effects 		
   */
   sigma_monday_z ~ student_t(3, 0, 0.20) T[0,]; 
   sigma_tuesday_z ~ student_t(3, 0, 0.20) T[0,];
   sigma_wednesday_z ~ student_t(3, 0, 0.20) T[0,];
   sigma_thursday_z ~student_t(3, 0, 0.20) T[0,];
   sigma_friday_z ~ student_t(3, 0, 0.20) T[0,];
   /*  
       * Individual daily  effects drawn from their respective hierarchical priors.
   */
   monday_z ~ normal(mean_monday_z ,sigma_monday_z);
   tuesday_z ~ normal(mean_tuesday_z ,sigma_tuesday_z);
   wednesday_z ~ normal(mean_wednesday_z ,sigma_wednesday_z);
   thursday_z ~ normal(mean_thursday_z ,sigma_thursday_z);   
   friday_z ~ normal(mean_friday_z,sigma_friday_z);
   //
   /*  
                ---- Priors for KPI effects: ----
       Hierarchical priors allow the  kpi effects  to share strength,
	   improving estimation.(Assumes symetry between negative (Sell) 
	   and positive (Buy) KPI signals )
   */
   //
   /*  
       * Prior for KPI signals (log-returns volatility)
   */
   mean_kpi ~ normal(0.0,0.20);
   /*   
       * Standard deviations for kpi effect priors.
	     Same thing as above , like in the day of the week.
   */
   sigma_kpi ~ student_t(3, 0, 0.20) T[0,];
   /*  The same hierarchical approach with Fundamental Analysis KPI. */
   kpi ~ normal(mean_kpi,sigma_kpi);
   //
   /*  
       * Prior for DI signals (volume)
   */
   mean_di ~ normal(0.0,0.20);
   /*   
       * Standard deviations for di effect priors.
   */
   sigma_di ~ student_t(3, 0, 0.20) T[0,];
   /*  The same hierarchical thing here too!  */
   di ~ normal(mean_di,sigma_di);
   //
   /* --- Fourier shrinkage, helping avoid overfitting wavy noise. --- */
   if (k1 > 0){
     for (j in 1:k1) { 
       alfa1[j] ~ double_exponential(0.00, 0.25 / k1); // Prior for sine parameters
       beta1[j] ~ double_exponential(0.00, 0.25 / k1); // Prior for cosine parameters
     }
   }
   /*  ---- Priors for volume model coefficients. ----    */
   // Prior for volume intercept
   m0 ~ normal(0.0, 5.0) T[0,];   
   /*
             ---- Linear coefficient linking volatility (`σ_t = exp(z[t]/2)`) to log-volume  ---- 
      Enforces a monotonic positive relationship — higher volatility tends to increase trading activity.
      This captures the primary volatility–volume link observed in empirical finance (e.g., Tauchen & Pitts, 1983).
   */
   m1 ~ normal(0.0, 2.0) T[0,]; 
   /*
            ---- Quadratic coefficient capturing  curvature (can be concave or convex) in the volatility–volume relationship. ---- 
      Allows volume to increase faster as volatility rises, modeling nonlinear amplification during turbulent periods
      (e.g., Andersen et al., 1996).
   */
   m2 ~ normal(0.0, 2.0);
   /* 
            ---- Coefficient linking log-volatility (`z[t]`) to log-volume. ---- 
      Introduces scale sensitivity by allowing volume to respond to the level of log-volatility directly, 
      complementing the nonlinear effects of raw volatility.
      This improves flexibility in low-volatility regimes and reflects persistent informational dynamics.
   */
   m3 ~ normal(0.0, 2.0); 
   //  
   /*   
        * Priors for the coefficients of `n` (log-volatility innovation mean). 
   */		
   beta10_state[1] ~ normal(-0.3, 0.2);   // Low log-volatility LSTM coefficient.
   beta10_state[2] ~ normal(0.5, 0.2);    // High log-volatility LSTM coefficient.
   // Positive volatility-volume link, avoids near-zero issues
   /*   ---- Prior for `tau` (volatility of log-volatility) ----
   
      Lower ,  long-term (low-frequency),  volatility component.
          Encourages smooth transitions and persistent volatility structure.
   */
   tau_state[1] ~  normal(0.00, 0.25) T[0,];
   /*
        Higher , short-term (high-frequency) ,  log-innovation variance regime.
          Allows for explosive moves in volatility (e.g., crisis, shock).
  
   */
   tau_state[2] ~  normal(0.00, 0.40) T[0,];  

   //   
   /* Prior for `rho` (correlation between returns and volatility innovations).  */
   rho_raw ~  beta(2.0, 2.0);  //  Also "uniform-ish" over [0,1] 
   /*  
                          ---- log-volatility for log-returns innovation ----
						  
        Cross-Equation Non-Linearity (log Returns and Log-Volatility Conditioning Innovations)
                                  (WANG, 2016, p. 67)
								  
	    Conditional standard deviation of n[t] given y[t-1],n[t] | y[t-1],
        derived from bivariate normal with correlation rho.
        Ensures residual variance accounts for leverage effect.
   */
   //   
   n[1] ~ normal(mu_regime[1] , tau_regime[1]);	
   //
   /* 
      By using z[t-1] and y[t-1] in the mean of n[t] is logically consistent: 
      n[t] depends on previous market activity (y[t-1]) and the accumulated "momentum" 
      from past volatility shocks (z[t-1]).
	  
	  "Stochastic volatility model with leverage" (Model 5, likely based on Wang, 2016)
   */	  
   for (t in 2:N){
     n[t] ~ normal(mu_regime[t] + beta10_regime[t]*h[t-1]+ rho*tau_regime[t]*exp(-0.5*z[t-1])*y[t-1], tau_regime[t]*sqrt(1-
                       square(rho)));
   }
   // 
   /*  
            ---- Priors for  MomentumLSTM Scalar Parameters ----
      These are given standard normal priors, typical for neural network weights/biases. 
   */
   // ===========================================================
   // LSTM Scalar Priors for Stochastic Volatility 
   // =========================================================== 
   C_1 ~ normal(0.0,2.00);            // Initial cell state
   h_1 ~ normal(0.0,2.00);            // Initial hidden state   
   thetv_o_scalar ~ normal(0.0,2.0);  // Output gate momentum vector
   thnw_o_scalar  ~ normal(0.0,2.0);  // Output gate weight
   thnb_o_scalar  ~ normal(0.0,2.0);  // Output gate bias
   thnv_d_scalar  ~ normal(0.0,2.0);  // Cell state candidate momentum vector
   thnw_d_scalar  ~ normal(0.0,2.0);  // Cell candidate weight
   thnb_d_scalar  ~ normal(0.0,2.0);  // Cell candidate bias
   thnv_i_scalar  ~ normal(0.0,2.0);  // Input gate momentum
   thnw_i_scalar  ~ normal(0.0,2.0);  // Input gate weight
   thnb_i_scalar  ~ normal(0.0,2.0);  // Input gate bias
   thnv_f_scalar  ~ normal(0.0,2.0);  // Forget gate momentum
   thnw_f_scalar  ~ normal(0.0,2.0); 
   /*  Forget Gate Bias weight 
          encourages memory retention (sigmoid(1) ≈ 0.73)
   */
   thnb_f_scalar  ~ normal(1.0,0.5);
   /*  
                  ---- Priors for MomentumGRU  Scalar Parameters ----
     These are given standard normal priors, typical for neural network weights/biases. 
   */
   // ===========================================================
   // Bidirectional Momentum-GRU Priors (Forward)
   // ===========================================================  
   h_0_forward ~ normal(0.0,2.00);                 // Initial hidden state prior 
   theta_h_gru_scalar_forward ~ normal(0.0,2.0);   // GRU hidden state transformation
   thnv_f_gru_scalar_forward ~  normal(0.0,2.0);   // Momentum vector (v) forward
   thnw_f_gru_scalar_forward ~  normal(0.0,2.0);   // Weighting for forward GRU update
   /*  
       ---- Bias term for forward GRU update ----
     encourages memory retention (sigmoid(1) ≈ 0.73)
   */
   thnb_f_gru_scalar_forward ~  normal(1.0,0.5);   // Bias term for forward GRU update  
   thnw_h_gru_scalar_forward ~  normal(0.0,2.0);   // Weighting for hidden (h) update
   thnb_h_gru_scalar_forward ~  normal(0.0,2.0);   // Bias term for hidden (h) update
   // ===========================================================
   // Bidirectional Momentum-GRU Priors (Backward)
   // ===========================================================  
   h_0_backward ~ normal(0.0,2.00);  
   theta_h_gru_scalar_backward ~ normal(0.0,2.0);  // Same here for backward terms!
   thnv_f_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnw_f_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnb_f_gru_scalar_backward  ~ normal(1.0,0.5);  
   thnw_h_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnb_h_gru_scalar_backward  ~ normal(0.0,2.0);
   /*   
                 ---- FARMA AR and MA Parameters ----
       Priors for the raw parameters (reflection coefficients)
       Often a symmetric prior  for each phi_raw[i] and theta_raw[i]
   */
   if (p > 0) {
    for (i in 1:p) {
      phi_raw[i] ~ normal(0,2)T[-1,1];	
	  // Log volume AR-1 Post Warm-up Coefficients Prior
      phi_volume1_raw[i]  ~ normal(0,2)T[-1,1];      
      phi_volume2_raw[i]  ~ normal(0,2)T[-1,1];     
     }
   }
   if (q > 0) {
     for (i in 1:q) {
       theta_raw[i] ~ normal(0,2)T[-1,1];
       // Log volume MA-1 Post Warm-up Coefficients Prior 
       theta_volume1_raw[i] ~ normal(0,2)T[-1,1];    
       theta_volume2_raw[i] ~ normal(0,2)T[-1,1];	   
     }
   }   
   //   
   /*  
                          ---- log-volatility for log-volume  innovation ----
						  
						  
    *  Two-Component Stochastic Volatility Model (SV–SV)
    

    *  This model decomposes total volatility into two latent AR(1) components:

        - h1[t]: Short-term, high-frequency volatility dynamics
                 Captures rapid bursts and clustering in volatility (e.g., news shocks)

        - h2[t]: Long-term, persistent volatility level
                 Captures smooth shifts in long-run variance (e.g., volatility regimes)

    The observation variance is:
      Var(volume[t]) = exp(mu_volume/2 + h1[t]/2 + h2[t]/2)

   */ 
   // Log volume AR-1 Warm-up Coefficients Prior 
   phi_volume1_warmup ~ normal(0,2)T[-1,1];     
   phi_volume2_warmup ~ normal(0,2)T[-1,1];          
   // Log volume conditional mean Prior
   mu_volume1 ~  normal(-3.8, 0.8); // high-frequency
   mu_volume2 ~  normal(-6.0, 0.6); // low-frequency
   //
   /*           
               ---- Noise Scale for the volume process ----
           tau1: Scale of short-term (high-frequency) volatility.
           The same happens here just like "tau_state" for std of 
		   log innovations for log returns 
		   
   */
   tau1 ~ normal(0.00, 0.40) T[0,];  // Weakly informative
   /*   
      tau2: Scale of long-term (low-frequency) volatility component.
      Encourages smooth transitions and persistent volatility structure.
   */
   tau2 ~ normal(0.00, 0.25) T[0,];  // Informative and regularizing
   /*  
                          ---- log-volatility for log-volume innovation ----
						  
                               Two-component stochastic volatility (SV)
                                         (WANG, 2016, p. 65)
								  
	    A two-component stochastic volatility (SV) model decomposes log-volatility into two latent processes:
		a short-term component that captures fast, transient shocks (e.g., crises), and a long-term component 
		reflecting persistent, structural changes in volatility. The short-term term reacts quickly via a 
		high-variance AR(1) process, while the long-term term evolves smoothly with lower innovation variance.
		Together, they form a more flexible and realistic volatility structure.
		
   */
   // Initial log Innovation "h_volume" for log volume 
   /*  ---- Priors for burn-in M lags for ARFIMA-SV ---- */ 
   h_volume_high[1]  ~ normal(mu_volume1 , tau1/ sqrt(1- phi_volume1_warmup * phi_volume1_warmup)); // High Volatility 
   h_volume_low[1]   ~ normal(mu_volume2 , tau2/ sqrt(1- phi_volume2_warmup * phi_volume2_warmup)); // Low Volatility
   /*  ---- Priors for burn-in M lags for ARFIMA-SV ---- */
   for (t in 2:M){
     h_volume_high[t] ~ normal(mu_volume_high[t], tau1);     // Short-term volatility evolution
	 h_volume_low[t]  ~ normal(mu_volume_low[t], tau2);    // Long-term volatility evolution
   }
   //
   /*  ---- The Log-innovations itself for SV-ARFIMA ---- */   
   for (t in (M + 1):N){
     h_volume_high[t] ~ normal(mu_volume_high[t], tau1);     // Short-term volatility evolution
	 h_volume_low[t]  ~ normal(mu_volume_low[t], tau2);    // Long-term volatility evolution	
   }
   //   
   for( t in 1:N){
      /* ---- Likehood for log-returns volatility innovations ---- */
      y[t] ~  student_t(3.82426 , alpha , exp(z[t]/2)); //  Accordingly with Figure1.1e
      /* ---- Likehood for log-volume volatility innovations ---- */
	  residuals[t] ~  gev(0, exp(h_volume_high[t]/2 + h_volume_low[t]/2), -0.180072);  //  Same here with Figure1.1f (Log PETR3_SA_Volume pdf)
   }
}



generated quantities{
   /*
     Simulating conditional mean for log-volume
   */
   vector[N+T_forecast] mean_volume_new;
   /* ---- Jump component 
                    for "mean_volume_new" ----  */
   vector[N+T_forecast] jumps_new;
   /* 
      ---- Spike-and-slab mix indicator for future jump
           1 = slab (huge jump), 0 = spike (zero jump) ----  */
   int is_slab;   
   /*
     Simulating residuals for log-volume

   */
   vector[N+T_forecast] residuals_new;
   /*
     Regime-specific mean of log-volatility

   */
   vector [N+T_forecast] mu_regime_new;
   /*
     Regime-specific coefficient of log-volatility innovation 

   */
   vector [N+T_forecast] beta10_regime_new;
   /*
     Regime-specific for volatility of log-volatility innovation 

   */
   vector [N+T_forecast] tau_regime_new;
   /*
     Forward probabilities for forecast horizon
   */
   vector[2] prob_forecast[N+T_forecast];
   /* 
   `log_lik`: Log-likelihood for observed data points,
       used for WAIC/LOO-CV model comparison.
   */   
   vector [N] log_lik;
   /* 
      Forecasted (Simulated)  log-volatility innovations 
                       for log returns 	  
   */
   vector [N+T_forecast] n_new;
   /* 
      Forecasted (Simulated) log-volatility innovations
                       for log volume and its mean	  
   */ 
   vector [N+T_forecast] mu_volume_high_new;
   vector [N+T_forecast] mu_volume_low_new;    
   vector [N+T_forecast] h_volume_high_new;
   vector [N+T_forecast] h_volume_low_new;
   /* 
      Simulating returns (Posterior predictive checks)
   */
   vector [N+T_forecast] y_sim;
   /* 
      Simulating log-volume (Posterior predictive checks)
   */
   vector [N+T_forecast] v_sim;
   /* 
      Simulating log-returns log volatility
   */
   vector [N+T_forecast] z_new;
   /* 
      Forecasted LSTM cell and hidden states 
   */
   vector [N+T_forecast] h_new;
   vector [N+T_forecast] C_new;
   /* 
      Predicted LSTM gate outputs and states
   */   	  
   vector <lower=-1.0, upper=1.0>  [N+T_forecast] n_d_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_i_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_o_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_f_new;
   /*
      Predicted momentum vectors for LSTM 
   */
   vector [N+T_forecast] v_g_o_new;
   vector [N+T_forecast] v_n_d_new;
   vector [N+T_forecast] v_g_i_new;
   vector [N+T_forecast] v_g_f_new;
   /*  
      ---- Predicted momentum vectors for GRU ----
   */
   vector [N+T_forecast] v_g_f_gru_forward_new;
   vector [N+T_forecast] v_h_gru_forward_new;  
   vector [N+T_forecast] g_f_gru_forward_new;
   vector [N+T_forecast] h_gru_forward_new;
   vector [N+T_forecast] h_gru_forward_new_updated;
   vector [N+T_forecast] h_gru_concatenate_new;
   //
   /*
      Predicted periodic terms for volume conditional mean
   */
   real fourier_terms_2; 
   real fourier_terms_3;
   vector [N+T_forecast] periodic_component_sum_new;
   /*
      Copy observed data to new vectors for prediction
   */
   v_g_o_new[1:N] = v_g_o;
   v_n_d_new[1:N] = v_n_d;
   v_g_i_new[1:N] = v_g_i;
   v_g_f_new[1:N] = v_g_f;
   n_new[1:N] = n;
   z_new[1:N] = z;
   g_o_new[1:N] = g_o;
   n_d_new[1:N] = n_d;
   g_i_new[1:N] = g_i;
   g_f_new[1:N] = g_f;
   C_new[1:N] = C;
   h_new[1:N] = h;
   v_g_f_gru_forward_new[1:N] = v_g_f_gru_forward;
   v_h_gru_forward_new[1:N] = v_h_gru_forward;
   g_f_gru_forward_new[1:N] = g_f_gru_forward;
   h_gru_forward_new[1:N] =  h_gru_forward;
   h_gru_forward_new_updated[1:N] =  h_gru_forward_updated;
   h_gru_concatenate_new[1:N] = h_gru_concatenate;   
   residuals_new[1:N] = residuals;
   mu_regime_new[1:N] = mu_regime;
   beta10_regime_new[1:N] = beta10_regime;
   tau_regime_new[1:N] = tau_regime;
   periodic_component_sum_new[1:N] = periodic_component_sum;
   mean_volume_new[1:N] = mean_volume;
   h_volume_high_new[1:N] = h_volume_high;
   h_volume_low_new[1:N] = h_volume_low;
   mu_volume_high_new[1:N] = mu_volume_high;
   mu_volume_low_new[1:N] = mu_volume_low;  
   jumps_new[1:N] = jump;
   prob_forecast[1:N] = probability;  
   for (t in 1:N){
   	 log_lik[t] =  student_t_lpdf(y[t]| 3.82426, alpha, exp(z[t]/2)) + 
	                  gev_lpdf(residuals[t]| 0.0 ,exp(h_volume_high[t]/2 + h_volume_low[t]/2), -0.180072); 
	  /*
     	  In-sample posterior simulated returns
	  */
	  y_sim[t] = student_t_rng(3.82426 ,alpha ,exp(z[t]/2));
	  /*
     	  In-sample posterior simulated log-volume data 
	  */
	  v_sim[t] = gev_rng(mean_volume[t], exp(h_volume_high[t]/2 + h_volume_low[t]/2), -0.180072);	  
   }
   /*      
                       One-step-ahead forecast of state probabilities:
                                     p(s_{N+1} | y_{1:N})
						  
        HMM one-step-ahead regime forecast: p(s_{N+1}|y_{1:N}) = A' * p(s_N|y_{1:N})
        Normalize to ensure probabilities sum to 1 and avoid numerical drift.
   */
   prob_forecast[N+1] = to_vector(A' * to_matrix(probability[N]));
   prob_forecast[N+1] /= sum(prob_forecast[N+1]); // normalizes
   /*   
	     ---- LSTM momentum update for t= N+1 ---- 
   */
   v_g_o_new[N+1] = (momentum * v_g_o[N] + epsilon*thetv_o_scalar*n[N]);
   v_n_d_new[N+1] = (momentum * v_n_d[N] + epsilon*thnv_d_scalar*n[N]);
   v_g_i_new[N+1] = (momentum * v_g_i[N] + epsilon*thnv_i_scalar*n[N]);
   v_g_f_new[N+1] = (momentum * v_g_f[N] + epsilon*thnv_f_scalar*n[N]);
   /*
      	 LSTM gate calculations  for forecast at t= N+1
   */
   g_o_new[N+1] = inv_logit(v_g_o_new[N+1] + thnw_o_scalar*h[N] + thnb_o_scalar);
   n_d_new[N+1] = tanh(v_n_d_new[N+1]+ thnw_d_scalar*h[N] + thnb_d_scalar);
   g_i_new[N+1] = inv_logit(v_g_i_new[N+1] + thnw_i_scalar*h[N] + thnb_i_scalar);
   g_f_new[N+1] = inv_logit(v_g_f_new[N+1] + thnw_f_scalar*h[N] + thnb_f_scalar);
   /* 
    	 LSTM cell and hidden state for forecast at t= N+1
   */
   C_new[N+1] =   g_i_new[N+1] *n_d_new[N+1]  + g_f_new[N+1] *C[N]; 
   h_new[N+1] =   g_o_new[N+1] * tanh(C_new[N+1]);
   mu_regime_new[N+1] =  dot_product(mu_1_state_sorted,   prob_forecast[N+1]);
   beta10_regime_new[N+1] = dot_product(beta10_state_sorted,   prob_forecast[N+1]);
   tau_regime_new[N+1] = dot_product(tau_state_sorted,   prob_forecast[N+1]);
   n_new[N+1] =  normal_rng(mu_regime_new[N+1] + beta10_regime_new[N+1]*h[N]+ rho*tau_regime_new[N+1]*exp(-0.5*z[N])*y[N] , 
                 tau_regime_new[N+1]*sqrt(1-square(rho)));
   /*   
	    ---- Simulating new log-volatility (z_new) at t= N+1 ---- 
	           ( Model 3, pg 64  WANG [2016] )
   */
   z_new[N+1] = n_new[N+1] + phi*(z[N]) + psi*(z[N-1]) +
   monday_z* date[N+1,1] + tuesday_z*date[N+1,2] + wednesday_z*date[N+1,3] 
   + thursday_z*date[N+1,4] + friday_z*date[N+1,5] + kpi*signals[N];	
   /*  
     	---- Simulating new log-returns (Posterior) at t= N+1 ---- 
   */
   y_sim[N+1] = student_t_rng(3.82426 ,alpha, exp(z_new[N+1]/2)); 
   //
   /*   
	    ---- Simulating new log-volume innovations at t= N+1  ---- 
   */
   mu_volume_high_new[N+1] = mu_volume1;
   mu_volume_low_new[N+1] = mu_volume2;
   // Simulated Short-term volatility evolution
   for (l in 1:M){
     mu_volume_high_new[N+1] += psi_volume1[l] * (h_volume_high[N - l] - mu_volume1);   // Frac diff terms
   }
   for (i in 1:p){
     mu_volume_high_new[N+1] += phi_volume1[i] * (h_volume_high[N-i]- mu_volume1);     // AR terms
   }
   for (j in 1:q){
     mu_volume_high_new[N+1] += theta_volume1[j] * (h_volume_high[N-j]- mu_volume_high[N-j]);	// MA terms 
   }
   // Simulated Long-term volatility evolution	   
   for (l in 1:M){   
     mu_volume_low_new[N+1]  += psi_volume2[l] * (h_volume_low[N - l]- mu_volume2);    // Frac diff terms
   }
   for (i in 1:p){
     mu_volume_low_new[N+1] += phi_volume2[i] * (h_volume_low[N-i]-mu_volume2);       // AR terms
   }
   for (j in 1:q){
     mu_volume_low_new[N+1] += theta_volume2[j] * (h_volume_low[N-j]- mu_volume_low[N-j]);	  // MA terms 
   }
   //
   h_volume_high_new[N+1] = normal_rng(mu_volume_high_new[N+1], tau1);
   h_volume_low_new[N+1]  = normal_rng(mu_volume_low_new[N+1], tau2);
   //
   /*
             ---- Simulating new residuals at t= N+1  ----
						 
         Simulates new residuals based on the log-volatility 
		 from the SV-ARFIMA above.
         
   */
   residuals_new[N+1] = gev_rng(0.0,exp(h_volume_high_new[N+1]/2 + h_volume_low_new[N+1]/2), -0.180072);
   //
   /* 
     	---- Simulating new volume conditional mean at t= N+1 ----     
   */ 
   fourier_terms_2 = 0.0;
   if (k1 > 0) {
       for (j in 1:k1) { 
		  fourier_terms_2 +=  alfa1[j] * sin((2*pi()*f1[j]) *(N+1)) + beta1[j] * cos((2*pi()*f1[j]) * (N+1));
         }
   }
   periodic_component_sum_new[N+1] = fourier_terms_2;
   /*			
	     ---- Conditional mean iterations for forecasting at t= N+1 ----
	                   *** Simulating new jumps ***
   */
   is_slab = 0;
   is_slab = bernoulli_rng(pi_z); // 1 = slab (Huge jump), 0 = spike (almost zero)
   if (is_slab == 1){
      jumps_new[N+1] = normal_rng(0, sigma_slab);
    }else{
      jumps_new[N+1] = normal_rng(0, sigma_spike);
   }
   //
   mean_volume_new[N+1] = m0;
   mean_volume_new[N+1] += m1*exp(z_new[N+1]/2) + m2*square(exp(z_new[N+1]/2)) + m3*z_new[N+1];
   mean_volume_new[N+1] += periodic_component_sum_new[N+1];
   mean_volume_new[N+1] += jumps_new[N+1]; 
   mean_volume_new[N+1] += di*Future_DI[N];
   /*   
	     ---- GRU momentum update for t= N+1 ---- 
   */
   v_g_f_gru_forward_new[N+1] = (momentum * v_g_f_gru_forward[N] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward[N]);
   g_f_gru_forward_new[N+1] =   inv_logit(v_g_f_gru_forward_new[N+1] + thnv_f_gru_scalar_forward*v[N] + thnb_f_gru_scalar_forward);
   v_h_gru_forward_new[N+1] =   (momentum *v_h_gru_forward[N] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward_new[N+1] + h_gru_forward[N]));
   h_gru_forward_new[N+1] =     tanh(v_h_gru_forward_new[N+1] + theta_h_gru_scalar_forward*v[N] + thnb_h_gru_scalar_forward);
   h_gru_forward_new_updated[N+1] = (1- g_f_gru_forward_new[N+1])*h_gru_forward[N] +  g_f_gru_forward_new[N+1] *h_gru_forward_new[N+1];     
   /*  
	     *** For the backward GRU in forecasting:
         Freezing the last backward hidden state.
         This is the most common approach for practical forecasting with B-RNNs.
         Use the last computed backward state from observed data
		 
   */
   h_gru_concatenate_new[N+1] = gru_hidden_state_scalar*((h_gru_backward_updated[N] + h_gru_forward_new_updated[N+1])/2);
   if (k > 0)  
    for (m in 1:k) {
      for (i in 1:k) {
         mean_volume_new[N+1] += g_1[i, m + 1] * v[(N+1)-m];
        }
    }
    if(p > 0) for (i in 1:p){ // If AR component is specified
		 mean_volume_new[N+1] += phi_mean[i]*(h_gru_concatenate_new[(N+1)-i]); // AR terms on past observed terms.
	}
    if(q > 0) for(j in 1:q){ // If MA component is specified
         mean_volume_new[N+1] += theta_mean[j] * residuals_new[(N+1)-j]; // MA terms on past residuals.
    }
	//
	mean_volume_new[N+1] = swish(mean_volume_new[N+1] , beta_swish);
	//
	/*    
	     ---- Simulating new log-volume (Posterior) at t= N+1 ---- 
	*/
	v_sim[N+1] = gev_rng(mean_volume_new[N+1],exp(h_volume_high_new[N+1]/2 + h_volume_low_new[N+1]/2), -0.180072); 
    // 
    /* 
       ---- Regime forecast from t = [N+2, N+T_forecast] ---- 
    */
    for(t in (N+2):(N+T_forecast)) {
     prob_forecast[t] = to_vector(A' * to_matrix(prob_forecast[t-1]));
     prob_forecast[t] /= sum(prob_forecast[t]);
	 //
	 v_g_o_new[t] = (momentum * v_g_o_new[t-1] + epsilon*thetv_o_scalar*n_new[t-1]);
     v_n_d_new[t] = (momentum * v_n_d_new[t-1] + epsilon*thnv_d_scalar*n_new[t-1]);
     v_g_i_new[t] = (momentum * v_g_i_new[t-1] + epsilon*thnv_i_scalar*n_new[t-1]);
     v_g_f_new[t] = (momentum * v_g_f_new[t-1] + epsilon*thnv_f_scalar*n_new[t-1]);
	 /*
      	 LSTM gate calculations  for forecast
	 */
	 g_o_new[t] = inv_logit(v_g_o_new[t] + thnw_o_scalar*h_new[t-1] + thnb_o_scalar);
     n_d_new[t] = tanh(v_n_d_new[t]+ thnw_d_scalar*h_new[t-1] + thnb_d_scalar);
	 g_i_new[t] = inv_logit(v_g_i_new[t] + thnw_i_scalar*h_new[t-1] + thnb_i_scalar);
     g_f_new[t] = inv_logit(v_g_f_new[t] + thnw_f_scalar*h_new[t-1] + thnb_f_scalar);
	 /* 
    	 LSTM cell and hidden state for forecast
	 */
     C_new[t] =   g_i_new[t] *n_d_new[t]  + g_f_new[t] *C_new[t-1]; 
     h_new[t] =   g_o_new[t] * tanh(C_new[t]);
     mu_regime_new[t] =  dot_product(mu_1_state_sorted,   prob_forecast[t]);
	 beta10_regime_new[t] = dot_product(beta10_state_sorted, prob_forecast[t]);
	 tau_regime_new[t] = dot_product(tau_state_sorted,  prob_forecast[t]);
     n_new[t] =  normal_rng(mu_regime_new[t] + beta10_regime_new[t]*h_new[t-1] + rho*tau_regime_new[t]*exp(-0.5*z_new[t-1])*y_sim[t-1] ,
   	                        tau_regime_new[t]*sqrt(1-square(rho)));
     //
     z_new[t] = n_new[t] + phi*(z_new[t-1]) + psi*(z_new[t-2]) 
	 + monday_z* date[t,1] + tuesday_z*date[t,2] + wednesday_z*date[t,3]
	 + thursday_z*date[t,4] + friday_z*date[t,5] + kpi*signals[t-1];	
     /*  
     	 Simulating new log-returns (Posterior)
	 */
	 y_sim[t] = student_t_rng(3.82426 ,alpha, exp(z_new[t]/2)); 
     //
     /*   
	    ---- Simulating new log-volume innovations  ---- 
     */
	 mu_volume_high_new[t] = mu_volume1;
	 mu_volume_low_new[t]  = mu_volume2;
	 // Simulated Short-term volatility evolution
     for (l in 1:M){
       mu_volume_high_new[t] += psi_volume1[l] * (h_volume_high_new[t - l] - mu_volume1);    // Frac diff terms
	 }
	 for (i in 1:p){
       mu_volume_high_new[t] += phi_volume1[i] * (h_volume_high_new[t - i]- mu_volume1);     // AR terms
	 }
	 for (j in 1:p){
       mu_volume_high_new[t] += theta_volume1[j] * (h_volume_high_new[t - j]- mu_volume_high_new[t-j]);	 // MA terms
	 }
	 // Simulated Long-term volatility evolution	 
     for (l in 1:M){	   
       mu_volume_low_new[t]  += psi_volume2[l] * (h_volume_low_new[t - l] - mu_volume2);     // Frac diff terms
	 }
	 for (i in 1:p){
       mu_volume_low_new[t] += phi_volume2[i] * (h_volume_low_new[t - i]-mu_volume2);        // AR terms
	 }
	 for (j in 1:p){
        mu_volume_low_new[t] += theta_volume2[j] * (h_volume_low_new[t - j]-mu_volume_low_new[t-j]);	 // MA terms 
	 }
     //
     h_volume_high_new[t] = normal_rng(mu_volume_high_new[t], tau1);
     h_volume_low_new[t]  = normal_rng(mu_volume_low_new[t], tau2);
     //  
     /*
           ---- Simulating new residuals  ----
     */	  
     residuals_new[t] = gev_rng(0.0,exp(h_volume_high_new[t]/2 + h_volume_low_new[t]/2), -0.180072);
     //	 
	 /*  
     	 Simulating new log-volume conditional mean 
	 */
	 fourier_terms_3 = 0.0;
     if (k1 > 0) {
       for (j in 1:k1) { 
          fourier_terms_3 += alfa1[j] * sin((2*pi()*f1[j]) *(t)) + beta1[j] * cos((2*pi()*f1[j]) * (t));
          }
     } 
     periodic_component_sum_new[t] = fourier_terms_3;
	 /*			
	      Simulating new jumps 
	 */
	 is_slab = 0;
     is_slab = bernoulli_rng(pi_z); 
     if (is_slab == 1){
        jumps_new[t] = normal_rng(0, sigma_slab);
     }else{
        jumps_new[t] = normal_rng(0, sigma_spike);
     }
	 //
	 mean_volume_new[t] = m0;
     mean_volume_new[t] += m1*exp(z_new[t]/2) + m2*square(exp(z_new[t]/2)) + m3*z_new[t];	 
     mean_volume_new[t] += periodic_component_sum_new[t];
	 mean_volume_new[t] += jumps_new[t];
     mean_volume_new[t] += di*Future_DI[t-1];     
     /*  
         GRU terms
     */
	 v_g_f_gru_forward_new[t] = (momentum * v_g_f_gru_forward_new[t-1] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward_new[t-1]);
     g_f_gru_forward_new[t] = inv_logit(v_g_f_gru_forward_new[t] + thnv_f_gru_scalar_forward*v_sim[t-1] + thnb_f_gru_scalar_forward);
	 v_h_gru_forward_new[t] =  (momentum *v_h_gru_forward_new[t-1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward_new[t] + h_gru_forward_new[t-1]));
	 h_gru_forward_new[t] =  tanh(v_h_gru_forward_new[t] + theta_h_gru_scalar_forward*v_sim[t-1] + thnb_h_gru_scalar_forward);
	 h_gru_forward_new_updated[t] = (1- g_f_gru_forward_new[t])*h_gru_forward_new[t-1] +  g_f_gru_forward_new[t] *h_gru_forward_new[t]; 
	 h_gru_concatenate_new[t] = gru_hidden_state_scalar*((h_gru_backward_updated[N] + h_gru_forward_new_updated[t])/2);			
     if (k > 0)  
	   for (m in 1:k){
         for (i in 1:k) {
           mean_volume_new[t] += g_1[i, m + 1] * v_sim[t - m];
          }
        }
        if(p > 0) for (i in 1:p){ // If AR component is specified
		   mean_volume_new[t] += phi_mean[i]*(h_gru_concatenate_new[t-i]); // AR terms on past observed terms.
	    }
        if(q > 0) for(j in 1:q){ // If MA component is specified
           mean_volume_new[t] += theta_mean[j] * residuals_new[t-j]; // MA terms on past residuals.
        } 
	mean_volume_new[t] = swish(mean_volume_new[t] , beta_swish);
	/*  
     	---- Simulating new log-volume (Posterior) ---- 
    */
	v_sim[t] = gev_rng(mean_volume_new[t],exp(h_volume_high_new[t]/2 + h_volume_low_new[t]/2), -0.180072);	 
	}
}

"""   

Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM  = pystan.StanModel(model_code=Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM)

#   Data truction to fit log returns length

KPI_lenght_to_trim =   len(Fundamental_analysis_KPI_PETR3_SA) - len(PETR3_SA_Close_fractional_difference_Linear_filtred)
Time_length_to_trim =  len(PETR3_SA['Volume']) - len(PETR3_SA_Close_fractional_difference_Linear_filtred)
Volume_length_to_trim =  len(PETR3_SA_volume_Linear_filtred_train) - len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)
Future_DI_length_to_trim = len(cdi_scaled) - len(PETR3_SA_Close_fractional_difference_Linear_filtred)


data  = ({
          'N' : len(PETR3_SA_Close_fractional_difference_Linear_filtred_train),
          'T_forecast' : forecasting_extrapolation_lengh,
		  'date': dates_dummies[Time_length_to_trim:].values.astype(np.int64),
		  'p' : 1,  # Figure_1.1g
		  'q' : 1,  # Figure_1.1h
		  'k' : 1,  # number of peaks in Figure_1-1i (Spectogram analysis)
		  'M' : 30, # Abritary choice , but literature recommends something around 30 lags.(To capture long range clusters)
          'y':  PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten().astype(np.float64),
		  'v':  PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.flatten().astype(np.float64),
		  'k1':  1, # Related to f1
		  'signals': Fundamental_analysis_KPI_PETR3_SA[KPI_lenght_to_trim:]["signal"].values.flatten().astype(np.int64),
          'Future_DI': cdi_scaled[Future_DI_length_to_trim:].values.flatten().astype(np.float64),
		  'f1': [1/61.50]
	     })
		 
		 
control = {}
control['max_treedepth'] = 7
control['adapt_delta'] = 0.9999

fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM = Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM.sampling(data=data, iter=15000, chains=1, warmup=7000 ,thin=1, seed=101, n_jobs = 1, control=control)

# Parameters of interest and their dimensions
param_names = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM.sim['pars_oi']
param_dims = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM.sim['dims_oi']
param_dict = dict(zip(param_names, param_dims))

# EXCLUDE: everything not declared in the `parameters {}` block

   	 
EXCLUDE = {
    # Transformed parameters
    "phi_mean", "theta_mean", "mu_mean", "residuals", "g_1", "u_1", "mean_volume", "fourier_terms_1", "periodic_component_sum", "rho", "mu_regime",
    "beta10_regime", "tau_regime", "phi", "psi", "psi_volume1", "psi_volume2","phi_volume1","phi_volume2","theta_volume1","theta_volume2",
    "mu_volume_high", "mu_volume_low", "z", "h", "C", "mu","n_d","g_i","g_o","g_f","v_g_o","v_n_d","v_g_i","v_g_f","accumulator","mu","log_alpha",
	"mu_1_state_sorted","beta10_state_sorted","tau_state_sorted","A","log_A","probability","v_g_f_gru_forward","v_h_gru_forward","v_g_f_gru_backward",
	"v_h_gru_backward","g_f_gru_forward","h_gru_forward","h_gru_forward_updated","g_f_gru_backward","h_gru_backward","h_gru_backward_updated",
	"h_gru_concatenate"
    
    # Generated quantities
    "mean_volume_new", "jumps_new", "is_slab", "residuals_new", "mu_regime_new", "beta10_regime_new","tau_regime_new","prob_forecast",
    "log_lik", "n_new", "mu_volume_high_new", "mu_volume_low_new", "h_volume_high_new", "h_volume_low_new","y_sim","v_sim","z_new",
    "h_new", "C_new", "n_d_new", "g_i_new","g_o_new","g_f_new","v_g_o_new","v_n_d_new","v_g_i_new","v_g_f_new","v_g_f_gru_forward_new",
    "v_h_gru_forward_new", "g_f_gru_forward_new", "h_gru_forward_new","h_gru_forward_new_updated","h_gru_concatenate_new","fourier_terms_2",
	"fourier_terms_3","periodic_component_sum_new"
}

# Filter only those in the `parameters` block
filtered_param_dict = {
    name: dim for name, dim in param_dict.items()
    if name not in EXCLUDE
}

# Count total number of scalar parameters
total_filtered_params = sum(np.prod(dim) if dim else 1 for dim in filtered_param_dict.values())

print("Total number of scalar parameters (parameters block only):", total_filtered_params)
#   Total number of scalar parameters (parameters block only): 1547

Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM = az.from_pystan(
    posterior=fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM,
    observed_data=["y", "v"],
    log_likelihood="log_lik"
)

# --- Calculate WAIC ---
waic_result = az.waic(Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM)

print("\n--- WAIC Results ---")
print(waic_result)

#   Computed from 8000 by 242 log-likelihood matrix

#             Estimate       SE
#   elpd_waic    24.73    19.20
#   p_waic      254.18        -



###################################*/                      \*###################################
#
#    Computed from 8000 by 242 log-likelihood matrix
#
#                Estimate     SE
#    elpd_waic      24.73  19.20
#    p_waic        254.18      -
#
#    There has been a warning during the calculation. Please check the results.
#
#    
#    24.73 (elpd_waic)  : 
#
#        *   The value elpd_waic = 24.73 represents the expected log predictive density 
#		     of the model for new data, based on posterior simulations.More positive (less negative) values 
#		     indicate better out-of-sample predictive performance. In absolute terms, the value itself has no specific threshold,
#		     it is only meaningful when compared to other models fitted to the same data.
#
#            A model with higher elpd_waic (closer to zero) is generally preferred, assuming differences exceed the combined standard
#		     error of comparison.
#
#	 19.20 (SE - Standard Error)  : 
#
#
#         *  This represents the uncertainty in the elpd_waic estimate.
#            A smaller standard error indicates a more precise estimate. When comparing models,
#	         if the difference in elpd_waic values is larger than the sum of their standard errors,
#	         it suggests a statistically significant difference in predictive performance.
#
#            For example, if:
#
#               Model A: elpd_waic = -224.20, SE = 21.64
#
#               Model B: elpd_waic = -200.00, SE = 18.00
#               
#               Difference = 24.20,
#               Sum of SEs = 21.64 + 18.00 = 39.64
#               Since 24.20 < 39.64 -> not significant
#
#	 p_waic: 254.18  :   
#
#
#         *  Although the model contains 1547 declared scalar parameters, 
#		     the effective number of parameters estimated by p_waic is only 254.18.
#            This substantial difference indicates that many parameters are regularized,
#			 latent, or weakly identified, and thus do not substantially increase the 
#			 model's flexibility or risk of overfitting.
#
#            The model leverages hierarchical structure, strong priors, and latent processes 
#			 (like the Pointwise , Quasi-LSTM, Quasi-GRU, and HMM components), which constrain posterior variance.
#
#            A lower p_waic than the raw parameter count is expected and often desirable,
#			 as it reflects model parsimony and Bayesian regularization in action
#
#            "Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM"  model has moderate 
#            predictive ability, with some room for improvement.   
#
###################################*/                      \*###################################

# Define the directory and filename
save_directory = r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA'
filename = 'Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM.pkl'

# Construct the full file path
full_path = os.path.join(save_directory, filename)

# Save the model
with open(full_path, 'wb') as f:
    pickle.dump(fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM, f)

# Load the model
with open(full_path, 'rb') as f:
    Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM = pickle.load(f)

#=============*/        \*=================	    

samples = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_MS_SVVol_MomentumLSTM.extract() 

#=============*/        \*=================	

   
volume_hat = samples["v_sim"]
  
pd.DataFrame(volume_hat).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\Volume_estimated.csv', index=None)
Volume_estimated = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/Volume_estimated.csv")

# ---- Plotting ----
#          (Volume train sample)

Volume_estimated_mean = Volume_estimated.mean(axis=0)

x_lower, x_upper = np.percentile(Volume_estimated, [2.5, 97.5], axis=0)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:] , label='Observed desazionated volume train (v)', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], Volume_estimated_mean[:-forecasting_extrapolation_lengh], label='Volume In Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], x_lower[:-forecasting_extrapolation_lengh],x_upper[:-forecasting_extrapolation_lengh], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Volume in Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()


# Figure_2.1a_In_Sample_Volume.


############################
#   In Sample regression   #
#        metrics KPI       #
#                          #
############################                                                                                                                                                            

MSE = mean_squared_error(Volume_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel(),PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)
#  RMSE: 0.426682


#             -------------------*/          \*-------------------
#
#   RMSE (Root Mean Squared Error):
#
#   The RMSE value of 0.4267 indicates the average magnitude of forecast errors.
#   In practical terms, it means that, on average, the model’s predicted volume
#   deviates from the actual observed volume by about 0.43 units.
#
#   *** Interpretation with Scaled Data (0.00 to 6.63): ***
#
#       Magnitude of Error:  Given that both observed and estimated volumes are scaled
#       between 0.00 and 6.63, an RMSE of 0.43 represents a small average deviation.
#
#       Relative to Scale:  This error is approximately (0.43 / 6.63) = 6.5% of the full scale range.
#       Such a small proportion indicates that the model captures most of the variation
#       in the observed series accurately.
#
#       Visual Consistency:  This aligns with Figure_2.1a, where the "Volume In Sample"
#       (blue line) tracks the "Observed desazonized volume" (gray line) closely, with
#       most observations lying within the 95% credible interval.
#
#       Overall:  An RMSE around 0.43 on this scale indicates a strong in-sample fit,
#       suggesting that the model effectively learns the volume dynamics.
#
#             -------------------*/          \*------------------- 

mae = mean_absolute_error(Volume_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel(),PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.ravel())
print('MAE: %f' % mae)
#   MAE:  0.156023

#             -------------------*/          \*------------------- 
#
#   MAE (Mean Absolute Error):
#
#   The MAE value of 0.1560 indicates that, on average, the absolute difference
#   between predicted and actual volumes is about 0.16 units.
#
#   *** Interpretation with Scaled Data (0.00 to 6.63): ***
#
#       Direct Interpretability:  Unlike RMSE, MAE is expressed in the same units
#       as the target variable, making it directly interpretable. Given that the
#       data are scaled between 0.00 and 6.63, an average absolute error of 0.16
#       is quite small.
#
#       Robustness to Outliers:  MAE is less sensitive to large deviations than RMSE,
#       as it uses linear rather than squared penalties for errors.
#
#       Comparative Performance:
#           * RMSE = 0.4267
#           * MAE  = 0.1560
#         The fact that MAE < RMSE is expected; their moderate difference (~0.27)
#         indicates a few larger errors — visible as occasional spikes missed by
#         the forecast (e.g., around Sept 2024 and Mar 2025) — but overall the
#         fit remains strong and consistent.
#
#       Overall:  An MAE of 0.16 confirms that the model performs well in capturing
#       the typical level and variation of the volume series in-sample.
#
#             -------------------*/          \*------------------- 

coefficient_of_dermination = r2_score(PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.ravel(), Volume_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel())
print('R2: %f' % coefficient_of_dermination)
#   R2:  0.772082 

#             -------------------*/          \*------------------- 
#
#   R-squared (R²): Coefficient of Determination
#
#   The R-squared value of 0.772 indicates that approximately 77.2% of the variance
#   in the observed filtered volume (PETR3_SA_volume_Linear_filtred_train)
#   can be explained by the model’s estimated volume (Volume_estimated_mean).
#
#   *** Interpretation: ***
#
#       Strength of Fit:
#       An R-squared of 0.77 is generally considered a reasonably good fit
#       for a regression model, especially in time-series forecasting contexts.
#       It suggests that the model effectively captures a large proportion of the
#       variability present in the observed volume data.
#
#       Proportion of Variance Explained:
#       This means that the dynamic patterns learned by the model
#       (e.g., through LSTM, GRU, HMM, and other structural components)
#       account for more than three-quarters of the total variation in volume.
#
#       Implications for Prediction:
#       A high R-squared such as this indicates that the model’s predictions
#       not only produce small average errors (as shown by the MAE and RMSE)
#       but also track the temporal fluctuations — peaks, troughs, and trends —
#       of the observed volume series closely.
#
#       Context with Scaled Data (0.0–6.63):
#       Given that the volume data are scaled within this range,
#       an R-squared of 0.77 confirms that the model explains the majority
#       of movements in the series with high accuracy.
#
#       Complementary Metric:
#       While R-squared quantifies the proportion of variance explained,
#       it should always be interpreted alongside error metrics such as RMSE and MAE.
#       The combination of a high R-squared (0.77), low RMSE (~0.43), and low MAE (~0.16)
#       strongly supports that the model provides a robust and reliable in-sample fit.
#
#             -------------------*/          \*------------------- 




Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.ravel(), Volume_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
#   statistic: 0.090909   ,pvalue: 0.270392


#             -------------------*/          \*------------------- 
#
#   Kolmogorov–Smirnov (KS) Two-Sample Test for Distribution Adherence
#
#   Statistic: 0.0909
#   p-value:   0.2704
#
#   Purpose:
#   The KS test evaluates whether the distribution of the model’s estimated
#   volume differs significantly from the distribution of the actual observed volume.
#   It compares their empirical cumulative distribution functions (ECDFs)
#   to measure the largest absolute deviation between them.
#
#   *** Interpretation: ***
#
#       Null Hypothesis (H0):
#           The two samples (observed and estimated volumes) are drawn from
#           the same underlying distribution.
#
#       Alternative Hypothesis (H1):
#           The two samples come from different distributions.
#
#       p-value = 0.2704 > 0.05:
#           Since the p-value is greater than the 5% significance level,
#           we fail to reject the null hypothesis.
#           This suggests there is no statistically significant difference
#           between the distribution of the model’s estimated volumes
#           and that of the observed data.
#
#       KS Statistic = 0.0909:
#           This value represents the maximum absolute difference between
#           the cumulative distribution functions (CDFs) of the two samples.
#           A smaller KS statistic indicates closer agreement between
#           the two distributions.
#
#       Conclusion:
#           The model reproduces the overall shape of the empirical volume
#           distribution reasonably well. The difference is not large enough
#           to be considered statistically significant at the 5% level.
#
#             -------------------*/          \*-------------------


# ---- QQplot between the two samples ----

# Sort both arrays ( It's needed to align the percentiles in both distributions)
sorted_obs = np.sort(PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.ravel().astype(np.float64))
sorted_hat = np.sort(Volume_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel().astype(np.float64))


# Q-Q Plot between y_obs and y_hat
plt.figure(figsize=(6, 6))
plt.plot(sorted_obs, sorted_hat, 'o')
plt.plot([sorted_obs.min(), sorted_obs.max()],
         [sorted_obs.min(), sorted_obs.max()], 'r--')  # 45-degree line
plt.title("Q-Q Plot: v_hat vs v_obs")
plt.xlabel("Observed Quantiles for Volume In-Sample")
plt.ylabel("Predicted Quantiles for Volume In-Sample")
plt.grid(True)
plt.show()


# Figure_2.1b_Q-Q_Plot_between_volume_In_Sample_and_estimated


#             -------------------*/          \*-------------------
#
#   Q–Q Plot Interpretation: Predicted (v_hat) vs. Observed (v_obs) Volume Quantiles
#
#   The in-sample Q–Q plot comparing predicted and observed volume quantiles
#   provides a clear assessment of the model’s distributional calibration.
#
#   Central Fit:
#       For most of the distribution (up to approximately the 3.5–4.0 quantile range),
#       the predicted quantiles lie very close to the 45° reference line.
#       This indicates that the model captures the central behavior of volume dynamics
#       — including the median and most frequent values — with high accuracy.
#
#   Tail Behavior and Bias:
#       A noticeable departure from the reference line occurs at the upper quantiles
#       (Observed Quantiles > ~3.5). The predicted quantiles (y-axis) fall consistently
#       below the 45° line in this region, indicating that the model tends to
#       underpredict extreme high-volume observations.
#       In other words, while the model tracks moderate fluctuations well,
#       it underestimates the magnitude of the most intense volume spikes.
#
#   Overall Conclusion:
#       The Q–Q plot suggests that the model is well-calibrated for typical
#       volume activity, but exhibits a systematic in-sample bias in the upper tail.
#       This limits its ability to fully reproduce the heavy-tailed characteristics
#       observed in the empirical volume distribution.
#
#             -------------------*/          \*-------------------


####################################
#  Out of Sample                   #
#        predictive KPI peformace  #
#                                  #
####################################

# (Volume test sample) 

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], PETR3_SA_volume_Linear_filtred_test , label='Observed desazionated volume test (v)', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], Volume_estimated_mean[-forecasting_extrapolation_lengh:], label='Volume Out of Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], x_lower[-forecasting_extrapolation_lengh:],x_upper[-forecasting_extrapolation_lengh:], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Volume Out of Sample Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()

# Figure_2.1c_Out_of_Sample_Volume

#  Due to the very small out-of-sample size (only 5 observations),
#  MAE was selected as the primary regression KPI,
#  since it directly measures individual forecast accuracy.

mae = mean_absolute_error(Volume_estimated_mean[-forecasting_extrapolation_lengh:].values.ravel(),PETR3_SA_volume_Linear_filtred_test.values.ravel())
print('MAE: %f' % mae)
#   MAE:  0.466743


#             -------------------*/          \*-------------------
#
#   Out-of-Sample MAE (Mean Absolute Error):
#
#   MAE: 0.466743
#
#   Interpretation:
#
#   Forecast Accuracy (MAE): 
#
#        The MAE of 0.466743 indicates that, on average, the forecast (the blue line) 
#        is off by about 0.47 units of volume from the actual observed volume (the grey line)
#        in the out-of-sample period. Given that the volume values range from 1 to about 4 in the test set,
#        an MAE of ≈0.47 seems reasonably good for the initial part of the forecast, perhaps less so towards the end.
#
#
#    Tracking/Bias:
#
#        The forecast (blue line) generally tracks the observed data (grey line) quite closely for the first 3 or 4 points 
#        (from 06-09 00 to 06-12 00). In this initial segment, the forecast appears to be quite accurate, which is consistent
#        with a relatively low MAE. For the final observation (at 06-13 00), the observed volume (≈3.75) appears slightly lower
#        than the forecast volume (≈4.0). The forecast slightly overestimated the final point.
#
#
#    Uncertainty (95% Credible Interval):
#
#         The 95% Credible Interval (light blue shaded area) starts relatively narrow and then expands dramatically over the 5 
#         observation points.The observed data (grey line) remains entirely within the 95% credible interval for all 5 observations. 
#         This is a positive sign, indicating that the model's estimate of uncertainty is correct or conservative, as the actual outcomes
#         fell within the predicted range.
#
#         The massive expansion of the interval towards the last point (reaching a high of ≈21 and a low of ≈−1) highlights the
#         growing uncertainty in the volume prediction over time. This is typical for time series forecasts, especially those based 
#         on models where uncertainty compounds over the prediction horizon. The model quickly loses confidence in the precision of its mean estimate.
#
#    Conclusion based on evidence: 
#          
#         The model provided a good short-term forecast, as evidenced by the low MAE and the close tracking of the observed data.
#         However, the rapidly expanding 95% credible interval indicates that the long-term prediction (even just across 5 steps)
#         is highly uncertain, suggesting the model is capturing a potentially volatile or unpredictable component in the volume series.
#
#             -------------------*/          \*-------------------


#########################################
#   Checking for volume residuals       #
#                    homocedasticity    #
#########################################

def calculate_residuals(observed_values, predicted_location, predicted_scale, epsilon=1e-8):
    """
     
    References :

        CRYER, Jonathan D.; CHAN, Kung-Sik. Time Series Analysis: With Applications in R. 2. ed. New York: Springer, 2008.

        HILPISCH, Yves. Python for Finance: Mastering Data-Driven Finance. 2. ed. Sebastopol, CA: O’Reilly Media, 2019.

        HYNDMAN, Rob J.; ATHANASOPOULOS, George. Forecasting: Principles and Practice. 3. ed. Melbourne: OTexts, 2021.
        Disponível em: [https://otexts.com/fpp3/](https://otexts.com/fpp3/). Acesso em: 25 set. 2025.

        TSAY, Ruey S. *Analysis of Financial Time Series . 3. ed. Hoboken, NJ: John Wiley & Sons, 2010.
    
    
    #   Calculates standardized residuals: (Observed - Predicted Location) / Predicted Scale.

    Args:
        observed_values (array-like): The actual observed values
        predicted_location (array-like or scalar): The model's conditional mean (location).
        predicted_scale (array-like): The model's conditional standard deviation (scale/volatility).
        epsilon (float): A small value used to check for near-zero division in the scale.

    Returns:
        numpy.ndarray: The standardized residuals.
    """
    observed_values = np.asarray(observed_values, dtype=np.float64).ravel()
    predicted_location = np.asarray(predicted_location, dtype=np.float64).ravel()
    predicted_scale = np.asarray(predicted_scale, dtype=np.float64).ravel()
    ########*/          \*########
    #
    # 1. Shape/Length Checks
    #
    ########*/          \*########
    if observed_values.shape != predicted_scale.shape:
        raise ValueError("Lengths of observed_values and predicted_scale must match.")
    ########################*/          \*########################
    #
    # Check if location is a scalar or matches the length of the data
    #
    ########################*/          \*########################
    if predicted_location.size > 1 and observed_values.shape != predicted_location.shape:
        raise ValueError("Lengths of observed_values and predicted_location must match if predicted_location is an array.")
    ########################*/          \*########################
    #
    # 2. Safety Check for Zero Division
    #
    ########################*/          \*########################
    if np.any(predicted_scale < epsilon):
        raise ValueError(
            "Predicted scale contains values too close to zero (below {}). Cannot calculate standardized residuals.".format(epsilon)
        )
    ########*/          \*########
    #
    # 3. Calculation
    #
    ########*/          \*########
    return (observed_values - predicted_location) / predicted_scale
    
    
mean_volume = samples["mean_volume"]
  
pd.DataFrame(mean_volume).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\mean_volume.csv', index=None)
mean_volume = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/mean_volume.csv")
    
mean_volume_mean = mean_volume.mean(axis=0)

#=============*/        \*=================

h_volume_high = samples["h_volume_high"]
  
pd.DataFrame(h_volume_high).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\h_volume_high.csv', index=None)
h_volume_high = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/h_volume_high.csv")
    
h_volume_high_mean = h_volume_high.mean(axis=0)

#=============*/        \*=================

h_volume_low = samples["h_volume_low"]
  
pd.DataFrame(h_volume_low).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\h_volume_low.csv', index=None)
h_volume_low = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/h_volume_low.csv")
    
h_volume_low_mean = h_volume_low.mean(axis=0)

#=============*/        \*=================

log_volume_location = mean_volume_mean

pd.DataFrame(log_volume_location).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\log_volume_location.csv', index=None)
log_volume_location = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/log_volume_location.csv")

#=============*/        \*=================

log_volume_scale = np.exp(h_volume_high_mean/2 + h_volume_low_mean/2)

pd.DataFrame(log_volume_scale).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\log_volume_scale.csv', index=None)
log_volume_scale = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/log_volume_scale.csv")

model_0_residuals_log_volume = calculate_residuals(PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.flatten().astype(np.float64),
                   log_volume_location, log_volume_scale)
                   
volume_in_sample = Volume_estimated_mean[:-forecasting_extrapolation_lengh]

#=============*/        \*=================				   
# Plot Residuals vs. Fitted Values 
plt.figure(figsize=(10, 6))
sns.scatterplot(x=volume_in_sample, y=model_0_residuals_log_volume, alpha=0.7)
plt.axhline(0, color='red', linestyle='--', linewidth=0.8)
plt.title('Residuals vs. Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# The model shows good overall behavior regarding linearity, as the residuals are scattered
# randomly around zero. However, the trend toward greater dispersion of residuals at
# higher fitted values indicates that the assumption of homoscedasticity may be violated.
# If heteroscedasticity is significant, it does not invalidate the model, but it makes
# the standard errors (and consequently, confidence intervals and t-tests) less reliable.

# Figure_2.1d_Residuals_vs_Fitted Values(Log-volume).


# --- Breusch-Pagan Test ---
# Arguments: residuals, X (exog, independent variables)
# Returns: lagrange_multiplier, p_value_lm, fvalue, p_value_f
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model_0_residuals_log_volume, 
PETR3_SA_volume_Linear_filtred_train[Volume_length_to_trim:].values.astype(np.float64))


print(f"\n--- Breusch-Pagan Test ---")
print(f"Lagrange Multiplier p-value: {lm_pvalue:.4f}")
print(f"F-statistic p-value:         {f_pvalue:.4f}")
if lm_pvalue < 0.05:
    print("  -> Significant p-value suggests presence of heteroscedasticity (reject H0).")
else:
    print("  -> No significant evidence of heteroscedasticity (fail to reject H0).")

#   No significant evidence of heteroscedasticity (fail to reject H0).


#=============*/        \*=================	


log_z_new = samples["z_new"]
z_new = np.exp(log_z_new/2)

pd.DataFrame(z_new).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\Returns_volatility.csv', index=None)
Returns_volatility = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/Returns_volatility.csv")

# ---- Plotting ----
#          (Volatility)

Returns_volatility_mean = Returns_volatility.mean(axis=0)

x_lower, x_upper = np.percentile(Returns_volatility, [2.5, 97.5], axis=0)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif, Returns_volatility_mean, label='Volatility', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif, x_lower,x_upper, color='blue', alpha=0.2, label='95% Credible Interval')
# Assume T_train is the number of training observations
Time_train = len(PETR3_SA_Close_fractional_difference_Linear_filtred_train)
split_date_index = PETR3_SA_Dates_log_dif[Time_train - 1] # This is the last date of the training set
# --- Add the vertical line for in-sample / out-of-sample separation ---
# Use ax.axvline() to draw a vertical line at the specific x-coordinate (date or index)
ax.axvline(x=split_date_index, color='red', linestyle='--', linewidth=1.5, label='In-sample / Out-of-sample Split')
ax.set_ylabel('Value')
ax.set_title('Estimated Volatility')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()

# Figure_2.1e_Log_Returns_Volatility. 

#             -------------------*/          \*-------------------
#
#    The graph displays the estimated Volatility (blue line) over time, along with its 95%
#    Credible Interval (95% CI - blue shaded area). The dashed red vertical line marks the
#    split point between the training (In-sample) and test (Out-of-sample) data.
#
#    1. Volatility Behavior
#    
#        Volatility Clustering: The most evident characteristic is the clustering of volatility.
#        We observe clear periods where volatility is consistently high (peaks), followed by
#        periods where it is consistently low (valleys).
#    
#        Notable Peaks: 
#         
#        Volatility reaches significant peaks, approaching or exceeding 0.5 and,
#        in one case, nearly 0.8 (in 2025-04).
#    
#        Implication: 
#         
#        This behavior is typical of financial time series and suggests the model
#        is correctly capturing the  long-term dependence and 'market mood' 
#        (turbulent days tend to be followed by turbulent days).
#
#    2. Credible Interval Analysis (Uncertainty)
#    
#        Constant Amplitude: 
#         
#        Unlike the estimated volume graph, the 95% CI does not appear to widen
#        significantly as time progresses (within the In-sample period).
#
#        Implication: 
#
#        This suggests that the uncertainty in the volatility estimate is relatively
#        stable, regardless of whether we are in a high or low volatility period. The model has
#        consistent confidence in its estimate.
#        
#        Future Volatility (Out-of-sample): 
#         
#        The last In-sample point (before the red line) shows a slight peak. The CI remains 
#        stable or slightly narrows in the Out-of-sample section, with the mean volatility falling slightly. 
#        This indicates the model predicts a reduction (or stabilization) in volatility post-split, with controlled uncertainty.
#
#    3. Seasonality or Periodicity
#
#        Recurring Pattern: 
#
#        There is a suggestion of a semi-regular pattern in the volatility
#        peaks (e.g., late 2024, early 2025, and again in spring/summer 2025).
#    
#        Implication: 
#         
#        While volatility is inherently hard to predict, modeling could benefit if
#        seasonality is confirmed. The model appears to be capturing this recurrence.
#
#             -------------------*/          \*-------------------


y_hat = samples["y_sim"]
  
pd.DataFrame(y_hat).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\Log_Returns_estimated_model_0.csv', index=None)
Log_Returns_estimated_model_0 = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/Log_Returns_estimated_model_0.csv")
			
# ---- Plotting ----
#          (Log Returns train sample)  

y_hat_in_sample = Log_Returns_estimated_model_0.mean(axis=0)

x_lower, x_upper = np.percentile(Log_Returns_estimated_model_0, [2.5, 97.5], axis=0)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], PETR3_SA_Close_fractional_difference_Linear_filtred_train , label='Observed log returns train (v)', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], y_hat_in_sample[:-forecasting_extrapolation_lengh], label='Log returns In Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], x_lower[:-forecasting_extrapolation_lengh],x_upper[:-forecasting_extrapolation_lengh], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Log Returns in Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()

# Figure_2.1f_In_Sample_Log_Returns.


############################
#   In Sample regression   #
#        metrics KPI       #
#                          #
############################                                                                                                                                                            

MSE = mean_squared_error(y_hat_in_sample[:-forecasting_extrapolation_lengh].values.ravel(), PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)

#  RMSE: 0.230652

#             -------------------*/          \*-------------------
#
#    The graph compares the observed log returns (grey line) with the estimated in-sample log returns (blue line)
#    and the 95% Credible Interval (95% CI - blue shaded area).
#
#    1. Mean Forecast (Blue Line):
#    
#        The estimated log return (blue line) is virtually flat and centered around zero throughout the entire period.
#        
#        Implication: 
#         
#        This is a typical and expected behavior for models predicting financial asset log returns.
#        The standard assumption of the Efficient Market Hypothesis (EMH) is that the best unconditional forecast
#        for tomorrow's return is zero, as consistent price movements cannot be predicted. The model is correctly
#        capturing the series' mean, which is close to zero.
#
#    2. Tracking Observed Data (Grey Line):
#
#        The actual returns (grey line) show large fluctuations (volatility), yet their mean (the blue line) remains at zero.
#        
#        Implication: 
#        
#        The model is not attempting to predict individual daily peaks, but rather the process's mean, which is zero.
#        This is an intrinsic limitation when predicting the mean of returns in an efficient market.
#
#    3. Credible Interval (95% CI):
#    
#        The 95% CI covers the vast majority of observed returns. The widening and narrowing of the interval over time
#        (capturing periods of high and low dispersion) successfully reflects the volatility clustering analyzed previously.
#    
#        Implication: 
#         
#        The model is successful in estimating the uncertainty (volatility) of returns, even though it does not
#        predict the direction of the daily return.
#
#    4. RMSE Metric Analysis:
#
#       Interpretation: 
#
#       An RMSE of 0.230652 means that, on average, the deviation of the prediction (which is approx. 0) from
#       the observed log return is about 0.23 units.
#    
#       Context: 
#        
#       For return models, the RMSE effectively serves as an estimate of the "In-Sample Daily Volatility"
#       (standard deviation of returns). A non-zero RMSE is expected because volatility is always present.
#       The value 0.23 aligns with the "average" volatility range observed in the volatility graph, confirming that the RMSE
#       is a good estimate of the returns' uncertainty.
#
#             -------------------*/          \*-------------------

mae = mean_absolute_error(y_hat_in_sample[:-forecasting_extrapolation_lengh].values.ravel(),PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel())
print('MAE: %f' % mae)

#  MAE: 0.164696

#             -------------------*/          \*-------------------
#
#    Interpretation of MAE for Log Returns:
#       
#       The MAE value of 0.164696 indicates that, on average, the predicted log return (which is close to zero)
#       deviates from the observed log return by approximately 0.16 units.
#       For financial return models, MAE, like RMSE, serves as a measure of the average magnitude of returns,
#       reflecting the inherent volatility in the series.
#       The MAE is typically lower than the RMSE (which was 0.230652), suggesting the presence of large deviations
#       (outliers or high peaks of volatility) in the returns data, which are penalized more heavily by the RMSE.
#
#             -------------------*/          \*-------------------

coefficient_of_dermination = r2_score(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel(), y_hat_in_sample[:-forecasting_extrapolation_lengh].values.ravel())
print('R2: %f' % coefficient_of_dermination)

#  R2: 0.001089


#             -------------------*/          \*-------------------
#
#    An R2 value of 0.001089 is extremely low, as it is very close to zero.
#    Meaning: This indicates that the model explains only about 0.11% of the total variance in the log returns.
#
#    Context in Finance: 
#
#       This result is NOT a sign of a poor model; rather, it's an expected
#       and often desired outcome when modeling returns for liquid assets like stocks.
#       If R2 were high (e.g., above 0.5), it would imply that returns are highly predictable,
#       which would violate the Efficient Market Hypothesis (EMH).
#
#       The low R2 simply confirms what the returns graph showed: the best forecast for tomorrow's
#       return is the mean (≈0). The model is not (and should not be) successfully predicting
#       individual daily price movements (the peaks and valleys).
#       
#       The primary objective here is to capture volatility, not the mean direction.
#       In summary, the low R2 is consistent with an efficient market.
#
#             -------------------*/          \*-------------------

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel(), y_hat_in_sample[:-forecasting_extrapolation_lengh].values.ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))

#   statistic: 0.483471   ,pvalue: 0.000000

#             -------------------*/          \*-------------------
#
#    Since the p-value (0.000000) is much smaller than the standard significance level (α=0.05),
#    we REJECT the Null Hypothesis (H0).
#
#    Implication: 
#
#    The distributions of the observed returns and the predicted returns are statistically different.
#
#             -------------------*/          \*-------------------

# ---- QQplot between the two samples ----

# Sort both arrays ( It's needed to align the percentiles in both distributions)
sorted_obs = np.sort(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel().astype(np.float64))
sorted_hat = np.sort(y_hat_in_sample[:-forecasting_extrapolation_lengh].values.ravel().astype(np.float64))


# Q-Q Plot between y_obs and y_hat
plt.figure(figsize=(6, 6))
plt.plot(sorted_obs, sorted_hat, 'o')
plt.plot([sorted_obs.min(), sorted_obs.max()],
         [sorted_obs.min(), sorted_obs.max()], 'r--')  # 45-degree line
plt.title("Q-Q Plot: y_hat vs y_obs")
plt.xlabel("Observed Quantiles for Log-Returns In-Sample")
plt.ylabel("Predicted Quantiles for Log-Returns In-Sample")
plt.grid(True)
plt.show()


# Figure_2.1g_Q-Q_Plot_between_log_returns_In_Sample_and_estimated 


#             -------------------*/          \*-------------------
#
#    1. Central Pattern and Variation
#
#       The Reference Line (Red Dashed Line): 
#        
#       Represents where the points should fall if the distributions were identical.
#    
#       Data Points (Forecasts): 
#        
#       The vast majority of points are clustered horizontally along the y=0 line on the vertical axis.
#       
#       Implication: 
#        
#       This extreme pattern is the visual confirmation of the results obtained from the Kolmogorov-Smirnov Test and the R2.
#       The model predicts that the mean of all returns is zero (the flat blue line in the previous graph).
#       Since the forecast is almost always y_hat ≈ 0, the distribution of its predicted quantiles is a straight line around zero,
#       regardless of the observed quantiles.
#
#    2. Tail Dispersion
#    
#       Observation: 
#
#       For the extreme observed quantiles (tails) – both negative (near -1.0) and positive (near 1.0) on the X-axis –
#       the predicted quantiles remain fixed at zero on the Y-axis.
#    
#       Implication: 
#        
#       This proves that the model fails to capture or predict the dispersion and extreme values (volatility) of the returns.
#
#       In a market return forecasting context, this graph is the clearest visualization of the low R2 and the KS test rejection.
#       It reinforces that the model is only estimating the unconditional mean (zero) and is not generating point forecasts
#       that resemble the actual high-volatility distribution of returns.
#
#       This graph can be used to argue that for risk (Volatility), the model must rely on the 95% Credible Interval
#       and not on the point prediction line (y_hat).


####################################
#  Out of Sample                   #
#        predictive KPI peformace  #
#                                  #
####################################

# (Log Returns test sample) 

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:],PETR3_SA_Close_fractional_difference_Linear_filtred_test , label='Observed test sample log-returns', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], y_hat_in_sample[-forecasting_extrapolation_lengh:], label='Log-returns Out of Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], x_lower[-forecasting_extrapolation_lengh:],x_upper[-forecasting_extrapolation_lengh:], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Log-returns Out of Sample Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()

# Figure_2.1h_Out_of_Sample_Log_Returns

#=============*/        \*=================

alpha = samples["alpha"]
  
pd.DataFrame(alpha).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\log_returns_location.csv', index=None)
log_returns_location = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/log_returns_location.csv")

log_returns_location_mean = log_returns_location.mean(axis=0)
    
log_returns_scale = Returns_volatility_mean

model_0_residuals_log_returns = calculate_residuals(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten().astype(np.float64),
                   log_returns_location_mean, log_returns_scale[:-forecasting_extrapolation_lengh])
                   
log_returns_in_sample = y_hat_in_sample[:-forecasting_extrapolation_lengh]

#=============*/        \*=================				   
# Plot Residuals vs. Fitted Values 
plt.figure(figsize=(10, 6))
sns.scatterplot(x=log_returns_in_sample, y= model_0_residuals_log_returns, alpha=0.7)
plt.axhline(0, color='red', linestyle='--', linewidth=0.8)
plt.title('Residuals vs. Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

# Figure_2.1i_Residuals_vs_Fitted Values(Log-volume).


#             -------------------*/          \*-------------------
#
#    This graph is used to assess whether your regression model meets the assumptions of
#    linearity and homoscedasticity (constant error variance).
#
#    Point Pattern and Linearity
#       
#       The ideal pattern for a well-specified model is a random cloud of points centered
#       around the horizontal line y=0 (the dashed red line).
#       
#       Observation: 
#        
#       The residuals in this graph (Y-Axis) are centered around zero and do not
#       form a clear curvilinear pattern (like a curve or banana shape).
#    
#       Implication: 
#
#       The absence of a strong curvilinear pattern suggests that the mean
#       relationship between the variables is reasonably well captured by the model,
#       thus meeting the assumption of linearity.
#
#    Homoscedasticity (Error Variance)
#
#
#       Homoscedasticity requires that the vertical dispersion of the points be uniform across
#       the entire spectrum of fitted values (X-Axis).
#    
#       Observation:
#    
#          a) The X-Axis has a very narrow scale (from approximately -0.035 to 0.000), indicating
#             that the fitted Log-volume values are very small and show little variation.
#
#          b) The cloud of points, although appearing vaguely symmetric, exhibits a fairly uniform
#             vertical dispersion across the narrow range of the X-Axis. There is no clear "cone shape."
#
#       Implication: 
#
#       The variance of the residuals (point dispersion) appears to be relatively
#       constant. This suggests that the assumption of homoscedasticity is plausible for this
#       specific model, which is a positive result.
#
#    Outliers (Extreme Values)
#
#
#       Observation: 
#
#       There are a few points that exhibit large errors (high residuals), such as
#       the top point at y ≈ 4.2 (near x ≈ -0.012) and another bottom point at y ≈ -4.0 (near x ≈ -0.015).
#    
#       Implication: 
#        
#       These are outliers (extreme observations) where the model made significantly
#       large forecasting errors. While they don't disqualify the model, they should be
#       investigated as they may have a disproportionate influence on the regression coefficients.
#
#             -------------------*/          \*-------------------


# --- Breusch-Pagan Test ---
# Arguments: residuals, X (exog, independent variables)
# Returns: lagrange_multiplier, p_value_lm, fvalue, p_value_f
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model_0_residuals_log_returns, 
PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.astype(np.float64))


print(f"\n--- Breusch-Pagan Test ---")
print(f"Lagrange Multiplier p-value: {lm_pvalue:.4f}")
print(f"F-statistic p-value:         {f_pvalue:.4f}")
if lm_pvalue < 0.05:
    print("  -> Significant p-value suggests presence of heteroscedasticity (reject H0).")
else:
    print("  -> No significant evidence of heteroscedasticity (fail to reject H0).")

#  No significant evidence of heteroscedasticity (fail to reject H0).

#  Getting conditional mean inference (Model 1) HMC U-NUTs sampler

Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X = """



/*                                        Abstract

             This model offers a robust framework for  financial time series. 
         	 It integrates a Quasi (scalar pointwise) Bidirectional Momentum GRU with Gegenbauer FARMA processes 
             to capture non-linearities and long-range dependencies. A Markov Switching GJR-GARCH-X, 
          	 governed by an HMM, models dynamic volatility regimes and asymmetric shocks.
			 In addition, a state-space oscillator component, estimated via a Kalman filter, captures cyclical dynamics in returns.
			 This latent oscillator provides a probabilistic representation of periodic market behaviors, smoothing residuals 
			 while preserving stochastic fluctuations.The model also accounts for jump processes and incorporates fundamental analysis signals, 
             providing a comprehensive tool for forecasting and understanding intricate market behaviors.   

                                            Resumo

             Este modelo oferece uma estrutura robusta para séries temporais financeiras.
             Integra um Quasi (pontual escalar) Bidirectional Momentum GRU com processos Gegenbauer FARMA para capturar não-linearidades e 
			 dependências de longo alcance.Um modelo Markov Switching GJR-GARCH-X, governado por uma Cadeia de Markov Oculta (HMM), representa 
			 regimes dinâmicos de volatilidade e choques assimétricos.Adicionalmente, um componente oscilatório em espaço de estados, estimado por meio de Filtro de Kalman,
			 é incorporado para capturar dinâmicas cíclicas nos retornos.Esse oscilador latente fornece uma representação probabilística de comportamentos periódicos de mercado, 
			 atuando no alisamento dos resíduos ao mesmo tempo em que preserva as flutuações estocásticas.O modelo também contempla processos de saltos e integra sinais 
			 de análise fundamentalista, constituindo uma ferramenta abrangente para previsão e interpretação de comportamentos complexos de mercado.
	
   
      *  References
 
      AGARAP, Abien Fred. Deep Learning using Rectified Linear Units (). 2018. 
      Disponível em: https://arxiv.org/abs/1803.08375. Acesso em: 13 jun. 2025.
	  
	  ARNOLD, Jeffrey R. N. Kalman Filter Example in Stan. GitHub Gist, 2013. Disponível em: https://gist.github.com/jrnold/4700387
.     Acesso em: 31 ago. 2025.

      BOX, George E. P.; JENKINS, Gwilym M. Time Series Analysis: Forecasting and Control.
      San Francisco: Holden-Day, 1976.

      CARPENTER, Bob et al. Stan: A Probabilistic Programming Language. Journal of Statistical Software,
      v. 76, n. 1, p. 1-32, 2017.

      CHUNG, Junyoung et al. Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling. 2014.
      Disponível em: https://doi.org/20.48550/arXiv.1412.3555. Acesso em: 13 jun. 2025.

      DAMIANO, Luis; PETERSON, Brian; WEYLANDT, Michael. A Tutorial on Hidden Markov Models using Stan. 2017. 
      Disponível em: https://luisdamiano.github.io/stancon18/hmm_stan_tutorial.pdf. Acesso em: 13 jun. 2025.

      DEY, R.; SALEM, F. M. Gate-var_tiants of Gated Recurrent Unit (GRU) Neural Networks. 
      In: IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS). [S. l.]: IEEE, 2017.
      DOI: 20.1209/MWSCAS.2017.8053243.

      FERRARA, L.; GUEGAN, Dominique. Forecasting Financial Times Series with Generalized Long Memory Processes. 
      In: Advances in Quantitative Asset Management. [S. l.]: [s. n.], 2000. p. 319-342. DOI: 20.02007/978-1-4620-4389-3_14.

      GELMAN, Andrew et al. Bayesian Data Analysis. 3. ed. Chapman and Hall/CRC, 2013.

      GLOSTEN, Lawrence R.; JAGANNATHAN, Ravi; RUNKLE, David E. On the Relation Between the Expected Value and the Volatility of the Nominal Excess Return on Stocks.
      The Journal of Finance, v. 48, n. 5, p. 1779-1801, 1993. Disponível em: https://doi.org/20.1111/j.2040-6261.1993.tb05128.x. Acesso em: 13 jun. 2025.

      HIDAYANA, R. A.; SUKUNO; NAPITUPULU, N. FARMA-GJR-GARCH Model for Determining Value-at-Risk and Back testing of Some Stock Returns.
      In: Proceedings of the Second Asia Pacific International Conference on Industrial Engineering and Operations Management Surakarta. Indonésia: [s. n.], 2021.
	  
	  HAMILTON, James D. Time Series Analysis. Princeton: Princeton University Press, 1994.

      HOCHREITER, Sepp; SCHMIDHUBER, Jürgen. Long Short-term Memory. Neural Computation, v. 9, n. 8, p. 1735-80, 1997. 
      DOI: 20.1162/neco.1997.9.8.1735.
	  
	  KOKKALA, Juho. kalman-stan-randomwalk. GitHub repository, 2018. Disponível em: https://github.com/juhokokkala/kalman-stan-randomwalk
      . Acesso em: 31 ago. 2025.

      NGUYEN, Tam M. et al. MomentumRNN: Integrating Momentum into Recurrent Neural Networks.
      Advances in Neural Information Processing Systems (NeurIPS), 2020. Disponível em: https://doi.org/20.48550/arXiv.2006.06919. Acesso em: 13 jun. 2025.
	  
	  MONAHAN, J. F. A note on enforcing stationarity in autoregressive-moving average models. Biometrika, 
      v. 71, n. 2, p. 403–404, 1984.

      RAMACHANDRAN, Prajit; ZOPH, Barret; LE, Quoc V. Swish: a Self-Gated Activation Function. 2017.
      Disponível em: https://arxiv.org/abs/1720.05941. Acesso em: 13 jun. 2025.
	  
	  ROWEIS, Sam; GHAHRAMANI, Zoubin. A Unifying Review of Linear Gaussian Models. Neural Computation, 
	  v. 11, n. 2, p. 305-345, 1999. DOI: 10.1162/089976699300016674.

      SCHUSTER, Mike; PALIWAL, Kuldip. Bidirectional recurrent neural networks. 
      Signal Processing IEEE Transactions on, v. 45, p. 2673-2681, 1997. DOI: 20.1209/78.650093.
   
      SHUMWAY, R. H.; STOFFER, D. S. Time Series Analysis and Its Applications: With R Examples.   
      4. ed. New York: Springer, 2017.
   	  
	  TUSELL, Fernando. Kalman Filtering in R. Journal of Statistical Software, v. 39, n. 2, p. 1-27, 2011.
	  DOI: 10.18637/jss.v039.i02. Disponível em: https://www.researchgate.net/publication/51018597_Kalman_Filtering_in_R
     . Acesso em: 31 ago. 2025.

*/

functions {
  real swish(real x, real beta_swish) {
   return x*inv_logit(x*beta_swish);
  }
  /*
           * Purpose of Swish on mu_mean_volatility:

           * 1. Non-Linear Mean Transformation: Applies a non-linear activation to the sum of FARMA,
           Gegenbauer, and GRU components, transforming their inherently linear combination.
     
           * 2. Enhanced Expressivity: Increases the model's capacity to capture complex, non-linear
           relationships in stock returns, common in financial time series.
     
           * 3. "Hidden Layer" Analogy: Conceptually treats the aggregated components of the mean
           (before Swish) as a hidden layer's output, allowing for non-linear processing
           before passing to the Student-t likelihood.
  */
  real ReLU(real x) {
   return fmax(x,1e-12);
  }
  /*
           * Rectified Linear Unit (ReLU) activation function: max(0, x)
             Simplely, widely used activation that outputs the input directly if positive,
             otherwise outputs zero.(GJR-GARCHX part)
  */
}


data {
   // Number of sine/cosine frequencies (for periodic components in mean)
   int<lower=0> k1;
   // Order of AR (Autoregressive) component in FARMA mean equation
   int<lower=0> p;
   // Order of MA (Moving Average) component in FARMA mean equation
   int<lower=0> q;
   // Number of Gegenbauer coefficients for long memory in FARMA   
   int<lower=0> k; 
   // Number of observed data points (time series length)
   int<lower=1> N;
   // Number of time steps to forecast
   int<lower=2> T_forecast;
   // frequency of PSD periodogram (E.g 30-days cycle)
   vector<lower=0,upper=0.5> [k1] f1;
   int<lower=-1,upper=1> signals[N+T_forecast];  //Fundamental analysis signals one-hot encoded 
   int<lower=0,upper=1> date[N+T_forecast, 5];   // Days-of-the-week dummies
   vector[N] y; // Observed returns
   // Flag that tells the model whether to use exogenous variables bellow
   int<lower=0,upper=1> use_x1;
   int<lower=0,upper=1> use_x2;
   // Exogenous Regressor X_1: Extrapolated volatility, likely influencing GJR-GARCH.
   vector[N+T_forecast] x_1; 
   // Exogenous Regressor X_2: Extrapolated volume, also likely influencing GJR-GARCH.
   vector[N+T_forecast] x_2; 
   // Exogenous Regressor X_3: ema_fast
   vector[N] x_3;
   // Exogenous Regressor X_4: ema_slow
   vector[N] x_4;   
   // Exogenous Regressor X_5: support
   vector[N] x_5; 
   // Exogenous Regressor X_6: resistance
   vector[N] x_6; 
}

parameters{
   /*
               ---- GRU Hidden State Scaling Prior ----

       "gru_hidden_state_scalar" rescales the concatenated hidden states
       from the bidirectional quasi-GRUs before they enter the AR-like
       structure of the log-returns mean equation.

       Purpose:
   
         *    Acts as a sensitivity knob controlling how strongly GRU-based
              nonlinear dynamics influence the conditional mean of log-returns.
			
       Scaling:
   
          *     Constrained to [1, 2], meaning the GRU signal is always amplified
                at least ×1 (no attenuation), but never more than ×2.
			 
          *     This keeps the recurrent signal interpretable and prevents
                runaway amplification that could destabilize HMC.


       This scalar regularizes the GRU contribution so it acts as an
       auxiliary, data-driven adjustment to the log-returns conditional mean
       rather than a dominant driver.
   
   */
   //
   real<lower=1.0, upper=2.0> gru_hidden_state_scalar;
   //   
   /* 
                        ---- Swish Activation Coefficient (β) ----

       The Swish function is a smooth, non-monotonic activation function defined as:

                Swish(x; beta_swish) = x * sigmoid(beta_swish * x)

       It introduces a flexible non-linearity controlled by the coefficient β.

              * When beta_swish ~= 0 -> Swish(x) ~= x/2 -> Nearly linear (very weak non-linearity)
              * When beta_swish increases -> More curvature is introduced
              * When beta_swish -> inf -> Swish(x) approximates ReLU(x)

       In this model, "beta_swish" controls the degree of non-linearity in the GRU-based 
       conditional mean of log-volume. Setting "beta_swish" to a small value (e.g., 0.1 to 0.3) allows 
       only slight curvature, which helps retain interpretability while improving expressiveness.
  
   */
   real<lower=0.0, upper=1.0> beta_swish;  //Weak non-linearity 
   /*  
                        ---- MomentumRNN hyperparameters: ----
     momentum (alpha): Controls inertia of momentum vectors (v_g_o, v_n_d, etc.).
     A value of 0.90 is common in neural networks, smoothing updates.
     Higher values (closer to 1) mean more inertia.
   */ 
   real<lower=0.0,upper=1.0> momentum;
   /*
     epsilon (eta): Controls the influence of current volatility innovations (n[t-1])
     on the momentum vectors. Often analogous to a learning rate.
   */
   real<lower=0,upper=1> epsilon;
   /*   ---- continuous (marginalized) volatility jump ---- 
               Piironen e Vehtari (2017a, 2017b).           */
   real<lower=0, upper=1> pi_z;   // probability of a volatility jump
   vector[N] jumps;
   real<lower=0> sigma_spike;
   real<lower=0> sigma_slab;
   // 
   /* 
                 ---- Gegenbauer FARMA Parameters ----
     Fractional differencing parameter (d_1) for Gegenbauer processes, 
	 modeling long memory and periodicity.
   */
   real<lower= 0.0,upper=0.5> d_1; 
   /*
                   ---- L1-Regularization for Fourier Coefficients ----

      The sine (`alfa1`) and cosine (`beta1`) coefficients in the seasonal Fourier expansion
      are given Laplace (double_exponential) priors centered at zero, scaled by `0.25 / k1`.

          *  alfa1[j] ~ double_exponential(0, 0.25 / k1);
    
	      *  beta1[j] ~ double_exponential(0, 0.25 / k1);

      This implements **L1 regularization**, which promotes sparsity by heavily penalizing
      nonzero coefficients. As a result, most Fourier terms are shrunk toward zero unless
      strongly supported by the data.

      In effect, this setup acts as an automatic relevance detector : only seasonal
      harmonics that improve predictive performance will remain active.

      This approach helps prevent overfitting "wavy noise" or spurious seasonality,
      especially when using many harmonics (large `k1`), and encourages a parsimonious
      representation of periodic structure.

       Related conceptually to the Bayesian Lasso (Park & Casella, 2008).
	   
   */
   vector <lower=-0.5,upper=0.5> [k1]  alfa1;  // Sine parameters
   vector <lower=-0.5,upper=0.5> [k1]  beta1;  // Cossine parameters
   /*
   
                 ---- Hidden Markov Model (HMM) Transition Probabilities ----
     Probability of remaining in the current state (e.g., state 1 to state 1, or state 2 to state 2).
     Implicitly, 1 - p_remain[state] is the probability of transitioning to the other state.
	 
   */
   real<lower=0.01, upper=0.99> p_remain[2];
   /*
   Initial GARCH variance
   */
   vector <lower=0> [2] sigma0;
   /*  
                 ---- FARMA Model Parameters (AR and MA) ----
     Autoregressive (AR) coefficients for the FARMA mean equation.
     These phi_raw are the reflection coefficients, which must be in (-1, 1)
     They are then used to derive the actual phi_mean AR and theta_mean MA coefficients
   */
   vector<lower=-1, upper=1>[p] phi_raw; 
   vector<lower=-1, upper=1>[p] theta_raw;
   /*
             ---- GJR-GARCH Model Parameters  ----
      Omega (constant term): Base var_tiance for each GARCH state.
   */ 
   vector <lower=0> [2] omega; 
   /*
     Alpha (ARCH term): Coefficients for squared past residuals (symmetric response).
   */  
   vector <lower=0> [2] alpha;
   /*
     Psi (Leverage term): Coefficients for the asymmetric effect of negative shocks (Glosten, Jagannathan, Runkle).
     Multiplied by (negative residual)^2 to capture asymmetry.
   
   */
   vector <lower=0> [2] psi;
   /*
     Beta (GARCH term): Coefficients for persistence of past conditional var_tiances.
     'q' here refers to the order of MA in the mean, which seems to be reused for GARCH here. 
   */ 
   vector <lower=0,upper=1> [2] beta;
   /* 
     phix_1 (Exogenous regressor X1 - GARCHX): Coefficients for the influence of 'x_1' on variance.
     'p1' refers to its order.
   */  
   vector <lower=-1, upper=1> [2] phix_1;
   /* 
     phix_2 (Exogenous regressor X2 - GARCHX): Coefficients for the influence of 'x_2' on variance.
     'p2' refers to its order. 
   */    
   vector <lower=-1, upper=1> [2] phix_2;
   /*  
     LSTM influence coefficient per volatility regime: controls how much the LSTM hidden state h_t
     modulates the conditional variance in regime u.
   */	 
   vector <lower= -1, upper=1> [2] phi_lstm; 
   /*
      ---- Mean of the conditional mean process (`nu`) ----
      This parameter represents the baseline level of log-returns.
   */
   real<lower=-1, upper=1>  mu; 
   /*   
     ---- Means and standard deviations for KPI fundamental and technical analysis effect priors  (Hierarchical Aproach). ----
	                                   (For log-returns conditional mean)
   */  
   real mean_kpi;
   //
   real<lower=0> sigma_kpi;
   // Fundamental Analysis Signal (One-Hot encoding)
   real kpi; 
   //    
   /*   
     ---- Means and standard deviations for daily effect priors  (Hierarchical Aproach). ----
	                            (For log-returns volatility)
   */	 
   vector[2] mean_monday;
   vector[2] mean_tuesday;
   vector[2] mean_wednesday;
   vector[2] mean_thursday;
   vector[2] mean_friday; 
   vector<lower=0>[2] sigma_monday;
   vector<lower=0>[2] sigma_tuesday;
   vector<lower=0>[2] sigma_wednesday;
   vector<lower=0>[2] sigma_thursday;
   vector<lower=0>[2] sigma_friday;
   // Dates themselves  
   vector[2] monday;
   vector[2] tuesday;
   vector[2] wednesday;
   vector[2] thursday;
   vector[2] friday; 
   //   
   real mean_x_3;   // Hierarchical mean for EMA fast effect
   real mean_x_4;   // Hierarchical mean for EMA slow effect
   real mean_x_5;   // Hierarchical mean for Support zone effect
   real mean_x_6;   // Hierarchical mean for Resistance zone effect
   //
   real<lower=0> sigma_x_3; // Hierarchical scale for EMA fast effect
   real<lower=0> sigma_x_4; // Hierarchical scale for EMA slow effect
   real<lower=0> sigma_x_5; // Hierarchical scale for Support zone effect
   real<lower=0> sigma_x_6; // Hierarchical scale for Resistance zone effect
   //
   real price_action_3; // realized coefficient for EMA fast
   real price_action_4; // realized coefficient for EMA slow
   real price_action_5; // realized coefficient for Support
   real price_action_6; // realized coefficient for Resistance
   /*  
               ---- Scaled LSTM & GRU Gate Parameters (MomentumRNN-like) ----
      These are scalar versions of the weights for the var_tious gates and cell states,
      multiplied by epsilon and used in the momentum update step. 
   */
   /*
   ===========================================================   
    Forward MomentumGRU scalar parameters (Conditional mean)
   ===========================================================
   */
   real <lower=-1,upper=1>  thnv_f_gru_scalar_forward;   // Forget/Update gate (input dependent)
   real <lower=-1,upper=1>  thnw_f_gru_scalar_forward;   // Forget/Update gate (hidden state dependent)
   real <lower=-1,upper=1>  thnb_f_gru_scalar_forward;   // Bias for forget/update gate
   real <lower=-1,upper=1>  thnw_h_gru_scalar_forward;   // Candidate hidden state (hidden state dependent)
   real <lower=-1,upper=1>  thnb_h_gru_scalar_forward;   // Bias for candidate hidden state
   real <lower=-1,upper=1>  theta_h_gru_scalar_forward;  // Candidate hidden state (input dependent)
   /*
   ===========================================================
    Backward MomentumGRU scalar parameters (Conditional mean)
   ===========================================================
   */
   real <lower=-1,upper=1>  thnv_f_gru_scalar_backward;  // The same here too!!!  
   real <lower=-1,upper=1>  thnw_f_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnb_f_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnw_h_gru_scalar_backward;   
   real <lower=-1,upper=1>  thnb_h_gru_scalar_backward;   
   real <lower=-1,upper=1>  theta_h_gru_scalar_backward;
   /*        
      ---- GRU initial States Priors ----
   */
   // Initial hidden state for GRUs
   real h_0_forward;
   real h_0_backward;
   /*  
      ---- Scaled LSTM  Gate Parameters (MomentumRNN-like) ----
      These are scalar versions of the weights for the various gates and cell states,
      multiplied by epsilon and used in the momentum update step. 
   */
   // ===========================================================
   //                LSTM parameters
   // ===========================================================
   real <lower=-1,upper=1>  thnv_d;    // Candidate cell state (input dependent)
   real <lower=-1,upper=1>  thnw_d;    // Candidate cell state (hidden state dependent)
   real <lower=-1,upper=1>  thnv_i;    // Input gate (input dependent)
   real <lower=-1,upper=1>  thnw_i;    // Input gate (hidden state dependent)
   real <lower=-1,upper=1>  thetv_o;   // Output gate (input dependent)
   real <lower=-1,upper=1>  thnw_o;    // Output gate (hidden state dependent)
   real <lower=-1,upper=1>  thnv_f;    // Forget gate (input dependent)
   real <lower=-1,upper=1>  thnw_f;    // Forget gate (hidden state dependent)
   real <lower=-1,upper=1>  thnb_d;    // Bias for candidate cell state
   real <lower=-1,upper=1>  thnb_i;    // Bias for input gate
   real <lower=-1,upper=1>  thnb_o;    // Bias for output gate
   real <lower=-1,upper=1>  thnb_f;    // Bias for forget gate
   real <lower=-1,upper=1>  W_out;     // Weights for the hidden LSTM hidden states (Non-Linear filtred residuals)
   real <lower=-1,upper=1>  B_out;     // Linear eq bias
   /*        
      ---- LSTM initial States Priors ----
   */
   // Initial hidden and cell state for LSTM
   real h_0;
   real C_0;	
   /* ---- Additive latent oscillator (marginalized) ---- */
   real<lower=0, upper=1> osc_r;       // radial damping (0..1)
   real<lower=0> osc_sigma_eta;        // process noise for oscillator state
   real<lower=0> osc_sigma_eps;        // measurement noise (oscillator part)
   real<lower=0> osc_sigma_nu;         // RW step size for omega
   real osc_omega1;                    // initial angle (radians) 
   vector[N] osc_omega;                // latent oscillator frequencies
   vector<lower=0>[N] w;               // Student-t scale mixture weights
}

transformed parameters{
    /* 
       	---- This section declares all transformed parameters ----
      Gegenbauer FARMA Coefficient (Transformed AR and MA Coefficients)
    */ 
    vector[p] phi_mean;   // The actual AR coefficients
    vector[q] theta_mean; // The actual MA coefficients
    /*
              ---- Temporal Mean and Residuals ----
        mu_mean_volatility: Conditional mean of the volatility 
        at each time point, derived from GRU's hidden states on 
        Gegenbauer-FARMA.
    */
    vector  [N] mu_mean;
    //  Residuals (observed y minus conditional mean mu).
    vector  [N] residuals;
	real fourier_terms_1;  // Accumulator for period terms 
    real fourier_terms_2;
    real fourier_terms_3; 	
    vector[N] periodic_component_sum; // vector storing fourier terms
    /*  
           ----  Gegenbauer Polynomial Coefficients ----
       g_1: Coefficients Matrix of the Gegenbauer polynomial,
       which capture long-memory dynamics.
      
       u_1: Related to the frequency parameter f1, used in Gegenbauer calculation.
    */  
    matrix[k, k+1] g_1;    // Gegenbauer coefficients for each frequency
    vector[k] u_1;         // cos(2πf1)
    /*
	                 ---- Hidden Markov Model (HMM) Transition Matrix ----
						  
       'A' represents the transition probabilities between the two hidden states 
	   (e.g., low and high volatility regimes).A[i, j] is the probability of 
	   transitioning from state 'i' to state 'j'.p_remain[s] is the probability of staying in state 's'.
	   
	*/
	matrix[2, 2] A;
    /*	'probability' stores the normalized posterior probabilities of
       	being in each state at each time point.
	*/
	vector[2] probability[N];
	/*
	                       ---- HMM Log-Likelihood for Each State ----
       log_alpha[t, s] stores the log of the joint probability of the observations up to time 't'
       and being in state 's' at time 't'. Used in the forward algorithm for HMM inference. 
    */    
    vector [2] log_alpha[N];
	/*
           
		   ---- Sorted Regime-Specific Volatility Levels ----

       The original GJR_GARCHX volatility `sigma` vector contains regime-dependent intercepts
       for the log-volatility innovations (sigma[t]). However, without constraints,
       their labels (e.g., "low" vs. "high" volatility) are exchangeable — leading
       to label switching across MCMC chains.

       We sort all GJR-GARCH terms  in ascending order to enforce an identifiability constraint,
       ensuring that, for exemple: 
	   
           sigma_sorted[1] < sigma_sorted[2] , and etc...
    
	   This helps prevent multimodal posteriors and improves mixing in the HMM component.

       All downstream computations (HMM, mu_regime, forecasts) use the sorted version.
	
    */
	vector <lower=0.0> [2] sigma0_sorted;
	vector <lower=0.0> [2] omega_sorted;
	vector <lower=0.0> [2] alpha_sorted;
	vector <lower=0.0> [2] psi_sorted;
	vector <lower=0.0,upper=1> [2] beta_sorted;
	vector <lower=-1, upper=1>  [2] phix_1_sorted;
	vector <lower=-1, upper=1>  [2] phix_2_sorted;
	vector <lower=-1, upper=1>  [2] phi_lstm_sorted;
	//
	vector [2] 	monday_centered;
	vector [2] 	tuesday_centered;
	vector [2] 	wednesday_centered;
	vector [2] 	thursday_centered;
	vector [2] 	friday_centered;
	/*  
	
	   "week_effect_avg" represents the average of the five weekly effects (Monday–Friday) 
	   in regime u. By subtracting m from each individual effect, we impose a zero-sum constraint
       between the weekly effects. This centering prevents the weekday dummies
       from altering the unconditional level of variance, ensuring stationarity and
       better identifiability between intercept and seasonal effects.
	*/
	real week_effect_avg;
    vector [N] residuals_sq;
    vector [N] var_t;	
	/*
	                       ---- Model-Derived Parameters ----
       Conditional Standard Deviation for each of the 2 HMM states, where sigma[t, state] represents 
	   the conditional standard deviation of the residuals at time 't', given the system is in a 
	   particular hidden state. This comes from the GJR-GARCHX component.
	*/
	// Conditional std dev
	matrix <lower=0> [N,2] sigma;
    //	
	/*   
         ---- Bidirectional GRU momentum vectors ----  
    */
    //  Corresponds to the update/reset gate ***(Forward)    
    vector [N] v_g_f_gru_forward;
    // Corresponds to the update/reset gate  ***(Forward)
    vector [N] v_h_gru_forward;
    //  Corresponds to the update/reset gate ***(Backward)  
    vector [N] v_g_f_gru_backward;
    // Corresponds to the update/reset gate  ***(Backward)  
    vector [N] v_h_gru_backward; 
    /*        
                              ---- GRU States ----
        g_f_gru: Forget/Update gate for the GRU.
        h_gru: Hidden states of the GRU, which will be used in the mean component.
    */
    vector [N] g_f_gru_forward;
    vector [N] h_gru_forward; 
    vector [N] h_gru_forward_updated; 
    vector [N] g_f_gru_backward;
    vector [N] h_gru_backward;
    vector [N] h_gru_backward_updated;
    /*   
                  ---- Concatenated Hidden State from Bidirectional GRU ----
      
	  Combines the forward and backward hidden states, often by averaging or concatenation.
      This combined state summarizes information from both past and future contexts for each time point.
  
    */
    vector [N] h_gru_concatenate;  
    // 
	/*  
       ---- LSTM vectors declaration ---- 
    */
    // LSTM hidden state
    vector [N] h;
    // LSTM cell state
    vector [N] C;
    // LSTM candidate cell state
    vector <lower=-1.0, upper=1.0> [N] n_d;
    // LSTM input gate
    vector <lower= 0.0, upper=1.0> [N] g_i;
    // LSTM output gate
    vector <lower= 0.0, upper=1.0> [N] g_o;
    // LSTM forget gate
    vector <lower= 0.0, upper=1.0> [N] g_f;
    // LSTM momentum for output gate
    vector [N] v_g_o;
    // LSTM momentum for candidate cell state
    vector [N] v_n_d;
    // LSTM momentum for input gate
    vector [N] v_g_i;
    // LSTM momentum for forget gate
    vector [N] v_g_f;
    // LSTM activation layer 
	vector [N] eta;
	// Kalman filter state vectors (for log-lik marginalization)
    vector[2] H;
    vector[2] m;          // filtered mean
    matrix[2,2] Pmat;     // filtered covariance
	real c;               // cosine of current frequency
    real s;               // sine of current frequency
    matrix[2,2] Rmat;     // 2×2 rotation matrix
	/*
  	============================================================
     Temporary scalars and vectors for Kalman update (used inside loop)
    ============================================================
    */
	//
    /*  
	           ---- Predicted measurement (scalar). ----
       This is the observation implied by the current state estimate.
	*/
    real yhat;
    /*         
	          ---- Regime-mixed conditional variance (scalar). ----
       Weighted average of regime-specific variances omega², using smoothed probs.
	*/
    real regime_var;
    /*    
	          ---- Observation variance under Student-t scale mixture (scalar). ----
                      Equals regime_var divided by latent precision w[t].
	*/ 				  
    real obs_var;
    /*       
	         ---- Total predictive variance (scalar). ----
          Combines uncertainty from state estimate + observation noise.
    */
    real S;
    /*     
            ---- Kalman gain (2-dim vector). ----
         Tells how much to adjust latent state (cos/sin) 
         when seeing the innovation residuals.
    */
    vector[2] Kgain;
    /*            
	             ---- Accumulator for HMM Forward Algorithm ---- 
        Used to sum log-probabilities for the HMM forward algorithm, 
		helping to avoid numerical underflow.
		
    */
	real accumulator[2];
	// --- HMM Transition Matrix Initialization ---
    // A[1,1] = p_remain[1] means P(State 1 -> State 1)
    // A[1,2] = 1 - p_remain[1] means P(State 1 -> State 2)
    // A[2,1] = 1 - p_remain[2] means P(State 2 -> State 1)
    // A[2,2] = p_remain[2] means P(State 2 -> State 2)
	//
	A[1, 1] = inv_logit(p_remain[1]);
	A[1, 2] = 1 - inv_logit(p_remain[1]);
	A[2, 1] = 1 - inv_logit(p_remain[2]);
	A[2, 2] = inv_logit(p_remain[2]);
	/*
    ===========================================================
                    Transformation Coefficients
    ===========================================================
	*/
	for (u in 1:2) {
      week_effect_avg = (monday[u] + tuesday[u] + wednesday[u] +
                         thursday[u] + friday[u]) / 5;
      monday_centered[u]    = monday[u]    - week_effect_avg;
      tuesday_centered[u]   = tuesday[u]   - week_effect_avg;
      wednesday_centered[u] = wednesday[u] - week_effect_avg;
      thursday_centered[u]  = thursday[u]  - week_effect_avg;
      friday_centered[u]    = friday[u]    - week_effect_avg;
    } 
	/*	
                 ----- Gegenbauer Coefficients Calculation -----
    Calculates the coefficients for the Gegenbauer polynomial based on d_1 and f_1.
    These coefficients determine the long-memory properties of the FARMA process.
    */
    for (i in 1:k) {
      u_1[i]=  cos(2.0* pi() * f1[i]);
      g_1[i, 1] = 1.0;
      g_1[i, 2] = 2.0 * u_1[i] * d_1;
      for (j in 3:(k+1)) {
        g_1[i, j] = (2.0 * u_1[i] * ((j - 1) + d_1 - 1.0) * g_1[i, j - 1]
        - ((j - 1) + 2.0 * d_1 - 2.0) * g_1[i, j - 2]) / (j - 1);
      }
    }
	//
    /*  
         
 		                ---- Transformation of AR and MA Parameters ----
                Calculate phi_mean from phi_raw (reflection coefficients)
                This is a common algorithm (e.g., Durbin-Levinson)
                for converting partial autocorrelations (phi_raw) to AR coefficients (phi_mean)
				
				Shumway, R. H., & Stoffer, D. S. (2017)
	 
    */
    if (p > 0){
      matrix[p, p] P; // Temporary matrix for algorithm
      // Initialize the first reflection coefficient
      P[1, 1] = phi_raw[1];
      phi_mean[1] = P[1, 1];
      // Compute for higher orders
      for (i in 2:p) {
        P[i, i] = phi_raw[i];
        for (j in 1:(i-1)) {
          P[i, j] = P[i-1, j] - P[i, i] * P[i-1, i-j];
        }
        for (j in 1:i) {
          phi_mean[j] = P[i, j];
        }
      }
    }
    /* 
     Calculate theta_mean from theta_raw (reflection coefficients for MA)
     The process is analogous to AR coefficients for invertibility
    */  
    if (q > 0) {
      matrix[q, q] Q; // Temporary matrix for algorithm
      Q[1, 1] = theta_raw[1];
      theta_mean[1] = Q[1, 1];
      for (i in 2:q) {
        Q[i, i] = theta_raw[i];
        for (j in 1:(i-1)) {
          Q[i, j] = Q[i-1, j] - Q[i, i] * Q[i-1, i-j];
        }
      for (j in 1:i) {
        theta_mean[j] = Q[i, j];
        }
      }
    }
	/*
        ---- Conditional mean. Initialize Bidirectional GRU States and Momentum for t=1 ----
                Sets initial hidden and cell states to zero.
           Sets initial forward and backward momentum  to zero.
    */ 
    // Forward terms   
    v_g_f_gru_forward[1] = 0.0;
    v_h_gru_forward[1] =   0.0;
    v_g_f_gru_forward[1] =  (momentum * v_g_f_gru_forward[1] + epsilon*thnw_f_gru_scalar_forward*h_0_forward);
    g_f_gru_forward[1]   =  inv_logit(v_g_f_gru_forward[1] + thnv_f_gru_scalar_forward*y[1] +  thnb_f_gru_scalar_forward);
    v_h_gru_forward[1]   =  (momentum *v_h_gru_forward[1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward[1] + h_0_forward));
    h_gru_forward[1]     =   tanh(v_h_gru_forward[1] + theta_h_gru_scalar_forward*y[1]  + thnb_h_gru_scalar_forward);
    h_gru_forward_updated[1] = (1- g_f_gru_forward[1])*h_0_forward +  g_f_gru_forward[1] *h_gru_forward[1];
	// Backward terms 
    v_g_f_gru_backward[1] = 0.0;
    v_h_gru_backward[1] =   0.0;
    v_g_f_gru_backward[1] =  (momentum * v_g_f_gru_backward[1] + epsilon*thnw_f_gru_scalar_backward*h_0_backward);
    g_f_gru_backward[1]   =  inv_logit(v_g_f_gru_backward[1] + thnv_f_gru_scalar_backward*y[N] +  thnb_f_gru_scalar_backward);
    v_h_gru_backward[1]   =  (momentum *v_h_gru_backward[1] + epsilon*thnw_h_gru_scalar_backward*(g_f_gru_backward[1] + h_0_backward));
    h_gru_backward[1]     =  tanh(v_h_gru_backward[1] + theta_h_gru_scalar_backward*y[N]  + thnb_h_gru_scalar_backward);	
    h_gru_backward_updated[1] = (1- g_f_gru_backward[1])*h_0_backward +  g_f_gru_backward[1] *h_gru_backward[1];  
	/*
        GRU update rule: (1 - update_gate) * previous_hidden_state + update_gate * candidate_hidden_state
        Here, "g_f_gru_forward" and "g_f_gru_backward" are acting as the update gate.
    */
    //
    /*
                   --- Concatenate Bidirectional GRU Hidden States (for t = 1) ---
                  Combines the forward hidden state  with the backward hidden state at time 't'
     	  
    */ 
    h_gru_concatenate[1] =  gru_hidden_state_scalar*((h_gru_forward_updated[1] + h_gru_backward_updated[1])/2); 
	/*
		 Conditional mean (Gegenbauer-FARMAX) Process for t=1
	*/
    fourier_terms_1 = 0.0; // Initialize accumulator
    if (k1 > 0) {
       for (j in 1:min(1,k1)) { // If periodic components are specified
		  fourier_terms_1 +=  alfa1[j] * sin((2*pi()*f1[j]) *(1)) + beta1[j] * cos((2*pi()*f1[j]) * (1));
         }
    }
    periodic_component_sum[1] = fourier_terms_1;               // Assign after summing
	mu_mean[1] = mu;                                           // Initialize with global mean
	mu_mean[1] += periodic_component_sum[1];	               // Fourier terms
	mu_mean[1] += jumps[1];                                    // jumps on the mean
	mu_mean[1] += phi_mean[1]*(h_gru_concatenate[1]);          // AR 1 at time t=1
	mu_mean[1] = swish(mu_mean[1] , beta_swish);
    // Calculate unstandardized residual 
    residuals[1] = y[1] - mu_mean[1];
    /* 
           ---- Main Time Series Loop (t=2 to N) ----
    */
    for(t in 2:N){
      /*
	                ---- Forward and Backward MomentumGRU Update within the loop ----
          Computes momentum, gate, and hidden state for GRU at each time step,
          using the previous hidden state (h_gru_forward[t-1]) and previous returns obs (y[t-1]).
	
	  */
	  v_g_f_gru_forward[t] = (momentum * v_g_f_gru_forward[t-1] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward[t-1]);
	  g_f_gru_forward[t] = inv_logit(v_g_f_gru_forward[t] + thnv_f_gru_scalar_forward*y[t-1] + thnb_f_gru_scalar_forward);
	  v_h_gru_forward[t] =  (momentum *v_h_gru_forward[t-1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward[t] + h_gru_forward[t-1]));
	  h_gru_forward[t] =  tanh(v_h_gru_forward[t] + theta_h_gru_scalar_forward*y[t-1] + thnb_h_gru_scalar_forward);
	  h_gru_forward_updated[t] = (1- g_f_gru_forward[t])*h_gru_forward[t-1] +  g_f_gru_forward[t] *h_gru_forward[t]; 
	  /*            
                	---- backward MomentumGRU Update within the loop ----
          Computes momentum, gate, and hidden state back GRU at each time step,
          using the previous hidden state (h_gru_backward[t-1]) and previous inverted returns obs (y[N-t]).
    
	  */
	  v_g_f_gru_backward[t] = (momentum * v_g_f_gru_backward[t-1] + epsilon*thnw_f_gru_scalar_backward*h_gru_backward[t-1]);
      g_f_gru_backward[t] = inv_logit(v_g_f_gru_backward[t] + thnv_f_gru_scalar_backward*y[max(N-t+1,1)] + thnb_f_gru_scalar_backward);
      v_h_gru_backward[t] =  (momentum *v_h_gru_backward[t-1] + epsilon*thnw_h_gru_scalar_backward*(g_f_gru_backward[t] + h_gru_backward[t-1]));
      h_gru_backward[t] =  tanh(v_h_gru_backward[t] + theta_h_gru_scalar_backward*y[max(N-t+1,1)] + thnb_h_gru_scalar_backward);
      h_gru_backward_updated[t] = (1- g_f_gru_backward[t])*h_gru_backward[t-1] +  g_f_gru_backward[t] *h_gru_backward[t];  
	}	
    /*
                   --- Concatenate Bidirectional GRU Hidden States (for t > 1) ---
                  Combines the forward hidden state  with the backward hidden state at time 't'
     	  
    */
    for(t in 2:N){  
          h_gru_concatenate[t] =  gru_hidden_state_scalar*((h_gru_forward_updated[t] + h_gru_backward_updated[t])/2); 
	}  
    /*
		 Conditional mean (Gegenbauer-FARMAX) Process for t=2
	*/
    mu_mean[2]  = mu; 
	fourier_terms_2 = 0.0; 
    if (k1 > 0) {
       for (j in 1:min(1,k1)) { // If periodic components are specified
		  fourier_terms_1 +=  alfa1[j] * sin((2*pi()*f1[j]) *(2)) + beta1[j] * cos((2*pi()*f1[j]) * (2));
         }
    }
	periodic_component_sum[2] = fourier_terms_2;  
    mu_mean[2] += periodic_component_sum[2]; 
    if (k > 0)  for (i in 1:min(1,k)) { // If Gegenbauer (long memory) component is specified
      for (mm in 1:min(1, k)) {
        mu_mean[2]   += g_1[i, mm + 1] * y[2 - mm]; // Gegenbauer terms on observed terms.
        }
    }
    if(p > 0) for (i in 1:min(1,p)){ // If AR component is specified
	   mu_mean[2]   += phi_mean[i]*(h_gru_concatenate[2-i]); // AR terms on past observed terms.
	}
    if(q > 0) for(j in 1:min(1,q)){ // If MA component is specified
        mu_mean[2]   += theta_mean[j] * residuals[2-j]; // MA terms on past residuals.
    }
    mu_mean[2]   += kpi*signals[1]; // (Buy=+1, hold=0, Sell=-1 influence on the mean)
	mu_mean[2]   += jumps[2]; // jumps on the mean
	mu_mean[2]   += price_action_3 * x_3[1]; // Fast EMA
    mu_mean[2]   += price_action_4 * x_4[1]; // Slow EMA
    mu_mean[2]   += price_action_5 * x_5[1]; // Suport
    mu_mean[2]   += price_action_6 * x_6[1]; // Resistence 
    mu_mean[2]   = swish(mu_mean[2]  , beta_swish);
    // Calculate unstandardized residual 
    residuals[2] = y[2] - mu_mean[2];
    /*
           
		         ---- Sorted Regime-Specific Log-Volatility Levels ----

       The original `sigma_0` and others vectors contain regime-dependent intercepts
       for the log-volatility innovations (sigma_0). However, without constraints,
       their labels (e.g., "low" vs. "high" volatility) are exchangeable — leading
       to label switching across MCMC chains.

       We sort 'sigma_0_sorted' and others in ascending order to enforce an identifiability constraint,
       ensuring that:
	   
                 sigma_0_sorted[1] < sigma_0_sorted[2]
    
	   This helps prevent multimodal posteriors and improves mixing in the HMM component.

       All downstream computations (HMM, mu_regime, forecasts) use the sorted version.
	
    */
    sigma0_sorted = sort_asc(sigma0);	
	omega_sorted = sort_asc(omega);
    alpha_sorted = sort_asc(alpha);
	psi_sorted = sort_asc(psi);
	beta_sorted = sort_asc(beta);
	phix_1_sorted = sort_asc(phix_1);
    phix_2_sorted = sort_asc(phix_2);
	phi_lstm_sorted = sort_asc(phi_lstm);
	//
	/*
             ---- GJR-GARCHX (1,1)  calculations at t=1 ---- 	
    */
	for (j in 1:2) {	
      residuals_sq[1] = pow(sigma0_sorted[j], 2);
	  //
	  v_g_o[1] = 0.0;
      v_n_d[1] = 0.0;
      v_g_i[1] = 0.0;
      v_g_f[1] = 0.0;
      v_g_o[1] = (momentum * v_g_o[1] + epsilon*thetv_o*h_0);
      v_n_d[1] = (momentum * v_n_d[1] + epsilon*thnv_d*h_0);
      v_g_i[1] = (momentum * v_g_i[1] + epsilon*thnv_i*h_0);
      v_g_f[1] = (momentum * v_g_f[1] + epsilon*thnv_f*h_0);
      g_o[1]  = inv_logit(v_g_o[1] + thnw_o*residuals_sq[1] + thnb_o);
      n_d[1]  = tanh(v_n_d[1] + thnw_d*residuals_sq[1] + thnb_d);
      g_i[1]  = inv_logit(v_g_i[1] + thnw_i*residuals_sq[1] + thnb_i);
      g_f[1]  = inv_logit(v_g_f[1] + thnw_f*residuals_sq[1] + thnb_f);
      C[1] =    g_i[1] *n_d[1]  + g_f[1] *C_0;
      h[1] =    g_o[1]* tanh(C[1]);
      eta[1] =  W_out*h[1] + B_out;	  
	  // Baseline GJR-GARCH structure
      var_t[1] =  omega_sorted[j] 
	            + alpha_sorted[j] *residuals_sq[1]
				+ beta_sorted[j] * pow(sigma0_sorted[j],2);
	  // Long-Memory LSTM filter 
	  var_t[1] += phi_lstm_sorted[j]*eta[1];
	  // Days-of-the-week dummies
	  var_t[1]  += monday_centered[j]* date[1,1] + tuesday_centered[j]*date[1,2] + wednesday_centered[j]*date[1,3]
	  + thursday_centered[j]*date[1,4] + friday_centered[j]*date[1,5];
	  //
	  var_t[1]   = ReLU(var_t[1]);
      sigma[1,j] = sqrt(var_t[1]);
      /*        
	                       ---- HMM Forward Algorithm Initialization for t=1 ----
           Calculates the initial log-probability of being in each state, given the first observations.
           Assumes equal initial state probabilities (0.5).
	  */
      log_alpha[1, j] = log(0.5) + student_t_lpdf(residuals[1] |3.82426, 0.0 , sigma[1,j]);
    }
	// Convert log-probs to regime probabilities using softmax
	probability[1] = softmax(to_vector(log_alpha[1]));
	/*
                    ---- GJR-GARCHX (1,1)  calculations at t=2  ---- 	
    */
	residuals_sq[2] = pow(residuals[1], 2);
	//
	v_g_o[2] = (momentum * v_g_o[1] + epsilon*thetv_o*h[1]);
    v_n_d[2] = (momentum * v_n_d[1] + epsilon*thnv_d*h[1]);
    v_g_i[2] = (momentum * v_g_i[1] + epsilon*thnv_i*h[1]);
    v_g_f[2] = (momentum * v_g_f[1] + epsilon*thnv_f*h[1]);
    g_o[2]  = inv_logit(v_g_o[1] + thnw_o*residuals_sq[1] + thnb_o);
    n_d[2]  = tanh(v_n_d[1] + thnw_d*residuals_sq[1] + thnb_d);
    g_i[2]  = inv_logit(v_g_i[1] + thnw_i*residuals_sq[1] + thnb_i);
    g_f[2]  = inv_logit(v_g_f[1] + thnw_f*residuals_sq[1] + thnb_f);
    C[2] =    g_i[2] *n_d[2]  + g_f[2] *C[1];
    h[2] =    g_o[2]* tanh(C[2]);
    eta[2] =  W_out*h[2] + B_out;	
	//       
    for (j in 1:2) { // Current state
       var_t[2] = omega_sorted[j]
            + alpha_sorted[j] * residuals_sq[2]
            + beta_sorted[j] * pow(sigma[1,j],2);
	   if (use_x1 == 1){
	        var_t[2] += phix_1_sorted[j] * x_1[1];
	   }
	   if (use_x2 == 1){
            var_t[2] += phix_2_sorted[j] * x_2[1];
	   }
	   // Leverage effect (negative residuals amplify variance)
       if (residuals[1] < 0){
            var_t[2] += psi_sorted[j] * residuals_sq[2];
	   }
       // Long-Memory LSTM filter 
	   var_t[2] += phi_lstm_sorted[j]*eta[2];
	   // Days-of-the-week dummies
	   var_t[2]  += monday_centered[j]* date[2,1] + tuesday_centered[j]*date[2,2] + wednesday_centered[j]*date[2,3]
	   + thursday_centered[j]*date[2,4] + friday_centered[j]*date[2,5];
	   //
	   var_t[2] = ReLU(var_t[2]);
    sigma[2,j] = sqrt(var_t[2]);
   }  
   /*     ---- HMM Forward Algorithm for t=2 ----       */
   for(j in 1:2) { // Current state
	   for(i in 1:2) { // Previous state
	   accumulator[i] = log_alpha[1, i] + // Log-probability from previous observation and state i
	                       log(A[i, j]) + // Log-transition probability from i to j
	                       student_t_lpdf(residuals[2] |3.82426, 0.0, sigma[2,j]);  // Log-likelihood of residual given previous state i's volatility.
         }	 
	     log_alpha[2, j] = log_sum_exp(accumulator); // Sums log-probabilities for all paths leading to state j
	}
	probability[2] = softmax(to_vector(log_alpha[2])); // Normalizes to get posterior probabilities for current time 't'.  
	/*	  
	      Conditional mean (Gegenbauer-FARMAX) Process for t=[3,N]
	*/
    for(t in 3:N){
      fourier_terms_3 = 0.0;
      if (k1 > 0) {
        for (j in 1:min(t-1,k1)){
         fourier_terms_2 += alfa1[j] * sin((2*pi()*f1[j]) *(t)) + beta1[j] * cos((2*pi()*f1[j]) * (t));
         }
      }
      periodic_component_sum[t] = fourier_terms_3; 
      mu_mean[t]   = mu;  
      mu_mean[t] += periodic_component_sum[t];  
      if (k > 0) for (i in 1:min(t-1,k)){
         for (mm in 1:min(t-1,k)) {
           mu_mean[t] += g_1[i, mm + 1] * y[t - mm];
         }
      }
      if(p > 0) for (i in 1:min(t-1,p)){
	     mu_mean[t] += phi_mean[i]*(h_gru_concatenate[t-i]);
	  }
      if(q > 0) for(j in 1:min(t-1,q)){ 
         mu_mean[t] += theta_mean[j] * residuals[t-j];
      }
      mu_mean[t] += kpi*signals[t-1];
      mu_mean[t] += jumps[t]; 
      mu_mean[t] += price_action_3 * x_3[t-1]; // Fast EMA
      mu_mean[t] += price_action_4 * x_4[t-1]; // Slow EMA
      mu_mean[t] += price_action_5 * x_5[t-1]; // Suport
      mu_mean[t] += price_action_6 * x_6[t-1]; // Resistence
	  mu_mean[t] = swish(mu_mean[t]   , beta_swish);
	  residuals[t] = y[t] - mu_mean[t];
	  /*         
                    ---- GJR-GARCHX calculations ----
                        (Regime shift for t=[3,N])					
      */
	  residuals_sq[t] = pow(residuals[t-1], 2);
      //
	  v_g_o[t] = (momentum * v_g_o[t-1] + epsilon*thetv_o*h[t-1]);
      v_n_d[t] = (momentum * v_n_d[t-1] + epsilon*thnv_d*h[t-1]);
      v_g_i[t] = (momentum * v_g_i[t-1] + epsilon*thnv_i*h[t-1]);
      v_g_f[t] = (momentum * v_g_f[t-1] + epsilon*thnv_f*h[t-1]);
      g_o[t]  = inv_logit(v_g_o[t-1] + thnw_o*residuals_sq[t-1] + thnb_o);
      n_d[t]  = tanh(v_n_d[t-1] + thnw_d*residuals_sq[t-1] + thnb_d);
      g_i[t]  = inv_logit(v_g_i[t-1] + thnw_i*residuals_sq[t-1] + thnb_i);
      g_f[t]  = inv_logit(v_g_f[t-1] + thnw_f*residuals_sq[t-1] + thnb_f);
      C[t] =    g_i[t] *n_d[t]  + g_f[t] *C[t-1];
      h[t] = g_o[t]* tanh(C[t]);
      eta[t] = W_out*h[t] + B_out;		  
      //  
	  for (j in 1:2) { // Current state
	   	 // Baseline GJR-GARCH structure
         var_t[t] = omega_sorted[j]
            + alpha_sorted[j]  * residuals_sq[t]
            + beta_sorted[j]   * pow(sigma[t - 1,j],2);
		 // Exogenous regressors (GARCHX terms)
	     if (use_x1 == 1){
	        var_t[t] += phix_1_sorted[j] * x_1[t-1];
	     }
	     if (use_x2 == 1){
            var_t[t] += phix_2_sorted[j] * x_2[t-1];
	     }
		 // Leverage effect (negative residuals amplify variance)
         if (residuals[t-1] < 0){
            var_t[t] += psi_sorted[j] * residuals_sq[t];
	     }
		 // Long-Memory LSTM filter 
		 var_t[t]  += phi_lstm_sorted[j]*eta[t];
		 // Days-of-the-week dummies
		 var_t[t]  += monday_centered[j]* date[t,1] + tuesday_centered[j]*date[t,2] + wednesday_centered[j]*date[t,3]
	     + thursday_centered[j]*date[t,4] + friday_centered[j]*date[t,5];
		 //
		 var_t[t] = ReLU(var_t[t]);
         sigma[t,j] = sqrt(var_t[t]);       // std deviation	
       }
       //
       /*		               ---- HMM Forward Algorithm Update ----
              Calculates log_alpha[t,j] (log-probability of path ending in state 'j' at time 't')
              by summing over all possible previous states 'i'.
              log_alpha[t, j] = log_sum_exp(log_alpha[t-1, i] + log(A[i,j]) + normal_lpdf(residuals[t] | 0, Sigma[t,j]))
	   */
	   for(j in 1:2) { // Current state
	     for(i in 1:2) { // Previous state
	          accumulator[i] = log_alpha[t-1, i] + // Log-probability from previous observation and state i
	                       log(A[i, j]) + // Log-transition probability from i to j
	                       student_t_lpdf(residuals[t] | 3.82426, 0.0 , sigma[t,j]);  // Log-likelihood of residual given previous state i's volatility.
           }						   
	       log_alpha[t, j] = log_sum_exp(accumulator); // Sums log-probabilities for all paths leading to state j
	     }
	     probability[t] = softmax(to_vector(log_alpha[t])); // Normalizes to get posterior probabilities for current time 't'.
	}
	/*
     ====================================================================
               KALMAN FILTER WITH LATENT OSCILLATOR
     ====================================================================

     Purpose:
     --------
   
       This block tracks a hidden **cyclical component** in returns
       that is not explained by AR/MA, GRU, GARCH, or jumps.

     Representation:
     ---------------
   
          * The oscillator state is 2D: [cos(θ_t), sin(θ_t)]'.
            Think of it as a point on the unit circle.

          * At each time t, the state rotates by angle omega[t] (frequency).
            That means the oscillator evolves as:
         
		      "x_{t+1} = R(omega[t]) * x_t";
           
		   where R(omega[t]) is a 2×2 rotation matrix.

         * Only the first coordinate (cos component) contributes
           to observed returns. This is extracted with H = [1, 0].

     Kalman filter role:
     -------------------
   
        * m      = filtered mean estimate of oscillator state
        * Pmat   = covariance (uncertainty) of the estimate
        * Kgain  = Kalman gain for updating state estimates

     Workflow:
     ---------
   
        1. Start with prior state (m, Pmat).
        2. Predict observation mean yhat = dot_product(H, m).
        3. Mix regime variances -> total obs variance S.
        4. Update state (m, Pmat) given residuals[t] and S.
        5. Propagate oscillator forward by rotating with omega[t],
           applying damping (osc_r), and adding process noise
           (osc_sigma_eta).

     Interpretation:
     ---------------
   
        * In-sample: the filter extracts "hidden cycles" in residuals.
        * Out-of-sample: we propagate the oscillator state stochastically,
          so forecasts inherit cyclical dynamics.

     Intuition:
     ----------
   
        * The AR/MA/GRU/GARCH parts explain most of the structure.
        * The Kalman oscillator "soaks up" leftover oscillations,
          giving a smoother, cyclical signal.
        * Forecasts therefore include both: (a) learned cycles,
          and (b) learned cycles +  random shocks.
   */
   //
   /* 
   ======================================================================
       ---- Initialization of Kalman filter for latent oscillator ----
   ======================================================================
   */ 
   /* 
      H is the observation matrix (dimension 2->1).  
      Here H = [1, 0], meaning we only observe the cosine component 
      (first state dimension) of the latent oscillator.
   */
   H[1] = 1.0;        // observation matrix: pick out cosine component
   H[2] = 0.0;        // ignore sine component
   /* 
	  Initialize latent state mean "m" = [0, 0].  
      This is the prior guess for the oscillator's hidden state 
      (cosine & sine coordinates at time t=1).
   */
   m = rep_vector(0.0, 2);     // diffuse prior mean [cos,sin]
   /* 
      Initialize state covariance "Pmat" as diagonal with variance=5.  
      This encodes uncertainty about the initial oscillator state.
   */
   Pmat = diag_matrix(rep_vector(5.0, 2));    // large prior covariance
   /*
   ======================================================================
       ---- One full Kalman pass over observed sample t=[1,N] ----
 
    (executed in TRANSFORMED PARAMETERS)
    NOTE: no `target += ...` increments here — this is *just* computing  
	filtered state trajectories (m, Pmat) to reuse later.
    ======================================================================
   */
   for (tt in 1:N) {
     /*    
        	 ---- 1. One-step-ahead prediction ----
         Predicted measurement from oscillator state:
		      oscillator contribution (prior mean)
     */
     yhat = dot_product(H, m);      // (since H=[1,0], yhat ≈ current cosine component of state)
     /* 
      ---- 2. Observation variance (mixing HMM regimes + t mixture) ----
            Weighted, mixed,  variance across volatility regimes:
			    (weighted avg. of omega² across HMM states)
	 */
     regime_var = probability[tt,1] * square(sigma[tt,1])
                  + probability[tt,2] * square(sigma[tt,2]);   // Mixed regime variance (weighted avg. of omega² across HMM states)
     /* 
     	 Student-t scale mixture: divide variance by latent precision w[tt]
              (Conditional obs variance under Student-t mixture)	
     */
     obs_var = regime_var / w[tt];
     /*    Total predictive variance = state uncertainty + observation noise  */
     S = dot_product(H, Pmat * H) + obs_var;
     /*      
             	 ---- 3. Kalman update ----   
       Kalman gain: tells how much to adjust state estimate with new data
	        (how much to adjust oscillator state given y_t)
	   
	 */
     Kgain = (Pmat * H) / S;
     /*  Update latent mean given residual, innovations,  (obs - predicted)  */
     m = m + Kgain * (residuals[tt] - yhat);
     /*  Update covariance: shrink uncertainty in direction of observation  */
     Pmat = (diag_matrix(rep_vector(1.0,2)) - Kgain * H') * Pmat;
     /*    
     	 ---- 4. State propagation (latent oscillator dynamics) ----
     
	      Rotate the state vector forward by angle osc_omega[tt],
          then apply radial damping osc_r, then add process noise.
     */
     if (tt < N) {
       c = cos(osc_omega[tt]);
       s = sin(osc_omega[tt]);
       Rmat[1,1] = c;   Rmat[1,2] = -s;
       Rmat[2,1] = s;   Rmat[2,2] =  c;
       // Propagate mean: rotate & shrink by damping
       m = osc_r * (Rmat * m);
       // Propagate covariance: rotate, damp, then add process noise
       Pmat = osc_r * (Rmat * Pmat * (osc_r * Rmat'))
         + square(osc_sigma_eta) * diag_matrix(rep_vector(1.0,2));
       }
    } // end Kalman loop
}


model {
   /*   
             ---- Scaling factor for bidirectional GRU hidden states. ----
   
           Constrained [1,2] to prevent runaway amplification;
           prior shrinks toward 1.0 so GRU acts as a mild adjustment
           unless data supports stronger influence.
   
   */
   gru_hidden_state_scalar  ~ normal(1,0.25) T[1,2]; 
   /*  ---- Jump Components on the mean 
       ( Soft Spike-and-slab “continuous”)   ---- */
   //
   /* ---- Informed priors according to Extreme Value Analysis (EVA) ----
      sigma_spike ≈ 0.20, sigma_slab ≈ 0.18, p_jump ≈ 0.054
   */
   sigma_spike ~ normal(0.20, 0.2) T[0,];  
   sigma_slab  ~ normal(0.18, 0.5) T[0,];   
   //
   /*    
       Jump probability "pi_z" Beta with mean  ~ 0.054
   */
   pi_z ~ beta(5, 88);
   /*  ---- zero-mean normal marginalized spike-and-slab prior 
            with a var_tiance that depends on pi_z       ---- */
   for (t in 1:N){
      target += log_mix(pi_z,
                      normal_lpdf(jumps[t] | 0, sigma_slab),
                      normal_lpdf(jumps[t] | 0, sigma_spike));
   }
   /* 
                        ---- momentum should evolve smoothly ----
						
      Encourages the model to learn slow-moving, stable momentum patterns in volatility, 
      which is plausible for financial time series where latent dynamics often evolve gradually.

      Centered around 0.9, but openned to be anywhere between 0.8 and 1.0. 
      Helping to get persistent temporal dependencies.
	  
   */
   momentum ~ normal(0.9, 0.05) T[0,1];  
   epsilon ~ beta(2, 2);     // "uniform-ish" over [0,1]
   //  Beta Coefficient for swish 
   beta_swish ~ normal(0.00, 0.10) T[0,];
   /*  
                ---- Priors for Fundamental Analysis KPI effects: ----
       Hierarchical priors allow the  kpi effects  to share strength,
	   improving estimation.
   */
   /*  
       mean for KPI signals 
   */
   mean_kpi ~ normal(0.0,0.20);
   /*   
       Standard deviations for kpi effect priors, truncated at 0.
   */
   sigma_kpi ~ student_t(3, 0, 0.20) T[0,];
   /*  Realized effects shrink toward hierarchical means */
   kpi ~ normal(mean_kpi,sigma_kpi);
   /*   ---- Priors for Price Action KPI effects: ---- */ 
   mean_x_3  ~ normal(0.0,0.20);
   mean_x_4  ~ normal(0.0,0.20);
   mean_x_5  ~ normal(0.0,0.20);
   mean_x_6  ~ normal(0.0,0.20);
   /*   
       * Standard deviations 
   */
   sigma_x_3 ~ student_t(3, 0, 0.20) T[0,];
   sigma_x_4 ~ student_t(3, 0, 0.20) T[0,];
   sigma_x_5 ~ student_t(3, 0, 0.20) T[0,];
   sigma_x_6 ~ student_t(3, 0, 0.20) T[0,];
   /*  Realized effects for Price Action KPIs. */
   price_action_3 ~ normal(mean_x_3,sigma_x_3); 
   price_action_4 ~ normal(mean_x_4,sigma_x_4); 
   price_action_5 ~ normal(mean_x_5,sigma_x_5); 
   price_action_6 ~ normal(mean_x_6,sigma_x_6); 
   //   
   /*
       * Prior for the mean (mu)      
   */
   mu ~  normal(0, 0.05) T[-1,+1]; 
   //
   /* --- Fourier shrinkage, helping avoid overfitting wavy noise. --- */
   if (k1 > 0){
     for (j in 1:k1) { 
       alfa1[j] ~ double_exponential(0.00, 0.25 / k1); // Prior for sine parameters
       beta1[j] ~ double_exponential(0.00, 0.25 / k1); // Prior for cosine parameters
     }
   }
   /*  ---- HMM Transition Probabilities ----     */
   p_remain ~ beta(20, 1);  // High persistence of remaining in a state
   /*  
            ---- Priors for  MomentumLSTM Scalar Parameters ----
      These are given standard normal priors, typical for neural network weights/biases. 
   */
   // ===========================================================
   // LSTM Scalar Priors for Stochastic Volatility 
   // =========================================================== 
   C_0 ~ normal(0.0,2.00);            // Initial cell state
   h_0 ~ normal(0.0,2.00);            // Initial hidden state   
   thetv_o ~ normal(0.0,2.0);         // Output gate momentum vector
   thnw_o  ~ normal(0.0,2.0);         // Output gate weight
   thnb_o  ~ normal(0.0,2.0);         // Output gate bias
   thnv_d  ~ normal(0.0,2.0);         // Cell state candidate momentum vector
   thnw_d  ~ normal(0.0,2.0);         // Cell candidate weight
   thnb_d  ~ normal(0.0,2.0);         // Cell candidate bias
   thnv_i  ~ normal(0.0,2.0);         // Input gate momentum
   thnw_i  ~ normal(0.0,2.0);         // Input gate weight
   thnb_i  ~ normal(0.0,2.0);         // Input gate bias
   thnv_f  ~ normal(0.0,2.0);         // Forget gate momentum
   thnw_f  ~ normal(0.0,2.0); 
   /*  
       Forget Gate Bias weight 
          encourages memory retention (sigmoid(1) ≈ 0.73)
   */
   thnb_f  ~ normal(1.0,0.5);
   // ===========================================================
   // Linear Output Layer for LSTM hidden states
   // =========================================================== 
   W_out ~ normal(0.0, 0.1);              //  Hidden State Candidate Weights 
   /*
         The bias acts as a shift in the Softplus activation function. Unlike weights (W_out), 
		 which are multiplied by features (hidden states) and require strong regularization, 
		 the bias is a fixed term.Thus, it is crucial to give the bias a larger scale , 
		 such as 0.20, to make it less  constrained. This allows the model to have the 
		 necessary flexibility to find the ideal equilibrium point for nonlinear correction, 
		 without being overly restricted by a rigid a priori belief.
   */
   B_out ~ normal(0.0, 0.2);  
   /*  
                  ---- Priors for MomentumGRU  Scalar Parameters ----
     These are given standard normal priors, typical for neural network weights/biases. 
   */
   // ===========================================================
   // Bidirectional Momentum-GRU Priors (Forward)
   // ===========================================================  
   h_0_forward ~ normal(0.0,2.00);                 // Initial hidden state prior 
   theta_h_gru_scalar_forward ~ normal(0.0,2.0);   // GRU hidden state transformation
   thnv_f_gru_scalar_forward ~  normal(0.0,2.0);   // Momentum vector (v) forward
   thnw_f_gru_scalar_forward ~  normal(0.0,2.0);   // Weighting for forward GRU update
   /*  
       ---- Bias term for forward GRU update ----
     encourages memory retention (sigmoid(1) ≈ 0.73)
   */
   thnb_f_gru_scalar_forward ~  normal(1.0,0.5);   // Bias term for forward GRU update  
   thnw_h_gru_scalar_forward ~  normal(0.0,2.0);   // Weighting for hidden (h) update
   thnb_h_gru_scalar_forward ~  normal(0.0,2.0);   // Bias term for hidden (h) update
   // ===========================================================
   // Bidirectional Momentum-GRU Priors (Backward)
   // ===========================================================  
   h_0_backward ~ normal(0.0,2.00);  
   theta_h_gru_scalar_backward ~ normal(0.0,2.0);  // Same here for backward terms!
   thnv_f_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnw_f_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnb_f_gru_scalar_backward  ~ normal(1.0,0.5);  
   thnw_h_gru_scalar_backward  ~ normal(0.0,2.0); 
   thnb_h_gru_scalar_backward  ~ normal(0.0,2.0);
   /*   
                 ---- FARMA AR and MA Parameters ----
       Priors for the raw parameters (reflection coefficients)
       Often a symmetric prior  for each phi_raw[i] and theta_raw[i]
   */
   if (p > 0) {
    for (i in 1:p) {
      phi_raw[i] ~ normal(0,2)T[-1,1];  
     }
   }
   if (q > 0) {
     for (i in 1:q) {
       theta_raw[i] ~ normal(0,2)T[-1,1]; 
     }
   }
   for (u in 1:2) {
      // 
      /*                   ---- Priors for daily effects: ----
          Hierarchical priors allow the daily effects  to share strength,
	      improving estimation particularly if some weekdays have less data or high variability.
      */   
      mean_monday[u] ~  normal(0.0, u == 1 ? 0.10 : 0.25);
      mean_tuesday[u] ~ normal(0.0, u == 1 ? 0.10 : 0.25);
      mean_wednesday[u] ~ normal(0.0, u == 1 ? 0.10 : 0.25);
      mean_thursday[u] ~ normal(0.0, u == 1 ? 0.10 : 0.25);
      mean_friday[u] ~ normal(0.0, u == 1 ? 0.10 : 0.25);
      /* 
                 ---- Standard deviations for daily effect priors. ---- 
        It lets the model occasionally explore larger values if the data demand it,
        allowing robust shrinkage but with some tolerance for big effects 		
      */
      sigma_monday[u] ~ student_t(3, 0, u == 1 ? 0.10 : 0.30) T[0,]; 
      sigma_tuesday[u] ~ student_t(3, 0, u == 1 ? 0.10 : 0.30) T[0,];
      sigma_wednesday[u] ~ student_t(3, 0, u == 1 ? 0.10 : 0.30) T[0,];
      sigma_thursday[u] ~ student_t(3, 0, u == 1 ? 0.10 : 0.30) T[0,];
      sigma_friday[u] ~ student_t(3, 0, u == 1 ? 0.10 : 0.30) T[0,];
      /*  
       * Individual daily  effects drawn from their respective hierarchical priors.
      */
      monday[u] ~ normal(mean_monday[u] ,sigma_monday[u]);
      tuesday[u] ~ normal(mean_tuesday[u] ,sigma_tuesday[u]);
      wednesday[u] ~ normal(mean_wednesday[u] ,sigma_wednesday[u]);
      thursday[u] ~ normal(mean_thursday[u] ,sigma_thursday[u]);   
      friday[u] ~ normal(mean_friday[u],sigma_friday[u]);
      /*     
                                   ---- GJR-GARCH Parameters ---- 
								   
        Omega (baseline var_tiance level):
        This prior reflects our belief about the unconditional volatility in each regime.
   
          - Low-volatility regime (u == 1): Prior ~ Normal(0.005, 0.002), 
            representing calm market periods with small base volatility.
		 
          - High-volatility regime (u == 2): Prior ~ Normal(0.03, 0.005),
            allowing much higher var_tiance during crises or turbulent conditions.
		 
		 
        The tighter prior for regime 1 and broader support for regime 2 improve regime separability.
        Truncated at 0 to ensure positive var_tiance.
   
             *  omega[1] ~ normal(0.005, 0.002) (low-vol)
             *  omega[2] ~ normal(0.03, 0.005) (high-vol)
		 
       */
       omega[u] ~ normal(u == 1 ? 0.005 : 0.03, u == 1 ? 0.002 : 0.005) T[0,]; 
       /*  
	 
       Alpha (ARCH effect):
       Controls how much recent squared shocks (residuals^2) influence current volatility.
	 
         - Low-vol regime: Prior ~ Normal(0.03, 0.01), implies low reactivity.
	   
         - High-vol regime: Prior ~ Normal(0.10, 0.03), allowing strong responses to market shocks.
	   
       These priors reflect the intuition that turbulent markets react more sharply to recent movements.
       Truncated at 0 for positivity.
	   */
       alpha[u] ~ normal(u == 1 ? 0.03 : 0.10, u == 1 ? 0.01 : 0.03) T[0,];
       /* 
	
       Psi (GJR-GARCH leverage effect):
       Captures the asymmetric impact of negative returns on volatility (leverage effect).
	  
        - Low-vol regime: Prior ~ Normal(0.01, 0.005), implying weak asymmetry.
		
        - High-vol regime: Prior ~ Normal(0.12, 0.03), enabling strong leverage effects.
		
       This reflects the "bad news has stronger impact" behavior common in crisis regimes.
       Truncated at 0 to ensure contribution only when residuals are negative.	  
       */
       psi[u] ~ normal(u == 1 ? 0.01 : 0.12, u == 1 ? 0.005 : 0.03) T[0,];
       /*
  
       Regime-dependent prior for phi_lstm:
  
          - We center both phi_low and phi_high at 0.0, reflecting the default belief that the LSTM should
            not systematically inflate/reduce volatility without evidence.
  
          - For regime 1 (presumed low-vol), we use a tight prior SD=0.01 
  
          - For other regimes (e.g. high-vol), SD=0.03 allows slightly larger positive 
     	    phi but still small.
  		 
       */
       phi_lstm[u] ~ normal(u == 1 ? 0.005 : 0.01, u == 1 ? 0.01 : 0.03);
       /* 
       
	   Beta (GARCH persistence):
       Controls how much past volatility persists into the present.
	  
          - Low-vol regime: Beta(25, 2) → mean ≈ 0.93, very persistent.
		
          - High-vol regime: Beta(5, 5) → mean ≈ 0.50, more reactive and less persistent.
		
       This separation helps the model distinguish stable vs. shock-sensitive states.	  
       */
       beta[u] ~ beta(u == 1 ? 25 : 5, u == 1 ? 2 : 5);  // Mean ≈ 0.93 (low), 0.5 (high)
       /*   
	   
         	   ---- GARCH-X exogenous coefficients (phix_1, phix_2) ---- 
     	These capture the effect of external var_tiables (e.g., extrapolated volatility and volume)
        on the var_tiance equation. Centered at 0 with moderate scale, allowing for both positive 
		and negative influence.
		
		   - Low-vol regime: omega = 0.2, small sensitivity to external factors.
           - High-vol regime: omega = 0.5, allows stronger influence of external signals.
	  
        */
        phix_1[u] ~ normal(0, u == 1 ? 0.1 : 0.3);
        phix_2[u] ~ normal(0, u == 1 ? 0.1 : 0.3);
        /*
	  
                ---- Sigma0 (initial var_tiance per regime) ----
         
		     Sets the starting volatility for each hidden state.
		 
           - Low-vol regime: Prior ~ Normal(0.015, 0.005).
		 
           - High-vol regime: Prior ~ Normal(0.05, 0.01).
		 
         These priors prevent the model from starting with extreme or implausible volatility levels.
         Truncated at 0 to maintain positivity.
	     
		 */
         sigma0[u] ~ normal(u == 1 ? 0.015 : 0.05, u == 1 ? 0.005 : 0.01) T[0,]; 	 
  }
  /*
       ---- latent-gamma prior for Student-t scale-mixture  ---- 
  */
  for (u in 1:N) {
     w[u] ~ gamma(3.82426/2, 3.82426/2); // latent precision weight for Student-t ( Accordingly with Figure1.1e)
  }
  /*  
       ---- oscillator priors ----  
     RW prior for oscillator frequencies
  */
  osc_omega[1] ~ normal(osc_omega1, osc_sigma_nu);
  for (t1 in 2:N){
    osc_omega[t1] ~ normal(osc_omega[t1-1], osc_sigma_nu);
  }
  /*  
       ---- priors for oscillator params ---- 
  */
  osc_r ~ beta(2, 2);
  osc_sigma_eta ~ normal(0, 1) T[0,];
  osc_sigma_eps ~ normal(0, 1) T[0,];
  osc_sigma_nu ~ normal(0, 0.5) T[0,];
  osc_omega1 ~ normal(0, 1);
  /*  
       ---- likelihood contributions from the filter ---- 
  */
  for (tt in 1:N) {
    target += normal_lpdf(residuals[tt] | dot_product(H, m), sqrt(S));
  }
}

generated quantities{
   /* ---- Jump component 
                   for "mu_mean_new" ----  */
   vector[N + T_forecast] jumps_new;  
   /* 
      ---- Spike-and-slab mix indicator for future jump
           1 = slab (huge jump), 0 = spike (zero jump) ----  */
   int is_slab;
   /* 
      Simulating returns (Posterior predictive checks)
   */
   vector [N+T_forecast] y_sim;
   /*
      Predicted periodic terms for volume conditional mean
   */
   real fourier_terms_4; 
   real fourier_terms_5;
   vector [N+T_forecast] periodic_component_sum_new;
   /*
     Forward probabilities for forecast horizon
   */
   vector[2] prob_forecast[N+T_forecast];
   /*
                ---- Log-likelihood for WAIC/LOO ---- 
     These will be saved for model comparison and diagnostics.
	 
   */
   vector[N] log_lik;           // Kalman-marginalized log-likelihood contributions (main one for WAIC/LOO)
   vector[N] log_lik_mixture;   // Original Student-t mixture log-likelihood (diagnostic only, not used in WAIC/LOO)
   real total_log_lik;          // Sum of log_lik across all observations (optional convenience)
   real total_log_lik_mixture;  // Sum of log_lik_mixture (for diagnostic comparison)
   /*
                ---- Kalman filter helper variables ----
     These are used inside the in-sample pass of the Kalman filter
     to compute predictive means and variances at each time step.
	 
   */
   vector[2] H_gq;              // Observation matrix (projects 2D oscillator state down to scalar)
   vector[2] m_gq;              // Filtered mean of oscillator state (2D latent state)
   matrix[2,2] Pmat_gq;         // Filtered covariance of oscillator state (uncertainty in latent state)
   real regime_var_gq;          // Regime-mixed variance from MS-GARCH step at time t
   real obs_var_gq;             // Observation variance after dividing regime_var by latent precision w[t]
   real yhat_gq;                // Predicted mean of residuals at time t from Kalman filter
   real SS_gq;                  // Total predictive variance (oscillator + regime + mixture noise)
   real mixture_sigma_gq;       // Scale parameter for Student-t mixture (diagnostic likelihood only)
   /*
                 ---- Kalman update step variables ----
   */
   vector[2] Kgain_gq;          // Kalman gain at time t (2x1 vector, determines update weight)
   vector[2] xdraw_gq;          // Random draw from the latent oscillator state posterior at time t
   real osc_part_gq;            // Random contribution of oscillator to observed series at time t
   //     
   /*
               ---- predictive mean state vector for oscillator at forecast start. ----
			   
			   This carries forward the filtered latent oscillator state (2D phase space)
               from the in-sample Kalman filter. Dimension = 2 because oscillator
               is represented in cosine/sine coordinates.

   */
   vector[2] m_f;     
   /*
               ---- predictive covariance matrix of the oscillator state at forecast start. ----
			   
               This is the uncertainty carried forward about (cos,sin) state evolution,
               propagated during forecasting with process noise.
   */
   matrix[2,2] P_f;        
   /*
               ---- predictive oscillator frequency at forecast start. ----
			   
               Initialized from last estimated in-sample frequency osc_omega[N],
               then evolved forward by a random walk with step size osc_sigma_nu.
   */
   real omega_f; 
   //
   /* 
         
		  ---- Build rotation matrix for oscillator dynamics ----
   
       The oscillator latent state is represented as a 2D vector [x1, x2].
       At each step, the state evolves by a rotation in the plane
       with angle `omega_f` (the instantaneous frequency).

       Mathematically:
       
	        [x1(t+1)]   [ cos(omega_f)  -sin(omega_f) ] [x1(t)]
            [x2(t+1)] = [ sin(omega_f)   cos(omega_f) ] [x2(t)]

       This is just a 2×2 rotation matrix that rotates the state forward.
     
   */
   real c_gq;                // cosine of current frequency
   real s_gq;                // sine of current frequency
   matrix[2,2] Rmat_gq;      // 2×2 rotation matrix
   /*   
         ---- Draw forecast oscillator latent state ----
   
      We propagate uncertainty from the predictive mean state (m_f)
      and covariance (P_f) by sampling a latent 2D state vector.
   
          - "x_f[1]" ~ cosine component of oscillator
          - "x_f[2]" ~ sine component of oscillator

      This ensures the forecast accounts for both:
   
      * The deterministic rotation dynamics
      * The stochastic process noise in the oscillator.
	  
   */
   vector[2] x_f;
   /*   
   
        ---- Extract observable oscillator contribution ----
   
      The observable contribution of the oscillator to y comes from
      projecting the latent state x_f onto H = [1, 0].
   
        - This means we only take the first coordinate (cosine cycle).
        - Equivalent to: osc_part_forecast = x_f[1]

     If you want to include measurement noise in forecasts,
     you can replace it with:
      
	    osc_part_forecast = normal_rng(dot_product(H, x_f), osc_sigma_eps);
     
		which adds observation-level jitter on top of the latent cycle.
   
   */
   real osc_part_forecast;
   /*
            ---- Predicted  (Simulated)   conditional standard deviations for each state ---- 
   */
   vector <lower=0> [N+T_forecast] sigma_new;
   /*
     ---- GJR-GARCHX Model Parameters  ----
   */ 
   real <lower=0>  omega_regime_new;
   real <lower=0>  alpha_regime_new;
   real <lower=0>  psi_regime_new;
   real <lower=0,upper=1>  beta_regime_new;
   real <lower=-1, upper=1>  phix_1_regime_new;
   real <lower=-1, upper=1>  phix_2_regime_new;
   real <lower=-1, upper=1>  phi_lstm_regime_new;
   /*
      ---- Predicted  (Simulated)   mean ---- 
   */
   vector  [N+T_forecast] mu_mean_new;
   /*
      ---- Predicted  (Simulated)  residuals ----
   */	  
   vector [N+T_forecast] residuals_new;
   vector [N+T_forecast] residuals_sq_new;
   vector [N+T_forecast] var_t_new;
   /*  
      ---- Predicted momentum vectors for GRU ----
   */
   vector [N+T_forecast] v_g_f_gru_forward_new;
   vector [N+T_forecast] v_h_gru_forward_new;  
   vector [N+T_forecast] g_f_gru_forward_new;
   vector [N+T_forecast] h_gru_forward_new;
   vector [N+T_forecast] h_gru_forward_new_updated;
   vector [N+T_forecast] h_gru_concatenate_new;
   /* 
      Forecasted LSTM cell,  hidden states  and activation layer
   */
   vector [N+T_forecast] h_new;
   vector [N+T_forecast] C_new;
   vector [N+T_forecast] eta_new;  
   /* 
      Predicted LSTM gate outputs and states
   */   	  
   vector <lower=-1.0, upper=1.0>  [N+T_forecast] n_d_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_i_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_o_new;
   vector <lower= 0.0, upper=1.0>  [N+T_forecast] g_f_new;
   /*
      Predicted momentum vectors for LSTM 
   */
   vector [N+T_forecast] v_g_o_new;
   vector [N+T_forecast] v_n_d_new;
   vector [N+T_forecast] v_g_i_new;
   vector [N+T_forecast] v_g_f_new;
   /*       
             	---- Copy Observed Data to New Vectors for Forecasting ----
         Initializes the forecast vectors with the observed data to continue the sequence.
   */	
   v_g_f_gru_forward_new[1:N] = v_g_f_gru_forward;
   v_h_gru_forward_new[1:N] = v_h_gru_forward;
   g_f_gru_forward_new[1:N] = g_f_gru_forward;
   h_gru_forward_new[1:N] =  h_gru_forward;
   h_gru_forward_new_updated[1:N] =  h_gru_forward_updated;
   h_gru_concatenate_new[1:N] = h_gru_concatenate;   
   residuals_new[1:N] = residuals;
   periodic_component_sum_new[1:N] = periodic_component_sum;
   mu_mean_new[1:N] = mu_mean;
   jumps_new[1:N] = jumps;
   prob_forecast[1:N] = probability; 
   residuals_sq_new[1:N] = residuals_sq;
   v_g_o_new[1:N] = v_g_o;
   v_n_d_new[1:N] = v_n_d;
   v_g_i_new[1:N] = v_g_i;
   v_g_f_new[1:N] = v_g_f;
   g_o_new[1:N] = g_o;
   n_d_new[1:N] = n_d;
   g_i_new[1:N] = g_i;
   g_f_new[1:N] = g_f;
   C_new[1:N] = C;
   h_new[1:N] = h;
   eta_new[1:N] = eta;
   var_t_new[1:N] = var_t;
   //
   /*   
   ==================================
     Kalman Filter  Initialization
   ==================================
   */   
   H_gq[1] = 1;
   H_gq[2] = 0;
   m_gq = rep_vector(0.0, 2);                 // Start with diffuse zero mean
   Pmat_gq = diag_matrix(rep_vector(5.0, 2)); // Large initial uncertainty
   total_log_lik = 0;
   total_log_lik_mixture = 0;
   /*    
    =======================
     In-sample Kalman pass
    =======================  
   */
   for (t in 1:N) {
      /*  ---- regime-mixed variance from MS-GARCH ---- */
      regime_var_gq = probability[t,1] * square(sigma[t,1])
                    + probability[t,2] * square(sigma[t,2]);
      /*
         ---- adjust by Student-t latent weight ----
	  */
      obs_var_gq = regime_var_gq / w[t];
	  /*
         ---- predictive mean from current state ----
	  */
      yhat_gq = dot_product(H_gq, m_gq);
	  /*
         ---- predictive variance (state + obs noise) ----
	  */
      SS_gq = dot_product(H_gq, Pmat_gq * H_gq) + obs_var_gq;
	  /*  
        ---- Calculate `log_lik` for model comparison and `y_hat`  
             for in-sample y_sim posterior predictive checks. ---- 
      */
	  //
	  /*
         ---- Kalman-marginalized log-likelihood contribution ----
	  */
      log_lik[t] = normal_lpdf(residuals[t] | yhat_gq, sqrt(SS_gq));
	  /*
         ---- diagnostic: original Student-t mixture log-likelihood ----
	  */
      mixture_sigma_gq = sqrt(regime_var_gq / w[t]);
      log_lik_mixture[t] = student_t_lpdf(residuals[t] | 3.82426, 0, mixture_sigma_gq);  //  Accordingly with Figure1.1e
	  /*
          Accumulate totals
	  */
      total_log_lik += log_lik[t];
      total_log_lik_mixture += log_lik_mixture[t];
	  /*
                        ---- Kalman update ----
	  */
      Kgain_gq = (Pmat_gq * H_gq) / SS_gq;
      m_gq = m_gq + Kgain_gq * (residuals[t] - yhat_gq);
      Pmat_gq = (diag_matrix(rep_vector(1.0,2)) - Kgain_gq * H_gq') * Pmat_gq;
	  /*
            ---- Draw oscillator contribution for posterior predictive ----
	  */
      xdraw_gq = multi_normal_rng(m_gq, Pmat_gq);     // Draw latent oscillator state
      osc_part_gq = normal_rng(dot_product(H_gq, xdraw_gq), osc_sigma_eps);    // Contribution with observation jitter
	  /* ---- Populate the historical part of sigma_new ----*/
      if (probability[t, 1] >= probability[t, 2]) {
          sigma_new[t] = sigma[t, 1];
         } else {
             sigma_new[t] = sigma[t, 2];
      }
	  /* 
	        ---- Simulated return ----
	  */
      y_sim[t] = student_t_rng(3.82426, mu_mean[t] + osc_part_gq, sigma_new[t]); //  Accordingly with Figure1.1e
	  //
	  // Propagate oscillator forward
      if (t < N) {
        c_gq = cos(osc_omega[t]);
        s_gq = sin(osc_omega[t]);
        Rmat_gq[1,1] = c_gq;  Rmat_gq[1,2] = -s_gq;
        Rmat_gq[2,1] = s_gq;  Rmat_gq[2,2] =  c_gq;
        m_gq = osc_r * (Rmat_gq * m_gq);
        Pmat_gq = osc_r * (Rmat_gq * Pmat_gq * (osc_r * Rmat_gq'))
            + square(osc_sigma_eta) * diag_matrix(rep_vector(1.0,2));
        }
   }
   //  Start regime forecast from t = N+1
   fourier_terms_4 = 0.0;
      if (k1 > 0) {
        for (j in 1:k1) { 
		  fourier_terms_4 +=  alfa1[j] * sin((2*pi()*f1[j]) *(N+1)) + beta1[j] * cos((2*pi()*f1[j]) * (N+1));
         }
   }
   periodic_component_sum_new[N+1] = fourier_terms_4;
   /*      
                       One-step-ahead forecast of state probabilities:
                                     p(s_{N+1} | y_{1:N})
						  
        HMM one-step-ahead regime forecast: p(s_{N+1}|y_{1:N}) = A' * p(s_N|y_{1:N})
        Normalize to ensure probabilities sum to 1 and avoid numerical drift.
   */
   prob_forecast[N+1] = to_vector(A' * to_matrix(probability[N]));
   prob_forecast[N+1] /= sum(prob_forecast[N+1]); // normalizes
   //
   omega_regime_new =  dot_product(omega_sorted,   prob_forecast[N+1]);
   alpha_regime_new =  dot_product(alpha_sorted,   prob_forecast[N+1]);
   psi_regime_new  =   dot_product(psi_sorted,   prob_forecast[N+1]);
   beta_regime_new = dot_product(beta_sorted,   prob_forecast[N+1]);
   phix_1_regime_new = dot_product(phix_1_sorted,   prob_forecast[N+1]);
   phix_2_regime_new = dot_product(phix_2_sorted,   prob_forecast[N+1]);
   phi_lstm_regime_new = dot_product(phi_lstm_sorted,   prob_forecast[N+1]);   
   //   
   /*
             GJR_GARCH_X terms
   */
   residuals_sq_new[N+1] = square(residuals[N]);
   //
   v_g_o_new[N+1] = (momentum * v_g_o_new[N] + epsilon*thetv_o*h[N]);
   v_n_d_new[N+1] = (momentum * v_n_d_new[N] + epsilon*thnv_d*h[N]);
   v_g_i_new[N+1] = (momentum * v_g_i_new[N] + epsilon*thnv_i*h[N]);
   v_g_f_new[N+1] = (momentum * v_g_f_new[N] + epsilon*thnv_f*h[N]);
   g_o_new[N+1]  = inv_logit(v_g_o_new[N+1] + thnw_o*residuals_sq[N] + thnb_o);
   n_d_new[N+1]  = tanh(v_n_d_new[N+1] + thnw_d*residuals_sq[N] +  thnb_d);
   g_i_new[N+1]  = inv_logit(v_g_i_new[N+1] + thnw_i*residuals_sq[N] + thnb_i);
   g_f_new[N+1]  = inv_logit(v_g_f_new[N+1] + thnw_f*residuals_sq[N] + thnb_f);
   C_new[N+1] = g_i_new[N+1]*n_d_new[N+1]  + g_f_new[N+1]*C_new[N];
   h_new[N+1] = g_o_new[N+1]*tanh(C_new[N+1]);
   eta_new[N+1] = W_out*h_new[N+1] + B_out;
   //
   var_t_new[N+1] = omega_regime_new
            + alpha_regime_new * residuals_sq_new[N+1]
            + beta_regime_new * square(sigma_new[N]);			
   if (use_x1 == 1){
	        var_t_new[N+1] += phix_1_regime_new * x_1[N];
   }
   if (use_x2 == 1){
            var_t_new[N+1] += phix_2_regime_new * x_2[N];
   }
   if (residuals[N] < 0){
            var_t_new[N+1] += psi_regime_new*residuals_sq_new[N+1];
   }
   // Long-Memory LSTM filter 
   var_t_new[N+1] += phi_lstm_regime_new*eta_new[N+1];	
   // Days-of-the-week dummies
   for (j in 1:2) { // Current state
     var_t_new[N+1] += monday_centered[j]* date[N+1,1] + tuesday_centered[j]*date[N+1,2] + wednesday_centered[j]*date[N+1,3]
                       + thursday_centered[j]*date[N+1,4] + friday_centered[j]*date[N+1,5];
   }
   //
   var_t_new[N+1]  = ReLU(var_t_new[N+1]);
   sigma_new[N+1]  = sqrt(var_t_new[N+1]);
   /* 
   ====================================================================
       ---- Oscillator contribution at t = N+1 (forecast step) ----
   ====================================================================
   */
   /*
   ====================================================================
                 FORECASTING WITH KALMAN OSCILLATOR
   ====================================================================

      Purpose:
      --------
	  
         Extend the in-sample Kalman oscillator state into the future
         (t = N+1 … N+T_forecast), so forecasts include cyclical dynamics.

      How it works:
      -------------
   
         1. Seed the forecast:
            * Use last in-sample Kalman state (m, Pmat) as the starting point.
            * Initialize frequency with last observed omega_N.

         2. Evolve frequency:
            * omega_f follows a random walk:
              
			  "omega[t+1] = omega[t] + osc_sigma_nu * Normal(0,1)";
			  
            * This allows frequency to drift stochastically across time.

         3. Propagate oscillator state:
            * Build rotation matrix R(omega_f) = [[cos omega, -sin omega], [sin omega, cos omega]].
            * Rotate the latent state forward and apply damping osc_r.
            * Add process noise osc_sigma_eta to spread uncertainty.

         4. Sample latent oscillator state:
            * x_f ~ MultiNormal(m_f, P_f).
            * x_f[1] = latent cosine cycle.
            * x_f[2] = latent sine cycle.

         5. Observable oscillator contribution:
            * Extract contribution with osc_part_forecast = H * x_f.
                
				Option A: latent cycle only (deterministic cycle).
                Option B: latent cycle + measurement noise:
				
            "osc_part_forecast = normal_rng(H * x_f, osc_sigma_eps)";

         6. Add oscillator to forecast mean:
            * mu_mean_new[t] += osc_part_forecast
            * Forecasts now carry cyclical dynamics + mensurament noise.

         Interpretation:
         ---------------
   
           * This produces forecasts that oscillate in a smooth,
             quasi-periodic fashion (cycle from oscillator),
             while still allowing for stochastic variation (random walk omega,
             process noise, measurement noise).
			 
          * Effectively: AR/MA/GRU/GARCH/jumps capture fast shocks,
            the oscillator adds "medium-term cycles",
            and both combine in y_sim[t].
   */
   //
   /*  
        Start predictive state from last in-sample Kalman filter 
		 (Seed forecast state from last filtered state)
   */
   m_f = m;        // seed predictive mean state from t = N
   P_f = Pmat;     // seed predictive covariance from t = N
   // Initialize frequency forecast with last known omega
   omega_f = osc_omega[N]; 
   {   
     //         Perform one-step-ahead forecast (N+1)
     /*   
	      ---- Random walk step for oscillator frequency ----
          
		        Evolve omega by one innovation step
		     (random walk with step-size osc_sigma_nu)
	 */
     omega_f = omega_f + osc_sigma_nu * normal_rng(0, 1);
     /*
     	 ---- Build rotation matrix for oscillator dynamics ---- 
	       This rotates the state forward by angle omega_f
     */
     c_gq = cos(omega_f);      // cosine component
     s_gq = sin(omega_f);      // sine component
     // Fill rotation matrix entries
     Rmat_gq[1,1] = c_gq;  
     Rmat_gq[1,2] = -s_gq;
     Rmat_gq[2,1] = s_gq;  
     Rmat_gq[2,2] =  c_gq;
     /* 
	      ---- Propagate oscillator state mean and covariance ----
        Apply damping (osc_r) and add process noise (osc_sigma_eta)  
		
     */
     m_f = osc_r * (Rmat_gq * m_f);
     P_f = osc_r * (Rmat_gq * P_f * (osc_r * Rmat_gq'))
       + square(osc_sigma_eta) * diag_matrix(rep_vector(1.0,2));
     /*   
	        ---- Draw forecast oscillator latent state ----
        Sample from predictive distribution of oscillator state   
	 */
     x_f = multi_normal_rng(m_f, P_f);
     /*   
        	        ---- Extract observable oscillator contribution ----
          First coordinate is the latent cycle contribution (cosine component) with measurement noise
	 */
     osc_part_forecast = normal_rng(dot_product(H, x_f), osc_sigma_eps);
   }
   //
   /*
                             ---- Sampling new residuals  ----
            Samples new residuals based on the sorted conditional standard deviation
		    from the HMM states.
         
   */	  
   residuals_new[N+1] = student_t_rng(3.82426, 0.0 , sigma_new[N+1]);
   /*   
	                      ---- GRU momentum update for t= N+1 ---- 
   */
   v_g_f_gru_forward_new[N+1] = (momentum * v_g_f_gru_forward[N] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward[N]);
   g_f_gru_forward_new[N+1] =   inv_logit(v_g_f_gru_forward_new[N+1] + thnv_f_gru_scalar_forward*y[N] + thnb_f_gru_scalar_forward);
   v_h_gru_forward_new[N+1] =   (momentum *v_h_gru_forward[N] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward_new[N+1] + h_gru_forward[N]));
   h_gru_forward_new[N+1] =     tanh(v_h_gru_forward_new[N+1] + theta_h_gru_scalar_forward*y[N] + thnb_h_gru_scalar_forward);
   h_gru_forward_new_updated[N+1] = (1- g_f_gru_forward_new[N+1])*h_gru_forward[N] +  g_f_gru_forward_new[N+1] *h_gru_forward_new[N+1]; 
   /*  
	         *** For the backward GRU in forecasting:
                 Freezing the last backward hidden state.
                 This is the most common approach for practical forecasting with B-RNNs.
                 Use the last computed backward state from observed data
		 
    */
    h_gru_concatenate_new[N+1] = gru_hidden_state_scalar*((h_gru_backward_updated[N] + h_gru_forward_new_updated[N+1])/2);
    /*			
	            ---- Conditional mean iterations for forecasting at t= N+1 ----
	                            *** Simulating new jumps ***
    */
    is_slab = 0;
    is_slab = bernoulli_rng(pi_z); // 1 = slab (Huge jump), 0 = spike (almost zero)
    if (is_slab == 1){
          jumps_new[N+1] = normal_rng(0, sigma_slab);
      }else{
          jumps_new[N+1] = normal_rng(0, sigma_spike);
    }
    /*   
	            ---- Conditional mean iterations for forecasting ----
		                              FARMAX
    */
    mu_mean_new[N+1] = mu;
    mu_mean_new[N+1]  += periodic_component_sum_new[N+1]; 
    if (k > 0)  for (i in 1:k) {
        for (mm in 1: k) {
          mu_mean_new[N+1]  += g_1[i, mm + 1] * y[N];
         }
    }
    if(p > 0) for (i in 1:p){ // If AR component is specified
		  mu_mean_new[N+1]  += phi_mean[i]*(h_gru_concatenate_new[(N+1)-i]); // AR terms on past observed terms.
	}
    if(q > 0) for(j in 1:q){ // If MA component is specified
          mu_mean_new[N+1]  += theta_mean[j] * residuals_new[(N+1)-j]; // MA terms on past residuals.
    }
    mu_mean_new[N+1]  += kpi*signals[N];
    mu_mean_new[N+1]  += jumps_new[N+1]; 
	mu_mean_new[N+1]  += price_action_3 * x_3[N]; 
    mu_mean_new[N+1]  += price_action_4 * x_4[N]; 
    mu_mean_new[N+1]  += price_action_5 * x_5[N]; 
    mu_mean_new[N+1]  += price_action_6 * x_6[N]; 
    mu_mean_new[N+1]  = swish( mu_mean_new[N+1] , beta_swish);	
    /*
               ---- Add oscillator contribution to the mean ----
    */
    mu_mean_new[N+1] = mu_mean_new[N+1] + osc_part_forecast;
    //
    /*    
	           ---- Simulating new log-returns (Posterior) at t= N+1 ---- 
    */	  
    y_sim[N+1] = student_t_rng(3.82426,mu_mean_new[N+1], sigma_new[N+1]);
    // Regime forecast from t = N+2 and so on ...
    for(t in (N+2):(N+T_forecast)) {
        prob_forecast[t] = to_vector(A' * to_matrix(prob_forecast[t-1]));
        prob_forecast[t] /= sum(prob_forecast[t]);
		//
        omega_regime_new =  dot_product(omega_sorted,   prob_forecast[t]);
        alpha_regime_new =  dot_product(alpha_sorted,   prob_forecast[t]);
        psi_regime_new  =   dot_product(psi_sorted,   prob_forecast[t]);
        beta_regime_new = dot_product(beta_sorted,   prob_forecast[t]);
        phix_1_regime_new = dot_product(phix_1_sorted,   prob_forecast[t]);
        phix_2_regime_new = dot_product(phix_2_sorted,   prob_forecast[t]);
		phi_lstm_regime_new= dot_product(phi_lstm_sorted,   prob_forecast[t]);
        /*
           GJR_GARCH-X terms
        */
        residuals_sq_new[t] = square(residuals_new[t-1]);
		//
        v_g_o_new[t] = (momentum * v_g_o_new[t-1] + epsilon*thetv_o*h_new[t-1]);
        v_n_d_new[t] = (momentum * v_n_d_new[t-1] + epsilon*thnv_d*h_new[t-1]);
        v_g_i_new[t] = (momentum * v_g_i_new[t-1] + epsilon*thnv_i*h_new[t-1]);
        v_g_f_new[t] = (momentum * v_g_f_new[t-1] + epsilon*thnv_f*h_new[t-1]);
        g_o_new[t]  = inv_logit(v_g_o_new[t-1] + thnw_o*residuals_sq_new[t-1] + thnb_o);
        n_d_new[t]  = tanh(v_n_d_new[t-1] + thnw_d*residuals_sq_new[t-1] +  thnb_d);
        g_i_new[t]  = inv_logit(v_g_i_new[t-1] + thnw_i*residuals_sq_new[t-1] + thnb_i);
        g_f_new[t]  = inv_logit(v_g_f_new[t-1] + thnw_f*residuals_sq_new[t-1] + thnb_f);
        C_new[t] = g_i_new[t]*n_d_new[t]  + g_f_new[t]*C_new[t-1];
        h_new[t] = g_o_new[t]*tanh(C_new[t]);
		eta_new[t] = W_out*h_new[t] + B_out;	
		//
        var_t_new[t] = omega_regime_new
            + alpha_regime_new * residuals_sq_new[t]
            + beta_regime_new * square(sigma_new[t-1]);
	    if (use_x1 == 1){
	        var_t_new[t] += phix_1_regime_new * x_1[t-1];
	        }
	    if (use_x2 == 1){
            var_t_new[t] += phix_2_regime_new * x_2[t-1];
	      }
        if (residuals_new[t-1] < 0){
          var_t_new[t] += psi_regime_new*residuals_sq_new[t];
        }
		// Long-Memory LSTM filter 
		var_t_new[t] += phi_lstm_regime_new*eta_new[t];
	    // Days-of-the-week dummies
		for (j in 1:2) { // Current state
	       var_t_new[t] += monday_centered[j]* date[t,1] + tuesday_centered[j]*date[t,2] + wednesday_centered[j]*date[t,3]
	                       + thursday_centered[j]*date[t,4] + friday_centered[j]*date[t,5];
		}
        //
        var_t_new[t] = ReLU(var_t_new[t]);		
        sigma_new[t] = sqrt(var_t_new[t]);
        //
        /*
           ---- Sampling new residuals  ----
        */	  
        residuals_new[t] = student_t_rng(3.82426, 0.0 , sigma_new[t]);
        /*  
           GRU terms
        */
	    v_g_f_gru_forward_new[t] = (momentum * v_g_f_gru_forward_new[t-1] + epsilon*thnw_f_gru_scalar_forward*h_gru_forward_new[t-1]);
		g_f_gru_forward_new[t] = inv_logit(v_g_f_gru_forward_new[t] + thnv_f_gru_scalar_forward*y_sim[t-1] + thnb_f_gru_scalar_forward);
		v_h_gru_forward_new[t] =  (momentum *v_h_gru_forward_new[t-1] + epsilon*thnw_h_gru_scalar_forward*(g_f_gru_forward_new[t] + h_gru_forward_new[t-1]));
		h_gru_forward_new[t] =  tanh(v_h_gru_forward_new[t] + theta_h_gru_scalar_forward*y_sim[t-1] + thnb_h_gru_scalar_forward);
		h_gru_forward_new_updated[t] = (1- g_f_gru_forward_new[t])*h_gru_forward_new[t-1] +  g_f_gru_forward_new[t] *h_gru_forward_new[t]; 
		h_gru_concatenate_new[t] = (h_gru_backward_updated[N] + h_gru_forward_new_updated[t])/2;
	    h_gru_concatenate_new[t] = gru_hidden_state_scalar*((h_gru_backward_updated[N] + h_gru_forward_new_updated[t])/2);			
	    /*			
	         Simulating new jumps 
	    */
	    is_slab = 0;
        is_slab = bernoulli_rng(pi_z); 
        if (is_slab == 1){
            jumps_new[t] = normal_rng(0, sigma_slab);
        }else{
            jumps_new[t] = normal_rng(0, sigma_spike);
        }
	    fourier_terms_5 = 0.0;
        if (k1 > 0) {
          for (j in 1:k1) { 
            fourier_terms_5 += alfa1[j] * sin((2*pi()*f1[j]) *(t)) + beta1[j] * cos((2*pi()*f1[j]) * (t));
             }
        }
	    periodic_component_sum_new[t] = fourier_terms_5;		
        /*			
	      FARMAX 
	    */
        mu_mean_new[t] = mu;
		mu_mean_new[t] += periodic_component_sum_new[t];
        if (k > 0)  for (i in 1:k) {
          for (mm in 1:k) {
            mu_mean_new[t] += g_1[i, mm + 1] * y_sim[t - mm];
          }
        }
        if(p > 0) for (i in 1:p){ // If AR component is specified
		    mu_mean_new[t] += phi_mean[i]*(h_gru_concatenate_new[t-i]); // AR terms on past observed terms.
	    }
        if(q > 0) for(j in 1:q){ // If MA component is specified
            mu_mean_new[t] += theta_mean[j] * residuals_new[t-j]; // MA terms on past residuals.
        }
	    mu_mean_new[t] += kpi*signals[t-1];
	    mu_mean_new[t] += jumps_new[t]; 
	    mu_mean_new[t] += price_action_3 * x_3[N];
        mu_mean_new[t] += price_action_4 * x_4[N];
        mu_mean_new[t] += price_action_5 * x_5[N];
        mu_mean_new[t] += price_action_6 * x_6[N];
	    mu_mean_new[t]  = swish( mu_mean_new[t], beta_swish);	
        /*
        ======================================================
           ---- Oscillator forecast update (Kalman style) ----
        ======================================================
        */
		//
        /*   
            ---- Random walk step for oscillator frequency ----
     
          The oscillator frequency evolves as a random walk:
        
		    *  omega_f(t) = omega_f(t-1) + omega_v * sigma,   sigma ~ N(0,1)

         This allows the cycle frequency to drift slowly over time.
		 
        */
        omega_f = omega_f + osc_sigma_nu * normal_rng(0, 1);
        /*
     
	        ---- Build rotation matrix for oscillator dynamics ----
     
          Rotate the 2D oscillator state forward by angle omega_f.
          This enforces sinusoidal dynamics in the latent oscillator state.
     
               [x1(t+1)]   [ cos(omega)  -sin(omega) ] [x1(t)]
               [x2(t+1)] = [ sin(omega)   cos(omega) ] [x2(t)]
        */
        c_gq = cos(omega_f);
        s_gq = sin(omega_f);
        Rmat_gq[1,1] = c_gq;  Rmat_gq[1,2] = -s_gq;
        Rmat_gq[2,1] = s_gq;  Rmat_gq[2,2] =  c_gq;
        /*
             
			 ---- Propagate oscillator state mean and covariance ----
     
          Apply:
            * Linear state transition (rotation + damping factor osc_r)
            * Process noise injection (osc_sigma_eta)
     
          This updates both:
            - m_f : predictive mean of oscillator state
            - P_f : predictive covariance of oscillator state
        */
        m_f = osc_r * (Rmat_gq * m_f);
        P_f = osc_r * (Rmat_gq * P_f * (osc_r * Rmat_gq'))
        + square(osc_sigma_eta) * diag_matrix(rep_vector(1.0,2));
        /*
     
	         ---- Draw forecast oscillator latent state ----
     
          Sample x_f ~ Normal(m_f, P_f)
          This represents the uncertain oscillator latent state at step t.
     
              - x_f[1] = cosine component
              - x_f[2] = sine component
        */
		x_f = multi_normal_rng(m_f, P_f);
        /*
     
	        ---- Extract observable oscillator contribution ----
     
          The observed contribution is projection onto H = [1,0].
          This picks out the cosine component of the latent state.

            Option A (default): use latent cycle directly
           		   osc_part_forecast = x_f[1]
     
	        Option B (with measurement noise):
                   osc_part_forecast = normal_rng(x_f[1], osc_sigma_eps)
        */
        // "osc_part_forecast = dot_product(H, x_f);  // = x_f[1] Option A "
		osc_part_forecast = dot_product(H, x_f);
        /*
     
	        ---- Add oscillator contribution to conditional mean ----
     
          The latent oscillator’s effect is added to mu_mean_new[t],
          alongside other components (Fourier terms, ARMA, GRU, jumps, etc.)
  
        */
        mu_mean_new[t] = mu_mean_new[t] +  osc_part_forecast;
	    /*  
     	      ---- Simulating new log-returns (Posterior) ---- 
        */		
	    y_sim[t] = student_t_rng(3.82426,mu_mean_new[t], sigma_new[t]);
    }	
}


"""
Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X= pystan.StanModel(model_code = Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X)

#   Data truction to fit log returns length

KPI_lenght_to_trim =   len(Fundamental_analysis_KPI_PETR3_SA) - len(PETR3_SA_Close_fractional_difference_Linear_filtred)
Time_length_to_trim =  len(PETR3_SA['Volume']) - len(PETR3_SA_Close_fractional_difference_Linear_filtred)


#   Exogenous Regressors 

Volume_estimated_mean_scaled = function_to_scale_data(Volume_estimated_mean.values.astype(np.float64),0.001,1.0)
Returns_volatility_mean_scaled = function_to_scale_data(Returns_volatility_mean.values.astype(np.float64),0.001,1.0)
ema_fast = function_to_scale_data(Price_Action_KPI_PETR3_SA.iloc[:,0].values.astype(np.float64),-1.0,1.0)
ema_slow = function_to_scale_data(Price_Action_KPI_PETR3_SA.iloc[:,1].values.astype(np.float64),-1.0,1.0)
support =  function_to_scale_data(Price_Action_KPI_PETR3_SA.iloc[:,2].values.astype(np.float64),-1.0,1.0)
resistance = function_to_scale_data(Price_Action_KPI_PETR3_SA.iloc[:,3].values.astype(np.float64),-1.0,1.0)

data  = ({'k1': 2, # Related to f1
          'p' : 2, # Figure_1.1h
		  'q' : 2, # Figure_1.1i
		  'k' : 2, # number of peaks in Figure_1-1m (Spectogram analysis)
          'N' : len(PETR3_SA_Close_fractional_difference_Linear_filtred_train),
          'T_forecast' : forecasting_extrapolation_lengh,
		  'f1' : [1/30.25, 1/34.57],
		  'signals': Fundamental_analysis_KPI_PETR3_SA[KPI_lenght_to_trim:]["signal"].values.flatten().astype(np.int64),
          'date': dates_dummies[Time_length_to_trim:].values.astype(np.int64),
		  'y':   PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten().astype(np.float64),
		  'use_x1': 1,
          'use_x2': 1,
		  'x_1': Volume_estimated_mean_scaled,
		  'x_2': Returns_volatility_mean_scaled,
          'x_3': ema_fast,
          'x_4': ema_slow,
          'x_5': support,
          'x_6': resistance
	     })
		 
control = {}
control['max_treedepth'] = 11
control['adapt_delta'] = 0.9999

fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X = Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.sampling(data=data, iter=15000, chains=1, warmup=7000 ,thin=1, seed=101, n_jobs = 1, control=control)

samples = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.extract()

# Parameters of interest and their dimensions
param_names = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.sim['pars_oi']
param_dims = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.sim['dims_oi']
param_dict = dict(zip(param_names, param_dims))

# EXCLUDE: everything not declared in the `parameters {}` block

EXCLUDE = {
    # Transformed parameters
    "phi_mean", "theta_mean", "mu_mean", "residuals", "g_1", "u_1", "fourier_terms_1", "fourier_terms_2","fourier_terms_3", "periodic_component_sum",
	"sigma0_sorted", "omega_sorted", "alpha_sorted", "psi_sorted", "beta_sorted", "phix_1_sorted", "phix_2_sorted", "phi_lstm_sorted", "monday_centered", 
    "tuesday_centered", "wednesday_centered" , "thursday_centered", "friday_centered", "week_effect_avg", "residuals_sq", "var_t", "sigma", "v_g_f_gru_forward", 
	"v_h_gru_forward", "v_g_f_gru_backward", "v_h_gru_backward","g_f_gru_forward","h_gru_forward","h_gru_forward_updated", "g_f_gru_backward","h_gru_backward",
	"h_gru_backward_updated", "h_gru_concatenate", "h","C", "n_d","g_i","g_o","g_f","v_g_o", "v_n_d","v_g_i","v_g_f", "eta", "H","m","Pmat","c","s","Rmat","yhat",
	"regime_var", "obs_var","S", "Kgain","accumulator","log_alpha","probability", "A"
  
	# Generated quantities
    "jumps_new", "int is_slab", "y_sim", "fourier_terms_4", "fourier_terms_5","periodic_component_sum_new", "prob_forecast", 
    "log_lik", "log_lik_mixture", "total_log_lik", "total_log_lik_mixture", "H_gq", "m_gq", "Pmat_gq", "regime_var_gq", 
    "obs_var_gq", "yhat_gq", "SS_gq", "mixture_sigma_gq", "Kgain_gq", "xdraw_gq", "osc_part_gq", "m_f", "P_f", "omega_f",
   	"c_gq","s_gq", "Rmat_gq", "x_f", "osc_part_forecast", "sigma_new", "omega_regime_new", "alpha_regime_new", "psi_regime_new",
    "beta_regime_new","phix_1_regime_new", "phix_2_regime_new", "phi_lstm_regime_new", "mu_mean_new", "residuals_new", "residuals_sq_new",
	"var_t_new","h_new","C_new","eta_new","n_d_new", "g_i_new", "g_o_new", "g_f_new", "v_g_o_new", "v_n_d_new", "v_g_i_new", "v_g_f_new",
	"v_g_f_gru_forward_new", "v_h_gru_forward_new", "g_f_gru_forward_new", "h_gru_forward_new", "h_gru_forward_new_updated",
    "h_gru_concatenate_new"	
}

# Filter only those in the `parameters` block
filtered_param_dict = {
    name: dim for name, dim in param_dict.items()
    if name not in EXCLUDE
}

# Count total number of scalar parameters
total_filtered_params = sum(np.prod(dim) if dim else 1 for dim in filtered_param_dict.values())

print("Total number of scalar parameters (parameters block only):", total_filtered_params)
#  Total number of scalar parameters (parameters block only): 1094


Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X = az.from_pystan(
    posterior=fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X,
    observed_data=["y"],
    log_likelihood="log_lik_mixture"
)


# --- Calculate WAIC ---
waic_result = az.waic(Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X)

print("\n--- WAIC Results ---")
print(waic_result)

Computed from 8000 by 242 log-likelihood matrix

#             Estimate       SE
#   elpd_waic -29829.22  1839.40
#   p_waic    29625.86        -


###################################*/                      \*###################################
#
#    
#
#               Model 0: elpd_waic = 24.73, SE = 19.20
#
#               Model 1: elpd_waic = -29829.22, SE = 1839.40
#               
#               Difference = 29804.49,
#               Sum of SEs = 19.20 + 1839.40 = 1858.60
#               Since 29804.49 > 1858.60 -> There are significant differences betweem them
#
###################################*/                      \*###################################

# Define the directory and filename
save_directory = r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA'
filename = 'Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.pkl'

# Construct the full file path
full_path = os.path.join(save_directory, filename)

# Save the model
with open(full_path, 'wb') as f:
    pickle.dump(fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X, f)

# Load the model
with open(full_path, 'rb') as f:
    Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X = pickle.load(f)

#=============*/        \*=================	    

samples = fit_Quasi_Bidirectional_MomentumGRU_Gegenbauer_FARMA_X_LSTM_MS_GJR_GARCH_X.extract()

#=============*/        \*=================	

y_hat = samples["y_sim"]
  
pd.DataFrame(y_hat).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\log_returns_estimated.csv', index=None)
log_returns_estimated = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/log_returns_estimated.csv")

# ---- Plotting ----
#          (Log-returns train sample)

log_returns_estimated_mean = log_returns_estimated.mean(axis=0)

x_lower, x_upper = np.percentile(log_returns_estimated, [2.5, 97.5], axis=0)

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten() , label='Observed log returns train (y_obs_train)', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], log_returns_estimated_mean[:-forecasting_extrapolation_lengh], label='Log-returns In Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[:-forecasting_extrapolation_lengh], x_lower[:-forecasting_extrapolation_lengh],x_upper[:-forecasting_extrapolation_lengh], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Log-returns in Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()


# Figure_2.2a_In_Sample_Log_Returns.

############################
#   In Sample regression   #
#        metrics KPI       #
#                          #
############################                                                                                                                                                            

MSE = mean_squared_error(log_returns_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel(),PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel())
 
RMSE = math.sqrt(MSE)
print('RMSE: %f' % RMSE)

#  RMSE: 0.155281

#             -------------------*/          \*-------------------
#
#   RMSE (Root Mean Squared Error):
#
#   Given that the log returns scale is primarily between −1 and +1, 
#   an RMSE of 0.155281 indicates a very good fit for the in-sample data.
#
#   The model's average prediction error is relatively small compared to 
#   the typical range and magnitude of the returns. Visually, the blue estimated line 
#   closely tracks the grey observed data. This suggests the model performs well in 
#   capturing the dynamics of the log returns within the training period.
#
#             -------------------*/          \*------------------- 

mae = mean_absolute_error(log_returns_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel(),PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel())
print('MAE: %f' % mae)

#  MAE: 0.137470


#             -------------------*/          \*-------------------
#
#   MAE (Mean Absolute Error):
#
#   Since the log returns scale is between −1 and +1, an MAE of 0.137470 indicates a strong performance in fitting the data. 
#   The average error is quite small relative to the typical magnitude of the returns.This relatively small difference 
#   between MAE and RMSE suggests that there are not many extremely large prediction errors (outliers) in the in-sample estimation.
#   If there were significant outliers, the RMSE would be much larger than the MAE.      
#
#             -------------------*/          \*------------------- 

coefficient_of_dermination = r2_score(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel(),log_returns_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel())
print('R2: %f' % coefficient_of_dermination)
#  R2:  0.547261

#             -------------------*/          \*------------------- 
#
#   R-squared (R²): Coefficient of Determination
#   
#   Value: An R2 of 0.547261 (or 54.73%) means that 54.73% of the variability in the PETR3_SA log returns within the training 
#   sample is explained by the model 1.
#
#   Goodness of Fit: 
#
#   For time-series modeling of financial returns—which are notoriously difficult to predict and highly volatile (as shown by the wide 95% Credible Interval),
#   an R2 value lie this is typically considered strong for an in-sample fit. It confirms that the model is capturing a significant portion of the systematic
#   movements in the returns, not just random noise.
#
#   Context: 
#   
#   While R2 values are often high in non-financial applications, for market data, achieving this R2 suggests the model has effectively learned the historical 
#   patterns and relationships within the training data.
#
#             -------------------*/          \*------------------- 

Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test = ks_2samp(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel(),log_returns_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel())
print('statistic: %f   ,pvalue: %f' % (Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[0],Kolmogorov_Smirnoff_Adherence_One_Dimensional_Sampling_Test[1]))
#   statistic: 0.363636   ,pvalue: 0.000000

#             -------------------*/          \*------------------- 
#
#   Kolmogorov–Smirnov (KS) Two-Sample Test for Distribution Adherence
#
#   Statistic: 0.363636
#   p-value:   0.000000
#
#   We must reject the null hypothesis (H0​) that the observed log returns and the estimated log returns 
#   come from the same underlying distribution.
#
#   In the context of your model's performance:
#
#   The KS test strongly suggests that, although your model provides a good point-estimate fit 
#   (as indicated by the low MAE and RMSE, and high R2), the distribution of its residuals (errors) 
#   is systematically different from the distribution of the actual observed log returns. 
#   Your model's predicted distribution is not a statistically accurate representation of the actual returns distribution. 
#   This is a common finding in financial modeling, where even models with a good fit might fail distributional tests 
#   due to the complex, non-Gaussian nature of financial data.
#
#             -------------------*/          \*-------------------

# ---- QQplot between the two samples ----

# Sort both arrays ( It's needed to align the percentiles in both distributions)
sorted_obs = np.sort(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.ravel().astype(np.float64))
sorted_hat = np.sort(log_returns_estimated_mean[:-forecasting_extrapolation_lengh].values.ravel().astype(np.float64))


# Q-Q Plot between y_obs and y_hat
plt.figure(figsize=(6, 6))
plt.plot(sorted_obs, sorted_hat, 'o')
plt.plot([sorted_obs.min(), sorted_obs.max()],
         [sorted_obs.min(), sorted_obs.max()], 'r--')  # 45-degree line
plt.title("Q-Q Plot: y_hat vs y_obs model 1")
plt.xlabel("Observed Quantiles for Log Returns In-Sample model 1")
plt.ylabel("Predicted Quantiles for Log Returns In-Sample model 1")
plt.grid(True)
plt.show()


# Figure_2.2b_Q-Q_Plot_between_log_returns_In_Sample_and_estimated

#             -------------------*/          \*-------------------
#
#   Q–Q Plot Interpretation: Predicted (v_hat) vs. Observed (v_obs) Volume Quantiles
#
#   Central Region (Around 0.0):
#
#       The data points in the center (from about −0.25 to 0.25 on the axes) lie very close to the red dashed line.
#
#       Conclusion: 
#		
#		This indicates that for the vast majority of typical (non-extreme) log returns, the model's predicted distribution 
#		is a very good match for the observed distribution.
#
#   Tails (Extremes):
#
#       Lower Tail (Negative Returns): 
#		
#		As we move towards the extreme negative returns (below −0.5 on the x-axis), the points deviate significantly below the red line.
#       The last two points are near x=−1.0 and x=−0.75, but the corresponding y values are lower (near −1.0 and −0.75 respectively, indicating they are further away from the mean).
#
#       Specific Insight: 
#		
#		The model underestimates the magnitude of extreme negative returns (i.e., it doesn't predict losses as severe as those actually observed, 
#		or the extreme observed losses occur less frequently than the model's distribution predicts).
#
#       Upper Tail (Positive Returns): 
#		
#		As we go towards the extreme positive returns (above 0.5 on the x-axis), the points deviate significantly above the red line.
#
#       Specific Insight: 
#       
#       The model underestimates the magnitude of extreme positive returns (i.e., it doesn't predict gains as large as those actually observed, 
#       or the extreme observed gains occur less frequently than the model's distribution predicts).
#
#   Relationship to the KS Test Result
#
#   This Q-Q plot visually confirms the result of the Kolmogorov-Smirnov (KS) test (p-value=0.000000).
#
#   Good Fit (Central Region): The small MAE and RMSE are driven by the excellent fit in the center.
#
#   Distributional Mismatch (Tails): The significant deviation in the tails is the reason the KS test rejected the null hypothesis. The model fails to accurately capture the full extent of the observed distribution's fat tails (extreme events), which is a common characteristic of financial return data.
#
#             -------------------*/          \*-------------------


####################################
#  Out of Sample                   #
#        predictive KPI peformace  #
#                                  #
####################################

# (Log-returns test sample) 

fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 10), sharex=True)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], PETR3_SA_Close_fractional_difference_Linear_filtred_test.values.flatten().astype(np.float64) , label='Observed log-returns  test (y_test)', color='grey', alpha=0.7)
ax.plot(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], log_returns_estimated_mean[-forecasting_extrapolation_lengh:], label='Log-returns Out of Sample', color='blue', linewidth=2)
ax.fill_between(PETR3_SA_Dates_log_dif[-forecasting_extrapolation_lengh:], x_lower[-forecasting_extrapolation_lengh:],x_upper[-forecasting_extrapolation_lengh:], color='blue', alpha=0.2, label='95% Credible Interval')
ax.set_ylabel('Value')
ax.set_title('Estimated Log-returns Out of Sample Sample')
ax.legend()
ax.grid(True, linestyle='--', alpha=0.6)
fig.autofmt_xdate() 
plt.tight_layout() 
plt.show()

# Figure_2.2c_Out_of_Sample_Volume

#  Due to the very small out-of-sample size (only 5 observations),
#  MAE was selected as the primary regression KPI,
#  since it directly measures individual forecast accuracy.

mae = mean_absolute_error(log_returns_estimated_mean[-forecasting_extrapolation_lengh:].values.ravel(),PETR3_SA_Close_fractional_difference_Linear_filtred_test.values.ravel())
print('MAE: %f' % mae)

#  MAE: 0.395102

#             -------------------*/          \*-------------------
#
#   The gray line (Observed log-returns test) shows significant movement,
#   peaking above 0.5 and then settling near 0.3.The blue line (Log-returns Out of Sample) 
#   is nearly flat, hovering very close to 0.
#
#   The large vertical distance between the actual (gray) and the forecast (blue) 
#   line visually confirms the large MAE≈0.40. The model is failing to capture the 
#   trend or directional change that occurred in the out-of-sample period, consistently
#   underestimating the positive returns.
#
#   Credible Interval:
#
#       Although the point estimate (blue line) is far from the observed return (gray line),
#       the observed return still remains well within the 95% Credible Interval (the shaded blue area).
#       This means the model is accurately quantifying the high uncertainty/risk of the forecast, 
#       even if its mean prediction is inaccurate.
#
#   Conclusion
#
#       While the model demonstrated a strong fit in-sample (R2>0.5), the out-of-sample performance,
#       quantified by the MAE= 0.395102, reveals a poor predictive capability on unseen data. The model's forecast (blue line)
#       is too conservative and fails to capture the actual positive returns that occurred (gray line).
#
#             -------------------*/          \*-------------------

#########################################
#   Checking for volume residuals       #
#                    homocedasticity    #
#########################################

def calculate_residuals(observed_values, predicted_location, predicted_scale, epsilon=1e-8):
    """
     
    References :

        CRYER, Jonathan D.; CHAN, Kung-Sik. Time Series Analysis: With Applications in R. 2. ed. New York: Springer, 2008.

        HILPISCH, Yves. Python for Finance: Mastering Data-Driven Finance. 2. ed. Sebastopol, CA: O’Reilly Media, 2019.

        HYNDMAN, Rob J.; ATHANASOPOULOS, George. Forecasting: Principles and Practice. 3. ed. Melbourne: OTexts, 2021.
        Disponível em: [https://otexts.com/fpp3/](https://otexts.com/fpp3/). Acesso em: 25 set. 2025.

        TSAY, Ruey S. *Analysis of Financial Time Series . 3. ed. Hoboken, NJ: John Wiley & Sons, 2010.
    
    
    #   Calculates standardized residuals: (Observed - Predicted Location) / Predicted Scale.

    Args:
        observed_values (array-like): The actual observed values
        predicted_location (array-like or scalar): The model's conditional mean (location).
        predicted_scale (array-like): The model's conditional standard deviation (scale/volatility).
        epsilon (float): A small value used to check for near-zero division in the scale.

    Returns:
        numpy.ndarray: The standardized residuals.
    """
    observed_values = np.asarray(observed_values, dtype=np.float64).ravel()
    predicted_location = np.asarray(predicted_location, dtype=np.float64).ravel()
    predicted_scale = np.asarray(predicted_scale, dtype=np.float64).ravel()
    ########*/          \*########
    #
    # 1. Shape/Length Checks
    #
    ########*/          \*########
    if observed_values.shape != predicted_scale.shape:
        raise ValueError("Lengths of observed_values and predicted_scale must match.")
    ########################*/          \*########################
    #
    # Check if location is a scalar or matches the length of the data
    #
    ########################*/          \*########################
    if predicted_location.size > 1 and observed_values.shape != predicted_location.shape:
        raise ValueError("Lengths of observed_values and predicted_location must match if predicted_location is an array.")
    ########################*/          \*########################
    #
    # 2. Safety Check for Zero Division
    #
    ########################*/          \*########################
    if np.any(predicted_scale < epsilon):
        raise ValueError(
            "Predicted scale contains values too close to zero (below {}). Cannot calculate standardized residuals.".format(epsilon)
        )
    ########*/          \*########
    #
    # 3. Calculation
    #
    ########*/          \*########
    return (observed_values - predicted_location) / predicted_scale
	
conditional_mean = samples["mu_mean_new"]
  
pd.DataFrame(conditional_mean).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\conditional_mean.csv', index=None)
conditional_mean_estimated = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/conditional_mean.csv")

log_returns_location = conditional_mean_estimated.mean(axis=0)

#=============*/        \*=================

residuals_volatility = samples["sigma_new"]
  
pd.DataFrame(residuals_volatility).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\residuals_volatility.csv', index=None)
residuals_volatility_estimated = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/residuals_volatility.csv")

log_returns_scale = residuals_volatility_estimated.mean(axis=0)

#=============*/        \*=================


residuals = samples["residuals_new"]
  
pd.DataFrame(residuals).to_csv(r'C:\Users\Rafael Xavier Botelho\Documents\Econometrics_CEA\residuals.csv', index=None)
residuals_estimated = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/residuals.csv")

residuals_estimated_mean = residuals_estimated.mean(axis=0)

#=============*/        \*=================

model_1_residuals_log_returns = calculate_residuals(PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.flatten().astype(np.float64),
                   log_returns_location[:-forecasting_extrapolation_lengh], log_returns_scale[:-forecasting_extrapolation_lengh])
                   
log_returns_in_sample = log_returns_estimated_mean[:-forecasting_extrapolation_lengh]

#=============*/        \*=================
				   
# Plot Residuals vs. Fitted Values 
plt.figure(figsize=(10, 6))
sns.scatterplot(x=log_returns_in_sample, y=model_1_residuals_log_returns, alpha=0.7)
plt.axhline(0, color='red', linestyle='--', linewidth=0.8)
plt.title('Residuals vs. Fitted Values')
plt.xlabel('Fitted Values')
plt.ylabel('Residuals')
plt.grid(True, linestyle='--', alpha=0.6)
plt.show()

#             -------------------*/          \*-------------------
#
#    The residuals are not randomly scattered around y=0. The strong concentration 
#    of points in the center suggests that for small predicted returns (most of your data), 
#    the model is systematically underestimating the actual returns, resulting in 
#    consistently positive residuals. This indicates that a non-linear relationship or an 
#    omitted variable might be present.
#
#    The vertical spread is not uniform. For fitted values between ≈−0.2 and 0.2, the residuals 
#    are highly condensed. For the more extreme fitted values (e.g., x≈−0.8 or x≈0.75), the 
#    residuals are more spread out (though sparse). Crucially, the range of residuals is much 
#    wider for small fitted values than for large fitted values.
#    
#    There is some  evidence of heteroscedasticity (non-constant variance), although it 
#    is less severe than the systematic pattern. The model's errors are not equally predictable 
#    across the range of predicted returns.
#
#             -------------------*/          \*-------------------

# --- Breusch-Pagan Test ---
# Arguments: residuals, X (exog, independent variables)
# Returns: lagrange_multiplier, p_value_lm, fvalue, p_value_f
lm, lm_pvalue, fvalue, f_pvalue = het_breuschpagan(model_1_residuals_log_returns, 
PETR3_SA_Close_fractional_difference_Linear_filtred_train.values.astype(np.float64))


print(f"\n--- Breusch-Pagan Test ---")
print(f"Lagrange Multiplier p-value: {lm_pvalue:.4f}")
print(f"F-statistic p-value:         {f_pvalue:.4f}")
if lm_pvalue < 0.05:
    print("  -> Significant p-value suggests presence of heteroscedasticity (reject H0).")
else:
    print("  -> No significant evidence of heteroscedasticity (fail to reject H0).")

#   No significant evidence of heteroscedasticity (fail to reject H0).

#=============*/        \*=================	

#  Bringing the estimated data back to y_obs actual values

PETR3_SA_Close_fractional_difference_Linear_filtred = pd.read_csv("C:/Users/Rafael Xavier Botelho/Documents/Econometrics_CEA/PETR3_SA_Close_fractional_difference_Linear_filtred.csv")

interval_min = np.min(PETR3_SA_Close_fractional_difference_Linear_filtred.values.flatten().astype(np.float64))
interval_max = np.max(PETR3_SA_Close_fractional_difference_Linear_filtred.values.flatten().astype(np.float64))

def function_to_scale_data(x , interval_min  , interval_max):
    return (x-x.min())/(x.max()-x.min()) * (interval_max - interval_min) + interval_min

	  
log_returns_estimated_mean_original_scaled = function_to_scale_data(log_returns_estimated_mean.values.flatten().astype(np.float64),interval_min,interval_max)
		
#######/                               \#######
#                                             #
#  ---- Passage 3 ----                        #
#      Risk KPI. Risk vs. Return Binomial     #
#                                             #
#######/                               \#######

#=============*/        \*=================
# Estimated In-sample log-returns 
#=============*/        \*=================

mean_model_01_train = np.mean(log_returns_estimated_mean_original_scaled[:-forecasting_extrapolation_lengh].flatten().astype(np.float64))
std_model_01_train  = np.std(log_returns_estimated_mean_original_scaled[:-forecasting_extrapolation_lengh].flatten().astype(np.float64))

print('return_model_01_training: %f , risk_model_01_training: %f' % (mean_model_01_train,std_model_01_train))
#  return_model_01_training: -0.001004 , risk_model_01_training: 0.012905

#=============*/        \*=================
#        Var and C-Var calculations
#=============*/        \*=================

# Set the confidence level
alpha = 0.95

# Empirical VaR (percentile)
VaR_emp_in_sample_estimated = np.percentile(log_returns_estimated_mean_original_scaled[:-forecasting_extrapolation_lengh], (1 - alpha) * 100)

# Empirical CVaR (average of losses worse than VaR)
CVaR_emp_in_sample_estimated = log_returns_estimated_mean_original_scaled[:-forecasting_extrapolation_lengh][log_returns_estimated_mean_original_scaled[:-forecasting_extrapolation_lengh] <= VaR_emp_in_sample_estimated].mean()

print(f"Empirical - VaR({alpha*100:.0f}%): {VaR_emp_in_sample_estimated:.4f}")
# Empirical - VaR(95%): -0.0144
print(f"Empirical - CVaR({alpha*100:.0f}%): {CVaR_emp_in_sample_estimated:.4f}")
# Empirical - CVaR(95%): -0.0311

#=============*/        \*=================
# Observed In-sample log-returns
#=============*/        \*=================

mean_train = np.mean(PETR3_SA_Close_fractional_difference_Linear_filtred[:-forecasting_extrapolation_lengh].values.flatten().astype(np.float64))
std_train  = np.std(PETR3_SA_Close_fractional_difference_Linear_filtred[:-forecasting_extrapolation_lengh].values.flatten().astype(np.float64))

print('return_training: %f , risk_training: %f' % (mean_train,std_train))
# return_training: -0.000801 , risk_training: 0.013397

#=============*/        \*=================
#        Var and C-Var calculations
#=============*/        \*=================

# Empirical VaR (percentile)
VaR_emp_in_sample_observed = np.percentile(PETR3_SA_Close_fractional_difference_Linear_filtred[:-forecasting_extrapolation_lengh].values.flatten().astype(np.float64), (1 - alpha) * 100)

# Empirical CVaR (average of losses worse than VaR)
CVaR_emp_in_sample_observed = PETR3_SA_Close_fractional_difference_Linear_filtred[:-forecasting_extrapolation_lengh][PETR3_SA_Close_fractional_difference_Linear_filtred[:-forecasting_extrapolation_lengh] <= VaR_emp_in_sample_observed].mean().values[0]

print(f"Empirical - VaR({alpha*100:.0f}%): {VaR_emp_in_sample_observed:.4f}")
# Empirical - VaR(95%): -0.0197
print(f"Empirical - CVaR({alpha*100:.0f}%): {CVaR_emp_in_sample_observed:.4f}")
# Empirical - CVaR(95%): -0.0338

#=============*/        \*=================
# Estimated Out-of-sample log-returns
#=============*/        \*=================

mean_model_01_test = np.mean(log_returns_estimated_mean_original_scaled[-forecasting_extrapolation_lengh:].flatten().astype(np.float64))
std_model_01_test  = np.std(log_returns_estimated_mean_original_scaled[-forecasting_extrapolation_lengh:].flatten().astype(np.float64))

print('return_model_01_test: %f , risk_model_01_test: %f' % (mean_model_01_test,std_model_01_test))
# return_model_01_test: -0.003729 , risk_model_01_test: 0.004244

#=============*/        \*=================
#        Var and C-Var calculations
#=============*/        \*=================

# Empirical VaR (percentile)
VaR_emp_outta_sample_estimated = np.percentile(log_returns_estimated_mean_original_scaled[-forecasting_extrapolation_lengh:], (1 - alpha) * 100)

# Empirical CVaR (average of losses worse than VaR)
CVaR_emp_outta_sample_estimated = log_returns_estimated_mean_original_scaled[-forecasting_extrapolation_lengh:][log_returns_estimated_mean_original_scaled[-forecasting_extrapolation_lengh:] <= VaR_emp_outta_sample_estimated].mean()

print(f"Empirical - VaR({alpha*100:.0f}%): {VaR_emp_outta_sample_estimated:.4f}")
# Empirical - VaR(95%):  -0.0096
print(f"Empirical - CVaR({alpha*100:.0f}%): {CVaR_emp_outta_sample_estimated:.4f}")
# Empirical - CVaR(95%): -0.0106

#=============*/        \*=================
# Observed Out-of-sample log-returns
#=============*/        \*=================

mean_test = np.mean(PETR3_SA_Close_fractional_difference_Linear_filtred[-forecasting_extrapolation_lengh:].values.flatten().astype(np.float64))
std_test  = np.std(PETR3_SA_Close_fractional_difference_Linear_filtred[-forecasting_extrapolation_lengh:].values.flatten().astype(np.float64))

print('return_test: %f , risk_test: %f' % (mean_test,std_test))
# return_test: 0.011757 , risk_test: 0.013466

#=============*/        \*=================
#        Var and C-Var calculations
#=============*/        \*=================

# Empirical VaR (percentile)
VaR_emp_outta_sample_observed = np.percentile(PETR3_SA_Close_fractional_difference_Linear_filtred[-forecasting_extrapolation_lengh:].values.flatten().astype(np.float64), (1 - alpha) * 100)

# Empirical CVaR (average of losses worse than VaR)
CVaR_emp_outta_sample_observed = PETR3_SA_Close_fractional_difference_Linear_filtred[-forecasting_extrapolation_lengh:][PETR3_SA_Close_fractional_difference_Linear_filtred[-forecasting_extrapolation_lengh:] <= VaR_emp_outta_sample_observed].mean().values[0]

print(f"Empirical - VaR({alpha*100:.0f}%): {VaR_emp_outta_sample_observed:.4f}")
# Empirical - VaR(95%): -0.0075
print(f"Empirical - CVaR({alpha*100:.0f}%): {CVaR_emp_outta_sample_observed:.4f}")
# Empirical - CVaR(95%): -0.0101

#=============*/        \*=================
# Full Estimated log-returns (Model 1)
#=============*/        \*=================

mean_y_hat = np.mean(log_returns_estimated_mean_original_scaled.astype(np.float64))
std_y_hat  = np.std(log_returns_estimated_mean_original_scaled.astype(np.float64))

print('return_y_hat: %f , risk_y_hat: %f' % (mean_y_hat , std_y_hat))
#  return_y_hat: -0.001059 , risk_y_hat: 0.012794

#=============*/        \*=================
#        Var and C-Var calculations
#=============*/        \*=================

# Empirical VaR (percentile)
VaR_emp = np.percentile(log_returns_estimated_mean_original_scaled, (1 - alpha) * 100)

# Empirical CVaR (average of losses worse than VaR)
CVaR_emp = log_returns_estimated_mean_original_scaled[log_returns_estimated_mean_original_scaled <= VaR_emp].mean()

print(f"Empirical - VaR({alpha*100:.0f}%): {VaR_emp:.4f}")
# Empirical - VaR(95%): -0.0190
print(f"Empirical - CVaR({alpha*100:.0f}%): {CVaR_emp:.4f}")
# Empirical - CVaR(95%): -0.0329

#######/                               \#######
#                                             #
#    Possible Stop Loss and Take Profit       #
#                                             #
#######/                               \#######


#=============*/        \*=================
# Risk-Reward (RR) Backtest via Simulation
#=============*/        \*=================

#########################*/                                    *\#########################
#
#     This script allows you to:
#
#        - Calculate the hit rate (win rate) of a setup for a given Stop Loss (SL)
#          and a list of possible Risk-Reward ratios (RR = reward / risk);
#
#        - Simulate, for each RR, trades that enter at the closing price (Close[t])
#          and check whether the Take Profit (TP) or Stop Loss (SL) is hit first 
#          within the next N days, using the High and Low columns;
#		  
#        - Compute metrics such as win rate, average gain, average loss, and 
#          expected return per trade (expectancy), and select the RR value that
#          maximizes expectancy — or the smallest positive RR that maintains a 
#          favorable expectation.
#
#########################*/                                    *\#########################

def breakeven_rr_from_winrate(win_rate):
    """Returns the break-even RR (RR = reward per 1 unit of risk) for a given hit rate."""
    if win_rate <= 0:
        return np.inf
    return (1 - win_rate) / win_rate

# ============================================================
# LONG SETUP — Bearish Reversal + RSI < 30
# ============================================================

def backtest_rr(
    df,
    sl_pct,               # Stop Loss as a proportion (e.g., 0.0094 = 0.94%)
    rr_list=None,         # List of candidate RRs (e.g., np.linspace(0.5,4,36))
    max_horizon=10,       # Maximum number of days to wait for TP/SL
    entry_mask=None       # Optional boolean mask to define entry points
):
    """
    Performs a brute-force backtest: for each entry day t,
    checks whether the price hits the Take Profit (TP) or Stop Loss (SL)
    first within a horizon of max_horizon days.
    Returns a DataFrame with aggregated performance metrics by RR.
    """
    #
    if rr_list is None:
        rr_list = np.linspace(0.5, 4.0, 36)
    # validations
    assert all(c in df.columns for c in ['Close', 'High', 'Low']), \
        "The DataFrame must contain 'Close', 'High', and 'Low' columns."
    #
    n = len(df)
    entry_idx = [i for i in np.where(entry_mask.values)[0] if i + max_horizon < n]
    #
    results = []
    # Main loop over each RR value
    for rr in rr_list:
        wins, gains, losses, no_hits = [], [], [], []
        #
        tp_multiplier = 1 + rr * sl_pct  # target price (TP)
        sl_multiplier = 1 - sl_pct       # stop price (SL)
        #
        for t in entry_idx:
            P0 = df['Close'].iat[t]
            price_tp = P0 * tp_multiplier
            price_sl = P0 * sl_multiplier
            #
            hit = None
            # check within the next days whether TP/SL was hit
            for k in range(1, max_horizon + 1):
                hi = df['High'].iat[t + k]
                lo = df['Low'].iat[t + k]
                #
                tp_hit = hi >= price_tp
                sl_hit = lo <= price_sl
                #
                if tp_hit and sl_hit:
                    # without intraday data → assume SL (conservative approach)
                    hit = 'SL'
                    break
                elif tp_hit:
                    hit = 'TP'
                    break
                elif sl_hit:
                    hit = 'SL'
                    break
            # compute realized return
            if hit is None:
                # no target hit → close at price after the horizon
                Pf = df['Close'].iat[t + max_horizon]
                no_hits.append(np.log(Pf / P0))
            elif hit == 'TP':
                gains.append(np.log(price_tp / P0))
                wins.append(1)
            else:  # hit == 'SL'
                losses.append(np.log(price_sl / P0))
                wins.append(0)
        # aggregate metrics
        wins = np.array(wins)
        win_rate = wins.mean() if wins.size > 0 else np.nan
        avg_win = np.mean(gains) if len(gains) > 0 else 0.0
        avg_loss = np.mean(losses) if len(losses) > 0 else 0.0
        avg_nohit = np.mean(no_hits) if len(no_hits) > 0 else 0.0
        #
        all_results = np.concatenate([
            np.array(gains) if len(gains) > 0 else np.array([]),
            np.array(losses) if len(losses) > 0 else np.array([]),
            np.array(no_hits) if len(no_hits) > 0 else np.array([])
        ])
        expectancy = all_results.mean() if all_results.size > 0 else np.nan
        #
        results.append({
            'RR': rr,
            'win_rate': win_rate,
            'avg_win': avg_win,
            'avg_loss': avg_loss,
            'avg_nohit': avg_nohit,
            'expectancy': expectancy,
            'n_trades': len(entry_idx),
            'n_wins': int(np.nansum(wins)) if wins.size > 0 else 0
        })
    #
    return pd.DataFrame(results)
	
# 1️ Reversal Mask: Entries after 3 consecutive days of decline

PETR3_SA = PETR3_SA[:-forecasting_extrapolation_lengh]

mask_reversal = (
    (PETR3_SA['Close'] < PETR3_SA['Close'].shift(1)) &
    (PETR3_SA['Close'].shift(1) < PETR3_SA['Close'].shift(2)) &
    (PETR3_SA['Close'].shift(2) < PETR3_SA['Close'].shift(3))
)

# 2️  RSI indicator for confirmation

PETR3_SA['RSI'] = ta.momentum.RSIIndicator(PETR3_SA['Close']).rsi()
mask_entry = mask_reversal & (PETR3_SA['RSI'] < 30)

# 3️  Test Parameters
rr_candidates = np.linspace(0.5, 3.0, 26)
stop_multipliers = [0.5, 1.0, 1.5, 2.0]
horizons = [3, 5, 7, 10]

# 4️  Results matrix (average expectation)
results = []

for sl_mult in stop_multipliers:
    sl_pct = sl_mult * abs(CVaR_emp)
    for horizon in horizons:
        df_long = backtest_rr(
            df=PETR3_SA,
            sl_pct=sl_pct,
            rr_list=rr_candidates,
            max_horizon=horizon,
            entry_mask=mask_entry
        )
        exp_mean = df_long['expectancy'].mean()
        results.append({
            'SL_mult': sl_mult,
            'Horizon': horizon,
            'Exp_mean': exp_mean
        })

df_results = pd.DataFrame(results)
pivot = df_results.pivot(index='SL_mult', columns='Horizon', values='Exp_mean')

# 5️ Heatmap
plt.figure(figsize=(8,5))
sns.heatmap(pivot, annot=True, fmt=".4f", cmap="YlGnBu")
plt.title("Expected Return Heatmap (Long)  - PETR3.SA\n(Reversal + RSI<30)")
plt.xlabel("Max Horizon (days)")
plt.ylabel("Stop Multiplier (× CVaR)")
plt.show()

# Figure_3.1a_Expected_Return_Heatmap_(Long) 

# 6️  Best combination
best_row = df_results.loc[df_results['Exp_mean'].idxmax()]
print(" Best combination found:")
#  Best combination found:
print(f"   Stop = {best_row['SL_mult']} × CVaR")
#  Stop = 1.0 × CVaR
print(f"   Horizon = {int(best_row['Horizon'])} days")
#  Horizon = 10 days
print(f"   Average Expectancy  = {best_row['Exp_mean']:.6f}")
#  Average Expectancy = 0.015295

# ================================================
# Expectancy × RR Curve  —  Best Setup Found
# Stop = 1.0 × CVaR | Horizon = 10 days
# ================================================

# 1️ Fixed parameters (from the optimal setup found)
best_stop_mult = 1.0
best_horizon = 10
sl_pct = best_stop_mult * abs(CVaR_emp)

# 2️ List of RRs to test
rr_candidates = np.linspace(0.5, 4.0, 36)

# 3️ Run the backtest only on this setup
df_best = backtest_rr(
    df=PETR3_SA,
    sl_pct=sl_pct,
    rr_list=rr_candidates,
    max_horizon=best_horizon,
    entry_mask=mask_entry
)

# 4️ Graph: Expectancy × RR
plt.figure(figsize=(8,5))
plt.plot(df_best['RR'], df_best['expectancy'], marker='o', linewidth=2, color='tab:blue')
plt.axhline(0, color='gray', linestyle='--', linewidth=1)
plt.title("Expectancy vs Risk-Reward (RR)-Long\nPETR3.SA — Stop=1×CVaR, Horizon=10d, RSI<30")
plt.xlabel("Risk-Reward Ratio (RR)")
plt.ylabel("Average Expectancy by trade (log-return)")
plt.grid(alpha=0.3)

# 5️ Optimal RR  (maximum expectancy)
best_rr_row = df_best.loc[df_best['expectancy'].idxmax()]
best_rr = best_rr_row['RR']
best_exp = best_rr_row['expectancy']

# 6 Mark the optimum point on the graph
plt.scatter(best_rr, best_exp, color='red', s=100, label=f"RR optimal = {best_rr:.2f}\nExp = {best_exp:.4f}")
plt.legend()
plt.show()

# Figure_3.1b_Expectancy_vs_Risk_Reward_(RR)_Long_Best_setup

# 7 Displays the optimal value
print("Best RR found for the setup (Stop=1.0×CVaR, Horizon=10d):")
# Best RR found for the setup (Stop=1.0×CVaR, Horizon=10d):  
print(f" Optimal RR  = {best_rr:.2f}")
# Optimal RR = 1.30    
print(f"   Average Expectancy  = {best_exp:.6f}")
# Average Expectancy = 0.022641
print(f"   TP = {best_rr:.2f} × |CVaR_emp| = {best_rr * abs(CVaR_emp):.4f}")
# TP = 1.30 × |CVaR_emp| = 0.0428

# ============================================================
# SHORT SETUP — Bullish Reversal + RSI > 70
# ============================================================

# 1️ Bullish Reversal Mask
mask_reversal_short = (
    (PETR3_SA['Close'] > PETR3_SA['Close'].shift(1)) &
    (PETR3_SA['Close'].shift(1) > PETR3_SA['Close'].shift(2)) &
    (PETR3_SA['Close'].shift(2) > PETR3_SA['Close'].shift(3))
)

# 2️ High RSI -> overbought condition
mask_entry_short = mask_reversal_short & (PETR3_SA['RSI'] > 70)

# 3️ Setup parameters (same as long version)
best_stop_mult = 1.0
best_horizon = 10
sl_pct = best_stop_mult * abs(CVaR_emp)
rr_candidates = np.linspace(0.5, 4.0, 36)

# 4️ Backtest function adapted for shorts
def backtest_rr_short(df, sl_pct, rr_list, max_horizon, entry_mask):
    results = []
    n = len(df)
    entry_idx = [i for i in np.where(entry_mask.values)[0] if i + max_horizon < n]
    for rr in rr_list:
        wins, gains, losses, no_hits = [], [], [], []
        tp_multiplier = 1 - rr * sl_pct  # Take profit (price falls)
        sl_multiplier = 1 + sl_pct       # Stop loss (price goes up)
        for t in entry_idx:
            P0 = df['Close'].iat[t]
            price_tp = P0 * tp_multiplier
            price_sl = P0 * sl_multiplier
            hit = None
            for k in range(1, max_horizon + 1):
                hi = df['High'].iat[t + k]
                lo = df['Low'].iat[t + k]
                tp_hit = lo <= price_tp
                sl_hit = hi >= price_sl
                if tp_hit and sl_hit:
                    hit = 'SL'
                    break
                elif tp_hit:
                    hit = 'TP'
                    break
                elif sl_hit:
                    hit = 'SL'
                    break
            if hit is None:
                Pf = df['Close'].iat[t + max_horizon]
                no_hits.append(np.log(P0 / Pf))  # invertido
            elif hit == 'TP':
                gains.append(np.log(P0 / price_tp))
                wins.append(1)
            else:
                losses.append(np.log(P0 / price_sl))
                wins.append(0)
        wins = np.array(wins)
        expectancy = (
            np.mean(gains + losses + no_hits)
            if len(gains + losses + no_hits) > 0
            else np.nan
        )
        results.append({
            'RR': rr,
            'win_rate': wins.mean() if len(wins) > 0 else np.nan,
            'expectancy': expectancy,
            'n_trades': len(entry_idx)
        })
    return pd.DataFrame(results)

# 5️ Run the short backtest
df_short = backtest_rr_short(
    df=PETR3_SA,
    sl_pct=sl_pct,
    rr_list=rr_candidates,
    max_horizon=best_horizon,
    entry_mask=mask_entry_short
)

# 6️ Plot the Expectancy × RR (Short) graph
plt.figure(figsize=(8,5))
plt.plot(df_short['RR'], df_short['expectancy'], marker='o', linewidth=2, color='tab:red')
plt.axhline(0, color='gray', linestyle='--', linewidth=1)
plt.title("Expectancy vs Risk-Reward (RR)\nPETR3.SA — SHORT, Stop=1×CVaR, Horizon=10d, RSI>70")
plt.xlabel("Risk-Reward Ratio (RR)")
plt.ylabel("Average Expectancy by trade (log-return)")
plt.grid(alpha=0.3)

# 7️ Identify the best short RR
best_short_row = df_short.loc[df_short['expectancy'].idxmax()]
best_short_rr = best_short_row['RR']
best_short_exp = best_short_row['expectancy']

plt.scatter(best_short_rr, best_short_exp, color='black', s=100, label=f"RR ótimo = {best_short_rr:.2f}\nExp = {best_short_exp:.4f}")
plt.legend()
plt.show()

# Figure_3.1c_Expectancy_vs_Risk_Reward_(RR)_Short_Best_setup

# 8️ Print a summary
print("Best RR (setup SHORT):")
# Best RR (setup SHORT):
print(f"RR optimal = {best_short_rr:.2f}")
# RR optimal =  1.10
print(f"Average Expectancy  = {best_short_exp:.6f}")
# Average Expectancy  =  0.036867
print(f"TP = {best_short_rr:.2f} × |CVaR_emp| = {best_short_rr * abs(CVaR_emp):.4f}")
# TP = 1.10 × |CVaR_emp| = 0.0362


plt.figure(figsize=(8,5))
plt.plot(df_long['RR'], df_long['expectancy'], 'b.-', label='LONG (RSI < 30)')
plt.plot(df_short['RR'], df_short['expectancy'], 'r.-', label='SHORT (RSI > 70)')

plt.axhline(0, color='gray', linestyle='--', lw=0.8)
plt.title("PETR3.SA - Expectancy vs Risk-Reward (Long vs Short)")
plt.xlabel("Risk-Reward Ratio (RR)")
plt.ylabel("Average Expectancy per Trade (log-return)")
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()

# Figure_3.1d_Expectancy vs Risk-Reward (Long vs Short)

# =============================
# Operational Decision Making
# =============================


exp_long_pct  = np.expm1(best_exp)
exp_short_pct = np.expm1(best_short_exp)

print(f"Expected % return (LONG) ≈ {exp_long_pct*100:.2f}%")
# Expected % return (LONG) ≈ 2.29%
print(f"Expected % return (SHORT) ≈ {exp_short_pct*100:.2f}%")
# Expected % return (SHORT) ≈ 3.76%

# ============================================================
#   
#   Momentum symmetry: 
#        
#   Risk-Reward sweet spots:
#
#       For longs, small targets (RR ≈ 1.3) seem to work best — rebounds are short-lived.
#       For shorts, also small targets (RR ≈ 1.1) capture the faster, deeper corrections.
#
# ============================================================

# --- y_hat: vector of estimated log-returns by the model ---
y_hat = np.array(log_returns_estimated_mean_original_scaled)

n = len(y_hat)
x = np.arange(n)

# === 1️ Rolling Quantile Regression for dynamic supports/resistances ===
def rolling_quantile_regression(y, x, window=30, q=0.15):
    """Fit rolling quantile regression to estimate dynamic support/resistance."""
    fitted = np.full_like(y, np.nan, dtype=float)
    for i in range(window, len(y)):
        yi = y[i-window:i]
        xi = x[i-window:i]
        model = QuantReg(yi, np.vstack([np.ones_like(xi), xi]).T)
        res = model.fit(q=q)
        fitted[i] = res.predict([1, x[i]])[0]
    return pd.Series(fitted).interpolate(limit_direction="both").to_numpy()

support_q = rolling_quantile_regression(y_hat, x, window=30, q=0.15)
resistance_q = rolling_quantile_regression(y_hat, x, window=30, q=0.85)

# === 2️ Last 5 forecasts and parameters ===
last_n = forecasting_extrapolation_lengh
lookback_confirm = 3
last_returns = y_hat[-last_n:]
last_support = support_q[-last_n:]
last_resistance = resistance_q[-last_n:]

avg_pred = np.mean(last_returns)
avg_sup = np.mean(last_support)
avg_res = np.mean(last_resistance)

# === 3️ Detect confirmed breakout ===
above_res = np.sum(last_returns[-lookback_confirm:] > last_resistance[-lookback_confirm:])
below_sup = np.sum(last_returns[-lookback_confirm:] < last_support[-lookback_confirm:])

if above_res >= lookback_confirm:
    decision = "BUY signal — breakout confirmed above resistance"
    zone_color = "limegreen"
    signal = "BUY"
elif below_sup >= lookback_confirm:
    decision = "SELL signal — breakdown confirmed below support"
    zone_color = "lightcoral"
    signal = "SELL"
elif avg_pred > avg_res:
    decision = "Weak bullish momentum (approaching resistance)"
    zone_color = "khaki"
    signal = "NEUTRAL"
elif avg_pred < avg_sup:
    decision = "Weak bearish momentum (approaching support)"
    zone_color = "khaki"
    signal = "NEUTRAL"
else:
    decision = "Neutral — inside channel"
    zone_color = "khaki"
    signal = "NEUTRAL"

# === 4️ Generate visual signals on the plot ===
buy_idx = np.where(y_hat > resistance_q)[0]
sell_idx = np.where(y_hat < support_q)[0]

# === 5️ Plot ===
plt.figure(figsize=(11, 6))
plt.plot(x, y_hat, color="steelblue", lw=1.8, label="Estimated log-returns")
plt.plot(x, support_q, '--', color='green', lw=1.2, label="Support (15%)")
plt.plot(x, resistance_q, '--', color='red', lw=1.2, label="Resistance (85%)")

# Green arrow for BUY, red arrow for SELL
plt.scatter(buy_idx, y_hat[buy_idx], marker='^', color='limegreen', s=80, label='Buy breakout ↑')
plt.scatter(sell_idx, y_hat[sell_idx], marker='v', color='red', s=80, label='Sell breakout ↓')

# Line that separates forecasts
plt.axvline(n-last_n, color='orange', lw=2, linestyle='--', label="Forecast boundary")

# Forecast area (colored according to decision)
plt.axvspan(n-last_n, n, color=zone_color, alpha=0.25, label=f"Forecast zone ({signal})")

# === 6 Plot details ===
plt.title("Quantile Support/Resistance & Confirmed Breakouts — ŷ Log-Returns", fontsize=13)
plt.xlabel("Observation index", fontsize=11)
plt.ylabel("Predicted Log-return", fontsize=11)
plt.legend()
plt.grid(True, linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()

# Figure_3.1e_Quantile_Support_Resistance_Confirmed_Breakouts_yhat_Log_Returns

best_rr = 1.30              # Optimal Risk/Reward (LONG setup)
best_exp = 0.015295         # Average Expectancy (LONG setup)
best_short_rr = 1.10        # Optimal Risk/Reward (SHORT setup)
best_short_exp = 0.036867   # Average Expectancy (SHORT setup)

# === 1️ Last price and model parameters ===
P_last = PETR3_SA["Close"].iloc[-1]

# Empirical CVaR (risk)
SL = abs(CVaR_emp)

# Optimal parameters
TP_long = best_rr * SL
TP_short = best_short_rr * SL

# === 2️ Stop and Target Limits ===
price_SL_long = P_last * np.exp(-SL)
price_TP_long = P_last * np.exp(TP_long)
price_SL_short = P_last * np.exp(SL)
price_TP_short = P_last * np.exp(-TP_short)

# === 3️ RSI (technical confirmation) ===
rsi_window = 14
delta = PETR3_SA["Close"].diff()
gain = delta.clip(lower=0).rolling(rsi_window).mean()
loss = -delta.clip(upper=0).rolling(rsi_window).mean()
rs = gain / loss
rsi = 100 - (100 / (1 + rs))
rsi_last = rsi.iloc[-1]

# === 4️ Last predicted values by the model ===
yhat_last = y_hat[-1]
support_last = support_q[-1]
resistance_last = resistance_q[-1]

# === 5️ Decision logic (breakout + RSI) ===
if yhat_last > resistance_last and rsi_last < 30:
    setup = "LONG"
    signal = "BUY CONFIRMED (broke resistance + RSI<30)"
    expectancy = best_exp
    price_SL = price_SL_long
    price_TP = price_TP_long
elif yhat_last < support_last and rsi_last > 70:
    setup = "SHORT"
    signal = "SELL CONFIRMED (broke support + RSI>70)"
    expectancy = best_short_exp
    price_SL = price_SL_short
    price_TP = price_TP_short
else:
    setup = "NEUTRAL"
    signal = "No confirmed breakout — maintain neutral position"
    expectancy = np.nan
    price_SL = np.nan
    price_TP = np.nan

# === 6️ Display result ===
print("\n===== AUTO DECISION SYSTEM (Model + RSI) =====")
# ===== AUTO DECISION SYSTEM (Model + RSI) =====
print(f"Last price: {P_last:.2f}")
# Last price: 31.57
print(f"Last predicted log-return (ŷ_t): {yhat_last:.5f}")
# Last predicted log-return (ŷ_t): 0.00227
print(f"Support (15%): {support_last:.5f}")
# Support (15%): -0.01227
print(f"Resistance (85%): {resistance_last:.5f}")
# Resistance (85%):-0.00350
print(f"RSI(14): {rsi_last:.2f}")
# RSI(14): 21.77
print(f"Final Setup: {setup}")
# Final Setup: LONG
print(f"Signal: {signal}")
# Signal: BUY CONFIRMED (broke resistance + RSI<30)

if setup != "NEUTRAL":
    tp = TP_long if setup == "LONG" else TP_short
    print(f"Expectancy: {expectancy:.6f}")
    print(f"Stop Loss (−{SL*100:.2f}%): {price_SL:.2f}")
    print(f"Take Profit (+{tp*100:.2f}%): {price_TP:.2f}")
else:
    print("No active operation — waiting for confirmed signal.")
	
# Expectancy: 0.015295
# Stop Loss (−3.11%): 30.55
# Take Profit (+4.05%): 32.95

# === 7️ (Optional) — quick plot RSI + final signal ===
fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6), sharex=True)
ax1.plot(y_hat, label='ŷ log-returns', color='steelblue')
ax1.plot(support_q, 'g--', alpha=0.6, label='Support')
ax1.plot(resistance_q, 'r--', alpha=0.6, label='Resistance')
ax1.set_title("Predicted log-returns with Support/Resistance")
ax1.legend()

# RSI
ax2.plot(rsi, color='purple', label='RSI(14)')
ax2.axhline(30, color='green', linestyle='--', alpha=0.7)
ax2.axhline(70, color='red', linestyle='--', alpha=0.7)
ax2.set_title("RSI confirmation zone")
ax2.legend()

plt.tight_layout()
plt.show()

# Figure_3.1e_Predicted_log_returns_with_Support_Resistance_&_RSI_zones






#Parte 4 Conclusão sobre o assunto aboradado

###########################/* Final Considerations *\ ########################################
#     
#     English :
#
#     Despite log-returns being a chaotic system type II ,pure stochastic process / pure randomness (Chaos) ,for its nature unpredictable, here it folows the  executive summary
#
#          Signal and Setup: The system validated a Confirmed Buy Signal (BUY CONFIRMED), establishing a LONG Setup (Buy operation/betting on appreciation).
#
#          Technical Justification: The decision is robust as it combines the model's positive forecast (yhat​=0.00227) with the momentum indicator RSI(14) at 21.77, which is in the oversold zone (below 30), and the break of resistance.
#
#          Operational Viability: The key financial metric, Expectancy, is positive (0.015295), indicating that, on average, the operation is expected to be profitable.
#
#          Execution Parameters: The operation has a current price of 31.57, with clearly defined risk management limits: Stop Loss at 30.55 and Take Profit at 32.95.
#
#      Disclaimer
#
#          "I am not, whatsoever,  responsible for any gains and/or losses you may incur if
# 		   you use these models for trading... For educational purposes only."
#
#
#
#   Português
#   
#   Embora os retornos logarítmicos sejam um sistema caótico do tipo II ,processo estocástico/aleatóridade(Caos) puro,  e, por sua natureza, imprevisíveis, segue-se o resumo executivo:
#
#
# # Resumo Executivo
#
# Sinal e Configuração (Setup): O sistema validou um Sinal de Compra Confirmado (BUY CONFIRMED), estabelecendo uma Configuração LONG (Operação de compra/apostando na valorização).
#
# Justificativa Técnica: A decisão é robusta, pois combina a previsão positiva do modelo (yhat{y}_t = 0.00227$) com o indicador de momento RSI(14) em 21.77, que se encontra na zona de sobre-venda (abaixo de 30), e o rompimento da resistência.
#
# Viabilidade Operacional: A métrica financeira-chave, a Expectância, é positiva (0.015295), o que indica que, em média, espera-se que a operação seja lucrativa.
#
# Parâmetros de Execução: A operação apresenta um preço atual de 31.57, com limites de gestão de risco claramente definidos: Stop Loss em 30.55 e Take Profit em 32.95.
#
#
# # Aviso Legal (Disclaimer)
#
# "Não sou, em hipótese alguma, responsável por quaisquer ganhos e/ou perdas que você possa incorrer caso utilize estes modelos para negociação (trading)... Exclusivamente para fins educacionais."               
#
###########################/*           The End            *\ ########################################
